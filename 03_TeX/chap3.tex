\chapter{Resultat zur Konvergenzgeschwindigkeit}
\label{chap:3}

In diesem Kapitel stellen wir das Hauptresultat dieser Arbeit vor.
Ziel im Folgenden ist es, eine Abschätzung des erwarteten $L_2$-Fehlers 
$$\E \int |m_n(x) - m(x)|^2  \mathds{P}_X(dx)$$
im Falle des Schätzers 
\begin{equation}
\label{schätzer}
m_n(x) \coloneqq T_{\beta_n}\tilde{m}_n (x),
\end{equation}
unter Annahme einer $(p,C)$-glatten Regressionsfunktion $m$ herzuleiten. Hierbei bezeichnet $T_{\beta}z = \max\{\min\{z, \beta\}, -\beta\}$
für $z \in \R$ und $\beta > 0$ und $\tilde{m}_n$ unser Neuronale-Netze-Regresssionsschätzer aus Kapitel~\ref{subsec:2.2} ist, welcher aus mehreren neuronalen Netzen konstruiert wurde. Für diesen gilt: 
Die Aktivierungsfunktion $\sigma$ ist der logistische Squasher, $N > q, M = M_n = \lceil c_2 \cdot n^{1/(2p + d)}\rceil$ mit $c_2 >0$ und unabhängig von $n$, $R = R_n = n^{d + 4}$ und $a = a_n = (\log n)^{1/(6(N + d))}.$

Nun kommen wir zu dem Hauptresultat dieser Arbeit.
\begin{mthm}[{\cite[Theorem 1]{kohler19}}]
\label{optstop}
Angenommen die Verteilung von $Y$ erfüllt 
$$ \E\Big[\mathrm{e}^{c_1 \cdot |Y|^2}\Big] < \infty$$
für eine Konstante $c_1 > 0$ und die Verteilung von $X$ besitzt einen beschränkten Träger $\supp(\mathds{P}_X)$. Sei $m(x) = \E[Y \mid X = x]$ die zu dem Tupel $(X, Y)$ gehörige Regressionsfunktion.
Angenommen $m$ ist $(p,C)$-glatt, mit $p = q + s$, wobei $q \in \N_0$, $s \in (0,1]$ und $C > 0$ ist.

Sei $\beta_n = c_3 \cdot \log(n)$ für eine hinreichend große und von $n$ unabhängige Konstante $c_3 > 0$ und sei $m_n$ aus Gleichung~(\ref{schätzer}) gegeben.

Dann erhalten wir für hinreichend großes $n$:
$$\E \int |m_n(x) - m(x)|^2  \mathds{P}_X(dx) \leq c_{\mathrm{fin}} \cdot (\log n)^3 \cdot n^{- \frac{2p}{2p + d}},$$
wobei $c_{\mathrm{fin}} > 0$ und unabhängig von $n$ ist.
\end{mthm}

Der folgende Abschnitt liefert Hilfsresultate welche wir im Beweis unsere Hauptresultats gebrauchen werden.

\section{Approximationsresultate für Hauptsatz 3.1}
Die nächsten  Definitionen und Lemmata benötigen wir für den Beweis unseres Hauptresultats, einer Aussage über die Konvergenzgeschwindigkeit unseres Neuronale-Netze-Schätzers. Die Lemmata werden hier nur der Vollständigkeit halber und ohne Beweis aufgeführt. 

\begin{lem}[{\cite[Lemma 5]{kohler19}}]
\label{lem:5}
Sei $M \in \N$ und $\sigma\colon \R \to [0, 1]$ $2$-zulässig.
Sei $a \geq 1$ und $R \in \R$ mit
\begin{equation*}
\begin{split}
R & \geq \max\biggl\{\frac{\|\sigma''\|_{\infty} \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma})|}, \frac{9 \cdot \|\sigma''\|_{\infty} \cdot a}{|\sigma'(t_{\sigma})|}, \\
& \quad \frac{20 \cdot \|\sigma'''\|_{\infty}}{3 \cdot |\sigma''(t_{\sigma})|} \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s}, 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty},\|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \biggr\},
\end{split}
\end{equation*}
wobei $s = \lceil\log_2(N + d)\rceil$ mit $N \in \N$. Sei $y \in [-a, a]^d$ und $\bj \in [\N_0]^d$ so, dass $|\bj|_1 \leq N$ gilt. Sei $f_{\id}, f_{\mult}$ und $f_{\mathrm{hat}, z}$ (für $z \in \R$) die neuronalen Netze aus Lemma~\ref{lem:1}, Lemma~\ref{lem:2} und Lemma~\ref{lem:4}. Wir definieren das Netz $f_{\mathrm{net},\bj,\bi}$ wie in Kapitel~\ref{subsec:2:1} Gleichung~(\ref{fnet}) mit $y$ anstelle von $x_{{i}_k}$.
Dann erhalten wir für $x \in [-a, a]^d$:
\begin{equation*}
\begin{split}
& \bigg|f_{\net,\bj,\bi}(x) - (x - y)^{\bj} \prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - y^{(j)}|)_+\bigg| \\
& \leq c \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*}
für eine von $n$ unabhängige Konstante $c > 0$.
\end{lem}

Als nächstes geben wir eine Definition von Überdeckungszahlen an, da wir im Beweis für unser Hauptresultat eine Abschätzung einer $L_p\text{-}\epsilon$-Überdeckungszahl anwenden.
\begin{defn}
\label{ueberdeckung}
$(X, \delta)$ sei ein pseudometrischer Raum \cite[Definition 2.1.1]{Topologie2015} . Für $x \in X$ und $\epsilon > 0$ sei:
$$U_{\epsilon}(x) = \{z \in X : \delta(x, z) < \epsilon\}$$
die Kugel um $x$ mit Radius $\epsilon$.
\begin{itemize}
\item[a)] $\{z_1,\dots,z_N\} \subseteq X$ heißt $\epsilon$\textit{-Überdeckung} einer Menge $A \subseteq X$, falls gilt:
$$A \subseteq \bigcup_{k = 1}^N U_{\epsilon}(z_k).$$
\item[b)] Ist $A \subseteq X$ und $\epsilon > 0$, so ist die sogenannte $\epsilon$\textit{-Überdeckungszahl} von $A$ in $(X,\delta)$ definiert als:
$$\mathcal{N}_{(X,\delta)}(\epsilon, A) = \inf\big\{|U| : U \subseteq X \text{ ist } \epsilon\text{-Überdeckung von } A\big\}.$$   
\end{itemize}
\end{defn} 
\begin{defn}
\label{lpe}
Sei $\mathcal{F}$ eine Menge von Funktionen $f\colon \R^d \to \R$, sei $\epsilon > 0$, $1 \leq p < \infty$ und seien $x_1,\dots,x_n \in \R^d$ und $x_1^n = (x_1,\dots,x_n).$ Dann ist die $L_p$-$\epsilon$\textit{-Überdeckungszahl} von $\mathcal{F}$ auf $x_1^n$ definiert durch:
$$\mathcal{N}_p(\epsilon, \mathcal{F}, x_1^n) \coloneqq \mathcal{N}_{(X,\delta)}(\epsilon, \mathcal{F}),$$
wobei der pseudometrische Raum $(X, \delta)$ gegeben ist durch
\begin{itemize}
\item $X = $ Menge aller Funktionen $f\colon \R^d \to \R$,
\item $\delta(f, g) = \delta_p(f, g) = (\frac{1}{n}\sum_{i = 1}^n |f(x_i) - g(x_i)|^p)^{1/p} .$
\end{itemize}
\end{defn}
In anderen Worten: $\mathcal{N}_p(\epsilon, \mathcal{F}, x_1^n)$ ist das minimale $N \in \N$, so dass Funktionen $f_1,\dots,f_N\colon \R^d \to \R$ existieren mit der Eigenschaft, dass für jedes $f \in \mathcal{F}$ gilt:
$$\min_{j = 1,\dots,N}\bigg(\frac{1}{n}\sum_{i = 1}^n|f(x_i) - f_j(x_i)|^p\bigg)^{1/p} < \epsilon.$$
Diese Definition benötigen wir, um nun im folgenden Lemma den erwarteten $L_2$-Fehler eines Schätzers, mithilfe einer $L_p$-$\epsilon$\textit{-Überdeckungszahl} abzuschätzen. Dieses Lemma ist ein Spezialfall von \cite[Lemma 8]{kohler19}.
  \begin{lem}
  \label{lem:8}
Sei $\beta_n = c_1 \cdot \log(n)$ für eine hinreichend große Konstante $c_1 > 0$. Angenommen die Verteilung von $Y$ erfüllt 
$$ \E\Big[\mathrm{e}^{c_2 \cdot |Y|^2}\Big] < \infty$$
für eine Konstante $c_2 > 0.$ Zudem nehmen wir an, dass der Betrag der Regressionsfunktion $m$ beschränkt ist. Sei $\mathcal{F}_n$ eine Menge von Funktionen $f\colon \R^d \to \R$ und wir nehmen an, dass der Schätzer $m_n$ 
$$m_n = T_{\beta_n}\tilde{m}_n$$ 
erfüllt, mit 
$$\tilde{m}_n(\cdot) = \tilde{m}_n(\cdot,(X_1, Y_1),\dots,(X_n, Y_n)) \in \mathcal{F}_n$$
und 
$$\frac{1}{n} \sum_{i = 1}^n |Y_i - \tilde{m}_n(X_i)|^2 \leq \frac{1}{n}\sum_{i = 1}^n |Y_i - g_{n}(X_i)|^2 + \mathrm{pen}_n(g_n)$$
mit einer deterministischen Funktion $g_{n}\colon \R^d \to \R$ und deterministischem Penalty Term $\mathrm{pen}_n(g_{n}) \geq 0$, wobei die Funktion $g_{n}\colon \R^d \to \R$ unabhängig von $(X_1, Y_1),\dots,(X_n,Y_n)$ ist.
Dann gilt für der erwarteten $L_2$-Fehler die Ungleichung
\begin{equation}
\label{lem:8:ungl}
\begin{split}
& \E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \frac{c \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (\supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \cdot \E\bigg(\int |g_{n}(x) - m(x)|^2 \mathds{P}_X(dx) + \mathrm{pen}_n(g_{n})\bigg),
\end{split}
\end{equation}
für $n > 1$ und eine von $n$ unabhängige Konstante $c > 0$.
  \end{lem}
Das nächste Lemma benötigen wir, um in Ungleichung~(\ref{lem:8:ungl}) die Überdeckungszahl $\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)$ weiter abzuschätzen.
\begin{lem}[{\cite[Lemma 9]{kohler19}}]
\label{lem:9}
Seien $a > 0$ und $d, N, J_n \in \N$ so, dass $J_n \leq n^{c_{1}}$ und setze $\beta_n = c_2 \cdot \log(n).$
Sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $\mathcal{F}$ die Menge aller Funktionen die durch Definition~\ref{def:nn} definiert sind mit $k_1 = k_2 = \cdots = k_L = 24 \cdot (N + d)$ und einer Beschränkung der Betrag der Gewichte durch $c_{3} \cdot n^{c_{4}}$.
$$ \mathcal{F}^{(J_n)} = \biggl\{\sum_{j = 1}^{J_n} a_j \cdot f_j : f_j \in \mathcal{F} \quad \text{und} \quad \sum_{j = 1}^{J_n} a_j^2 \leq c_{5} \cdot n^{c_{6}}\biggr\}.$$
Dann gilt für $n > 1:$
$$\log\bigg(\sup_{x_1^n\in[-a,a]^{d \cdot n}} \mathcal{N}_1\bigg(\frac{1}{n \cdot \beta_n}, \mathcal{F}^{(J_n)},x_1^n\bigg)\bigg) \leq c \cdot \log(n) \cdot J_n,$$
für eine Konstante $c$ die nur von $L, N, a$ und $d$ abhängt.
\end{lem}

Mit den Approximationsresultaten aus diesem Abschnitt verfügen wir nun über alle Bausteine, um unser Hauptresultat zu beweisen.

\section{Der Beweis von Hauptsatz 3.1}

Da nach Voraussetzung $\supp(\mathds{P}_X)$ beschränkt ist und definitionsgemäß immer abgeschlossen ist, wissen wir nach dem Satz von Heine-Borel (vgl. \cite[Satz 5]{forster2016}), dass $\supp(\mathds{P}_X)$ kompakt ist. Da $m$ als ($p,C$)-glatte Funktion insbesondere stetig ist, wissen wir, dass sie auf einer kompakten Menge ein Maximum und Minimum annimmt. Dadurch können wir $n$ so groß wählen, dass ohne Beschränkung der Allgemeinheit $\|m\|_{\infty} \leq \beta_n$ gilt.

Sei $\mathcal{F}$ die Menge aller Funktion aus Definition~\ref{def:nn} mit Aktivierungsfunktion $\sigma$ und $L = s + 2 = \lceil\log_2(N + d)\rceil + 2,$ mit $k_1 = k_2 = \cdots = k_L = 24 \cdot (N + d)$ und einer Konstante $c_4 > 0$ so, dass der Betrag der Gewichte durch $n^{c_{4}}$ beschränkt ist. Wir definieren
$$ \mathcal{F}^{(J_n)} \coloneqq \biggl\{\sum_{j = 1}^{J_n} a_j \cdot f_j : f_j \in \mathcal{F} \text{ und } \sum_{j = 1}^{J_n} a_j^2 \leq c_5 \cdot n \biggr\}$$
wobei
\begin{equation}
\label{constantc21}
\begin{split}
c_5 \coloneqq \max\Biggl\{\frac{1 + \E[Y^2]}{c_6}, c_2^d \cdot (N + 1)^d \cdot \max\biggl\{\Big| \frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\Big|^2 : \bj \in [q]^d,\, |\bj|_1 \leq q\biggr\}\Biggr\}.
\end{split}
\end{equation}
Hierbei bezeichnet $c_6$ die Konstante des Regularitätsterms aus Gleichung~(\ref{min}). Da wir uns in der nichtparametrischen Regressionsschätzung befinden, gilt unter anderem die Bedingung $\E[Y^2] < \infty$ und daher ist $c_5$ auch wohldefiniert.
Weiterhin ist
$J_n = \Big|\mathcal{F}^{(J_n)}\Big|$ 
und in Kapitel~\ref{subsec:2.2} haben wir bereits gezeigt, dass 
$$\Big|\mathcal{F}^{(J_n)}\Big| =  (M_n + 1)^d \cdot \binom{N + d}{d} \leq (M_n + 1)^d \cdot (N + 1)^d$$
gilt.
Da $m$ nach Voraussetzung $(p,C)$-glatt ist, folgt:
\begin{equation}
\label{bound}
z \coloneqq \max_{\bi \in [M_n]^d,\, \bj \in [q]^d,\, |\bj|_1 \leq q} \big|\partial^{\bj}m(x_{\mathbf{i}})\big| < \infty,
\end{equation}
denn das größte Elemente muss auch beschränkt sein, wenn der Abstand zweier beliebiger Elemente beschränkt ist.
Sei 
$$g_n(x) = \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}}) \cdot f_{\net,\bj,\mathbf{i}}(x).$$
Da nach Konstruktion $f_{\net,\bj,\mathbf{i}} \in \mathcal{F}$ ist, folgt mit Gleichung~(\ref{constantc21}), dass für $n$ hinreichend groß
\begin{equation*}
\begin{split}
\sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \big| \frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\big|^2 & \leq (M_n + 1)^d (N + 1)^d \cdot \big|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\big|^2 \\
& \leq (2 c_2 \cdot n^{1/2p + d})^d \cdot (N + 1)^d \cdot \big|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\big|^2 \\
& \leq c_5 \cdot n,
\end{split}
\end{equation*}
gilt und damit $g_n$ in $\mathcal{F}^{(J_n)}$ liegt. 
Wir wählen das Ereignis 
\begin{equation}
\label{event}
A_n \coloneqq \bigg[\frac{1}{n} \sum_{i = 1}^n Y_i^2 \leq 1 + \E[Y^2]\bigg].
\end{equation}
Wir wissen, dass aufgrund der Unabhängigkeit und identischen Verteiltheit der $\R^d \times \R$-wertigen Zufallsvariablen  $(X, Y), (X_1, Y_1), (X_2, Y_2), \dots$ auch die Zufallsvariablen $Y_1,\dots,Y_n$ unabhängig und identisch verteilt sind. 
Daraus folgern wir $\E\big[\frac{1}{n} \sum_{i = 1}^n Y_i^2\big] = \E[Y^2]$ mit der Linearität des Erwartungswerts.
Mit Hilfe der Monotonie der Wahrscheinlichkeitsfunktion~$\mathds{P}$ und der Chebyshev-Ungleichung für $\epsilon = 1$ \cite[Satz 5.11]{Klenke2013} erhalten wir:
\begin{equation*}
\begin{split}
\mathds{P}(A_n^{\mathsf{c}}) = \mathds{P}\Big(\frac{1}{n}\sum_{i=1}^n Y_i^2 - \E[Y^2] \geq 1\Big)
\leq \mathds{P}\Big(\Big|\frac{1}{n}\sum_{i=1}^n Y_i^2 - \E[Y^2]\Big| \geq 1\Big)
\leq \mathds{V}[\frac{1}{n}\sum_{i = 1}^nY_i^2].
\end{split}
\end{equation*}
Da die Zufallsvariablen $Y_1,\dots,Y_n$ u.\@i.\@v.\@ sind, folgt mit den Rechenregeln der Varianz:
\begin{equation}
\label{tscheby}
\begin{split}
\mathds{P}(A_n^{\mathsf{c}}) \leq \frac{n \cdot  \mathds{V}[Y^2]}{n^2}
= \frac{\mathds{V}[Y^2]}{n}
= \frac{c_7}{n},
\end{split}
\end{equation}
wobei $c_7 \coloneqq \mathds{V}[Y^2]$ ist.

Sei $$\hat{m}_n \coloneqq \mathds{1}_{A_n}m_n + \mathds{1}_{A_n^{\mathsf{c}}}T_{\beta_n}g_n$$
mit $m_n= T_{\beta_n}\tilde{m}_n$.

Für zwei beliebige reelle Zahlen $u, v \in \R$ gilt durch $0 \leq (u - v)^2 = u^2 + v^2 - 2uv$:
$$u^2 + v^2 \geq 2uv$$ und damit schließlich:
\begin{equation}
\label{ungl}
\begin{split}
|(u - v)^2| & = |u^2 - 2uv + v^2|
\leq  u^2 + 2uv + v^2
\leq 2u^2 + 2v^2.
\end{split}
\end{equation}
Mit Ungleichung~(\ref{ungl}) und durch die Unabhängigkeit von $A_n$ von den Zufallsvariablen $X, X_1, \dots, X_n$ erhalten wir:
\begin{equation}
\label{anc}
\begin{split}
 \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n^{\mathsf{c}}}\bigg] & =  \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \bigg] \cdot \mathds{P}(A_n^{\mathsf{c}}) \\
 & \leq \E \bigg[\int 2m_n(x)^2 + 2m(x)^2 \mathds{P}_X(dx)\bigg] \cdot \mathds{P}(A_n^{\mathsf{c}})\\
 & \leq \E \bigg[\int  2\beta_n^2 + 2\beta_n^2 \mathds{P}_X(dx)\bigg] \cdot \mathds{P}(A_n^{\mathsf{c}})\\
 & = 4\beta_n^2 \cdot \mathds{P}(A_n^{\mathsf{c}}) \\
 & \stackrel{(\ref{tscheby})}{\leq} \frac{4 \cdot c_7 \cdot \beta_n^2}{n}.
\end{split}
\end{equation}
Da wir nach Bemerkung~\ref{mtildebeschraenkt} wissen, dass $\tilde{m}_n$ beschränkt ist, ist $m_n$ nach Konstruktion ebenfalls beschränkt. Wir haben daher bei Ungleichung~(\ref{anc}) zudem verwendet, dass wir $n$ und $c_3$ so groß wählen, dass $\max\{\|m\|_{\infty}, \|m_n\|_{\infty}\} < \beta_n$ gilt. Bei der letzten Gleichung haben wir schließlich verwendet, dass $\beta_n$ deterministisch und $\mathds{P}(X \in \supp(\mathds{P}_X)) = 1$ ist.
Durch unsere Definition von $\hat{m}_n$ erhalten wir durch die Monotonie des Erwartungswerts und einer Abschätzung über den ganzen Raum:
\begin{equation}
\label{an}
\begin{split}
\E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] & = \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] \\
& \leq \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg].
\end{split}
\end{equation}
Zusammen mit Gleichungen~(\ref{tscheby}), (\ref{anc}), (\ref{an}) und der Linearität des Erwartungswerts erhalten wir dann:
\begin{equation}
\label{originaleq}
\begin{split}
\E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg] & = \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \cdot (\mathds{1}_{A_n^{\mathsf{c}}} + \mathds{1}_{A_n})\bigg] \\
& = \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n^{\mathsf{c}}}\bigg] \\
& \quad + \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] \\
& \leq \frac{4 \cdot c_7 \cdot \beta_n^2}{n} + \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg].
\end{split}
\end{equation}
Wir zeigen nun $\tilde{m}_n \in \mathcal{F}^{(J_n)}$.

Nach Gleichung~(\ref{umschreiben}) können wir unseren Schätzer $\tilde{m}_n$ darstellen durch:
$$\tilde{m}_n(x) = \sum_{j = 1}^{J_n}\hat{a}_j \cdot f_j$$
für geeignete $f_j \in \mathcal{F}$ und $\hat{a}_j$ welche 
\begin{equation*}
\begin{split}
\frac{c_6}{n}\sum_{j = 1}^{J_n} \hat{a}_j^2 & = \frac{c_6}{n} \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q}} a_{\mathbf{i},\bj}^2
\leq \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_6}{n} \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q}} a_{\mathbf{i},\bj}^2 
\leq \sum_{i = 1}^n Y_i^2,
\end{split}
\end{equation*}
erfüllen, wobei wir bei der letzten Ungleichung wie in Kapitel~\ref{chap:2} die minimierende Eigenschaft von $a_{\mathbf{i},\bj}$ verwendet haben und zum Schluss die Koeffizienten Null gesetzt haben. Da $c_6 > 0$ ist, erhalten wir dass, die Koeffizienten $\hat{a}_j$ die Eigenschaft
$$\sum_{j = 1}^{J_n} \hat{a}_j^2  \leq \frac{1}{n}\sum_{i = 1}^n Y_i^2 \cdot \frac{n}{c_6}$$
erfüllen müssen.
Aus der Definition des Ereignisses $A_n$ in Gleichung~(\ref{event}) und der Definition der Konstante $c_5$ in Gleichung~(\ref{constantc21}) erhalten wir dann die Abschätzung
$$\sum_{j = 1}^{J_n}\hat{a}_j^2 \leq \frac{1 + \E[Y^2]}{c_6} \cdot n \leq c_5 \cdot n,$$
woraus durch $f_j \in \mathcal{F}$ schließlich $\tilde{m}_n \in \mathcal{F}^{(J_n)}$ folgt.

Daher setzen wir $\hat{m}_n \coloneqq T_{\beta_n}\bar{m}_n$ für $\bar{m}_n \in \{\tilde{m}_n, g_n\} \subseteq \mathcal{F}^{(J_n)}$.
Die Funktionen $\tilde{m}_n$ und $g_n$ unterscheiden sich in den Vorfaktoren von $f_j$. Die Koeffizienten $a_{\mathbf{i},\bj}$ von $\tilde{m}_n$ haben wir durch Minimierung des Funktionals in Gleichung~(\ref{min}) erhalten. Nach Voraussetzung ist $N \geq q$ und damit gilt dann insbesondere $\{0,\dots,q\} \subseteq \{0,\dots,N\}$. Bei der Minimierung des Funktionals hat man daher auch die Koeffizienten von $g_n$ betrachtet. Daher erhalten wir:
\begin{equation}
\begin{split}
\label{tilde}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_6}{n} \cdot \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [N]^d \\ |\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + \frac{c_6}{n} \cdot \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \bigg|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\bigg|^2 \\
& = \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + c_{8} \cdot \frac{(M_n + 1)^d}{n},
\end{split}
\end{equation}
mit $c_{8} = c_6 \cdot (q + 1)^d \cdot \big|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\big|^2$ als Konstante die unabhängig von $n$ ist.
Wir erhalten damit:
\begin{equation}
\label{bed}
\frac{1}{n} \sum_{i = 1}^n|Y_i - \bar{m}_n(X_i)|^2 \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + c_8 \cdot \frac{(M_n + 1)^d}{n},
\end{equation}
da für $\bar{m}_n = g_n$ die Ungleichung unmittelbar folgt.

Da $g_n$ nach Definition deterministisch, damit also unabhängig von $(X_1, Y_1),\dots,(X_n, Y_n)$ ist, sind mit der einelementigen Parametermenge $\Theta_n$, der Funktion $g_{n,1} = g_n$, der Abschätzung~(\ref{bed}) für $\hat{m}_n = T_{\beta_n}\bar{m}_n$ mit $\hat{m}_n \in \mathcal{F}^{(J_n)}$ und dem \emph{Penalty Term} $\mathrm{pen}_n(g_{n,1}) = c_8 \cdot \frac{(M_n + 1)^d}{n} > 0$ die Voraussetzungen für Lemma~\ref{lem:8} erfüllt. Wir erhalten durch dessen Anwendung:
\begin{equation*}
\begin{split}
& \E \int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \frac{c_8 \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (\supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \cdot \E\bigg(\min_{l \in \Theta_n} \int |g_{n,l}(x) - m(x)|^2 \mathds{P}_X(dx) + c_8 \cdot \frac{(M_n + 1)^d}{n}\bigg) \\
& = \frac{c_8 \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (\supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \int |g_{n}(x) - m(x)|^2 \mathds{P}_X(dx) + 2 \cdot c_8 \cdot \frac{(M_n + 1)^d}{n},
\end{split}
\end{equation*}
wobei wir bei der letzten Gleichheit verwendet haben, dass der letzte Summand deterministisch ist. Zudem wissen wir, dass $c_8$ unabhängig von $n$ ist und $n > 1$, da wir $n$ hinreichend groß wählen.
Als Nächstes überprüfen wir die Voraussetzungen von Lemma~\ref{lem:9}, um damit dann die letzte Gleichung mit der Überdeckungszahl $\mathcal{N}_1$weiter abzuschätzen.
Nach Voraussetzung ist $\beta_n = c_3 \cdot \log(n)$ und $a_n = (\log n)^{1/(6(N + d))} > 0$ für hinreichend großes $n$. Nach Voraussetzung sind zudem $d, N, J_n \in \N$ und es gilt nach Gleichung~(\ref{jn}): 
$$J_n \leq (M_n + 1)^d \cdot (N + 1)^d \leq n^{c},$$
für hinreichend großes $n$ und Konstante $c > 0$. Wir betrachten hier den logistischen Squasher $\sigma$ welcher nach Lemma~\ref{lem:logsquasher} insbesondere 2-zulässig ist. Da die hier betrachtete Menge von Funktionen $\mathcal{F}^{(J_n)}$ identisch mit der aus Lemma~\ref{lem:9} ist, sind nun alle Voraussetzungen für Lemma~\ref{lem:9} erfüllt. Nach Voraussetzung wissen wir, dass $\supp(\mathds{P}_X)$ beschränkt ist, und wir können $n$ so groß wählen, dass wir ohne Beschränkung der Allgemeinheit annehmen können, dass $\supp(\mathds{P}_X) = \{x \in \R^d \mid \forall \epsilon > 0 : \mathds{P}_X(S_{\epsilon}(x)) > 0\} \subseteq [-a_n, a_n]^d$ ist, mit $S_{\epsilon}$ als $\epsilon$-Umgebung um $x \in \R^d$. In der nächsten Ungleichungskette fassen wir die Konstanten die sich von $c_8$ unterscheiden und die unabhängig von $n$ sind, zu einer Konstante $c$ zusammen. Wir erhalten damit durch Lemma~\ref{lem:9} für hinreichend großes $n$:
\begin{equation}
\label{lem9sol}
\begin{split}
& \frac{c_8 \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (\supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \leq \frac{c_8 \cdot \log(n)^2 \cdot \big(\big(c \cdot \log(n) \cdot (M_n + 1)^d \cdot (N + 1)^d \big) + 1\big)}{n} \\
& \leq \frac{c_8 \cdot \log(n)^2 \cdot \big(2 \cdot c \cdot \log(n) \cdot (M_n + 1)^d \cdot (N + 1)^d\big)}{n} \\
& \leq c_{9} \cdot \frac{\log(n)^3 \cdot (M_n + 1)^d \cdot (N + 1)^d}{n},
\end{split}
\end{equation}
wobei $c_9 \coloneqq c_8 \cdot c$ eine von $n$ unabhängige Konstante ist.
Sei 
\begin{equation*}
\begin{split}
P_{m,n}(x) \coloneqq \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj} m (x_{\mathbf{i}}) \cdot (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d\Big(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\Big)_+. 
\end{split}
\end{equation*}
Mit Ungleichung~(\ref{ungl}), zusammen mit einer Nulladdition und der Linearität des Integrals erhalten wir:
\begin{equation*}
\begin{split}
& \int |g_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = \int |g_n(x) - P_{m,n}(x) + P_{m,n}(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \int 2 |g_n(x) - P_{m,n}(x)|^2 + 2 |P_{m,n}(x) - m(x)|^2 \mathds{P}_X(dx).
\end{split}
\end{equation*}
Aus der Supremumseigenschaft folgt
\begin{equation}
\label{gnmx}
\begin{split}
& \int |g_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq 2 \int \sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_{m,n}(x)|^2 \mathds{P}_X(dx) + 2 \int \sup_{x \in [-a_n, a_n]^d} |P_{m,n}(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = 2 \sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_{m,n}(x)|^2 + 2 \sup_{x \in [-a_n, a_n]^d} |P_{m,n}(x) - m(x)|^2, 
\end{split}
\end{equation}
wobei wir im letzten Schritt $\supp(\mathds{P}_X) \subseteq [-a_n, a_n]^d$ und $\mathds{P}(X \in \supp(\mathds{P}_X)) = 1$ verwendet haben. Um die letzten beiden Summanden der Ungleichung~(\ref{gnmx}) weiterhin abzuschätzen möchten wir Lemma~\ref{lem:5} anwenden. Dafür überprüfen wir, ob dafür alle Voraussetzungen erfüllt sind. Wir betrachten wieder den logistischen Squasher $\sigma$ aus Gleichung~\ref{logsquasher}, welcher nach Lemma~\ref{lem:logsquasher} insbesondere 2-zulässig ist. Zudem ist für hinreichend großes $n$ die Bedingung 
\begin{equation*}
\begin{split}
R_n & \geq \max\biggl\{\frac{\|\sigma''\|_{\infty} \cdot (M_n + 1)}{2 \cdot |\sigma'(t_{\sigma})|}, \frac{9 \cdot \|\sigma''\|_{\infty} \cdot a_n}{|\sigma'(t_{\sigma})|}, \\
& \quad \frac{20 \cdot \|\sigma'''\|_{\infty}}{3 \cdot |\sigma''(t_{\sigma})|} \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s}, 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty},\|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M_n^3 \biggr\}
\end{split}
\end{equation*}
erfüllt und da unser neuronales Netz (\ref{fnet}) mit $x_{\mathbf{i}} \in [-a_n, a_n]^d$ identisch mit der Definition aus Lemma~\ref{lem:5} ist, sind alle Voraussetzungen für Lemma~\ref{lem:logsquasher} erfüllt.
Wir erhalten damit für $x \in [-a_n ,a_n]^d$ und $n$ hinreichend groß:
\begin{equation}
\label{vorbereitung}
\begin{split}
\bigg| f_{\net,\bj,\mathbf{i}}(x) - (x - x_{\mathbf{i}})^{\bj} \cdot \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \bigg| & \leq c \cdot 3^{3 \cdot 3^s} \cdot a_n^{3 \cdot 2^s} \cdot M_n^3 \cdot \frac{1}{R_n} \\
& \leq c\cdot a_n^{3 \cdot (N + d) \cdot 2} \cdot \frac{M_n^3}{R_n} \\
& = c\cdot \log(n) \cdot \frac{M_n^3}{R_n},  
\end{split}
\end{equation}
wobei wir in $c$ alle von $n$ unabhängigen Konstanten zusammenfassen, welche insbesondere die Konstante aus Lemma~\ref{lem:logsquasher} enthält.
Wir haben unter anderem verwendet, dass für hinreichend großes $n$:
$$a_n^{2^{\lceil\log_2(N + d)\rceil}} \leq a_n^{2^{\log_2(N + d) + 1}} = a_n^{(N + d) \cdot 2},$$ gilt. Im letzten Schritt haben wir in Ungleichung~(\ref{1stsum}) die Definition von $a_n$ eingesetzt. 
 Mit Ungleichung~(\ref{vorbereitung}) erhalten wir nun:
\begin{equation}
\label{1stsum}
\begin{split}
& |g_n(x) - P_{m,n}(x)| \\
& = \bigg| \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj} m (x_{\mathbf{i}}) \bigg| \cdot \bigg| f_{\net,\bj,\mathbf{i}}(x) - (x - x_{\mathbf{i}})^{\bj} \cdot \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \bigg| \\
& \stackrel{(\ref{bound})}{\leq} (M_n + 1)^d \cdot (q + 1)^d \cdot z \cdot \bigg| f_{\net,\bj,\mathbf{i}}(x) - (x - x_{\mathbf{i}})^{\bj} \cdot \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \bigg| \\
& \leq  (M_n + 1)^d \cdot (q + 1)^d \cdot c\cdot \log(n) \cdot \frac{M_n^3}{R_n},
\end{split}
\end{equation}
wobei wir in $c$ alle von $n$ unabhängigen Konstanten zusammenfassen, welche insbesondere die Konstante aus Lemma~\ref{lem:logsquasher} enthält.
Da nach Konstruktion $a_n > 0$, $m$ $(p, C)$-glatt und $P_{m,n}(x)$ nach Lemma~\ref{lem:loccon} eine Spline Interpolation von Taylorpolynomen von $m$ ist, erhalten wir mit Lemma~\ref{lem:pcsmooth}:
\begin{equation}
\label{2ndsum}
\begin{split}
|P_{m,n}(x) - m(x)| \leq c_{11} \cdot \frac{a_n^p}{M_n^p} \leq c_{11} \cdot \log(n) \cdot \frac{1}{M_n^p}.
\end{split}
\end{equation}
In dieser Ungleichung ist $c_{11} > 0$ eine von $n$ unabhängige Konstante aus Lemma~\ref{lem:pcsmooth} und wir haben zudem verwendet, dass:
$$a_n^p = a_n^{q + s} \leq a_n^{N + d} \leq a_n^{6 \cdot (N + d)} = \log(n),$$
für $p = q + s$ für hinreichend großes $n$ gilt, da nach Voraussetzung $N \geq q$ und $d \geq s$ mit $s \in (0, 1]$ ist.
Durch Quadrieren bleiben die Ungleichungen~(\ref{1stsum}) und (\ref{2ndsum}) auch erhalten und da in beiden Ungleichungen die rechte Seite unabhängig von $x$ ist, gelten die Ungleichungen ebenfalls für das Supremum über $x \in [-a_n,a_n]^d$. Durch Einsetzen der Definitionen von $M_n$ und $R_n$ erhalten wir für $n$ hinreichend groß:
\begin{equation}
\label{3rdsum}
\begin{split}
\sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_{m,n}(x)|^2 & \leq \bigg((M_n + 1)^d \cdot (q + 1)^d \cdot c \cdot \log(n) \cdot \frac{M_n^3}{R_n}\bigg)^2 \\
& \leq c^2 \cdot (M_n + 1)^{2d} \cdot \log(n)^2 \cdot \frac{M_n^6}{R_n^2} \\
& \leq  c_{10}^2 \cdot (M_n + 1)^{2d} \cdot \log(n)^2 \cdot \frac{(M_n + 1)^{6d}}{R_n^2} \\
& \leq  c^2 \cdot \log(n)^2 \cdot \frac{(M_n + 1)^{8d}}{R_n^2} \\
& \leq c_{10} \cdot \frac{n^{\frac{8d}{2p + d}}}{n^{2d + 8}} \cdot \log(n)^2 \\
& = c_{10} \cdot n^{\frac{8d}{2p + d} - 2d -8} \cdot \log(n)^2 \\
& \leq c_{10} \cdot n^{\frac{8d}{2p + d}  -8\frac{2p + d}{2p + d}} \cdot \log(n)^2 \\
& = c_{10} \cdot n^{-\frac{16p}{2p + d}} \cdot \log(n)^2 \\
& \leq c_{10} \cdot n^{-\frac{2p}{2p + d}} \cdot \log(n)^3,
\end{split}
\end{equation}
mit $c_{10} = c^2 \cdot (2c_2)^{8d} > 0$ und unabhängig von $n$ ist. Bei der letzten Ungleichung haben wir verwendet, dass $\frac{16p}{2p + d} > \frac{2p}{2p + d}$, da $p > 0$ ist und $\log(n)^2 < \log(n)^3$ für $n$ hinreichend groß gilt. Ebenfalls erhalten wir:
\begin{equation}
\label{4thsum}
\begin{split}
\sup_{x \in [-a_n, a_n]^d} |P_{m,n}(x) - m(x)|^2 & \leq \bigg(c_{11} \cdot \log(n) \cdot \frac{1}{M_n^p}\bigg)^2 \\
& \leq c_{11}^2 \cdot \log(n)^2 \cdot c_{2}^{-2p} \cdot n^{-\frac{2p}{2p + d}} \\
& \leq \bigg(\frac{c_{11}}{c_2^p}\bigg)^2 \cdot \log(n)^3 \cdot n^{-\frac{2p}{2p + d}}.
\end{split}
\end{equation}
Mit analogem Vorgehen erhalten wir für Ungleichung~(\ref{lem9sol}):
\begin{equation}
\begin{split}
& \frac{c_8 \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (\supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \leq c_9 \cdot \frac{\log(n)^3 \cdot (M_n + 1)^d \cdot (N + 1)^d}{n} \\
& \leq c_9 \cdot (N+1)^d \cdot \log(n)^3 \cdot \frac{(c_2 \cdot n^{\frac{1}{2p + d}} + 2)^d}{n} \\
& \leq c_9 \cdot (N+1)^d \cdot 2^d \cdot \log(n)^3 \cdot \frac{c_2^d\cdot n^{\frac{d}{2p + d}}}{n} \\
& = c_9 \cdot (N+1)^d \cdot 2^d \cdot c_2^d \cdot \log(n)^3 \cdot n^{\frac{d}{2p + d} - 1} \\
& = c_{12} \cdot \log(n)^3 \cdot n^{-\frac{2p}{2p + d}},
\end{split}
\end{equation}
mit einer von $n$ unabhängigen Konstante $c_{12} \coloneqq c_9 \cdot (N + 1)^d \cdot 2^d \cdot c_2^d > 0.$
Zudem erhalten wir, da für $n$ hinreichend groß $\log(n)^3 > 1$ gilt, mit analogem Vorgehen:
\begin{equation}
\label{5thsum}
\begin{split}
c_8 \cdot \frac{(M_n + 1)^d}{n} & \leq c_{13} \cdot \frac{n^{\frac{d}{2p + d}}}{n} \leq c_{13} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}} 
\end{split}
\end{equation} 
und 
\begin{equation}
\label{6thsum}
\begin{split}
\frac{4 \cdot c_7 \cdot \beta_n^2}{n} & \leq c_{14} \cdot \log(n)^3 \cdot n^{-1} \leq c_{14} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}},
\end{split}
\end{equation} 
mit von $n$ unabhängigen Konstanten $c_{13} \coloneqq c_8 \cdot (2c_2)^d > 0$ und $c_{14} \coloneqq 4 \cdot c_7 \cdot c_3^2 > 0$.
Nun haben wir alle Summanden von Ungleichung ($\ref{originaleq}$) abgeschätzt und erhalten schließlich mit Ungleichungen (\ref{1stsum}) - (\ref{6thsum}):
\begin{equation*}
\E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \leq c_{\mathrm{fin}} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}},
\end{equation*}
mit 
$$c_{\mathrm{fin}} = c_{14} + 2 \cdot c_{13} + c_{12} + 2 \cdot \Bigg(2 \cdot \bigg(\frac{c_{11}}{c_2^p}\bigg)^2 + 2 \cdot  c_{10}\Bigg),$$
wobei $c_{\mathrm{fin}}$ als Summe nichtnegativer und positiver Konstanten, die unabhängig von $n$ sind, nichtnegativ und unabhängig von $n$ ist.
Damit haben wir unser Hauptresultat bewiesen. $\hfill\square$ 	