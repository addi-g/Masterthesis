\chapter{Resultat zur Konvergenzgeschwindigkeit}
\label{chap:3}

In diesem Kapitel stellen wir das Hauptresultat dieser Arbeit vor.
Ziel im Folgenden ist es, eine Abschätzung des erwarteten $L_2$-Fehlers 
$$\E \int |m_n(x) - m(x)|^2  \mathds{P}_X(dx)$$
im Falle unseres Neuronale-Netze-Regresssionsschätzers~\ref{estimate} mit einer $(p,C)$-glatten Regressionsfunktion herzuleiten.

\begin{thm}\label{optstop}
Angenommen die Verteilung von $(X,Y)$ erfüllt 
$$ \E\Big(\mathrm{e}^{c_4 \cdot |Y|^2}\Big) < \infty$$
für eine Konstante $c_4 > 0$ und die Verteilung von $X$ hat einen beschränkten Träger $supp(\mathds{P}_X)$. Sei $m(x) = \E[Y \mid X = x]$ die entsprechende Regressionsfunktion. 
Angenommen $m$ ist $(p,C)$-glatt, mit $p = q + s$ für $q \in \N_0$ und $s \in (0,1].$ Wir betrachten unseren neuronale Netze Regressionsschätzer $\tilde{m}_n$ aus \ref{estimate}, wobei $\sigma$ der logistische squasher ist und $N \geq q, M = M_n = \lceil c_5 \cdot n^{1/(2p + d)}\rceil, R = R_n = n^{d + 4}$ und $a = a_n = (\log n)^{1/(6(N + d))}.$
Sei $\beta_n = c_6 \cdot \log(n)$ für eine hinreichend große Konstante $c_6 > 0$ und sei $m_n$ gegeben durch
$$m_n(x) = T_{\beta_n}\tilde{m}_n (x)$$
mit $T_{\beta}z = \max\{\min\{z, \beta\}, -\beta\}$ für $z \in \R$ und $\beta > 0.$ Dann erhalten wir für hinreichend großes $n$:
$$\E \int |m_n(x) - m(x)|^2  \mathds{P}_X(dx) \leq c_7 \cdot (\log n)^3 \cdot n^{- \frac{2p}{2p + d}},$$
wobei $c_7 > 0$ ist und nicht von $n$ abhängt.
\end{thm}
Die nächsten Lemmata benötigen wir für den Beweis unseres Hauptresultats, einer Aussage über die Konvergenzgeschwindigkeit unseres Neuronale-Netze-Schätzers. Diese Lemmata werden hier nur der Vollständigkeit halber und ohne Beweis aufgeführt.
\begin{lem}[\cite{kohler19}, Lemma 5]
\label{lem:5}
Sei $M \in \N$ und $\sigma\colon \R \to [0, 1]$ $2$-zulässig nach Definition~\ref{nzulässig}.
Sei $a \geq 1$ und 
\begin{equation*}
\begin{split}
R & \geq \max\biggl\{\frac{\|\sigma''\|_{\infty} \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma, \id})|}, \frac{9 \cdot \|\sigma''\|_{\infty} \cdot a}{|\sigma'(t_{\sigma, \id}|}, \\
& \quad \frac{20 \cdot \|\sigma'''\|_{\infty}}{3 \cdot |\sigma''(t_{\sigma})|} \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s}, 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty},\|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma, \id})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \biggr\}
\end{split}
\end{equation*}
und sei $y \in [-a, a]^d.$ Sei $N \in \N$ und $\bj \in [\N_0]^d$ so, dass $|\bj|_1 \leq N$ gilt und wir setzen $s = \lceil\log_2(N + d)\rceil$. Sei $f_{\id}, f_{\mult}$ und $f_{\mathrm{hat}, z}$ (für $z \in \R$) die neuronalen Netze aus Lemma~\ref{lem:1}, Lemma~\ref{lem:2} und Lemma~\ref{lem:4}. Wir definieren das Netz $f_{net,\bj,y}$ durch:
\begin{align*}
\label{fnet}
& f_{net,\bj,y}(x) = f_1^{(0)}(x). \\
\intertext{wobei} 
& f_k^{(l)}(x) = f_{\mult}\Big(f_{2k - 1}^{(l + 1)}(x),f_{2k}^{(l + 1)}(x)\Big) \\
\intertext{für $1 \leq k \leq 2^l$ und $0 \leq l \leq s - 1$ und} 
& f_k^{(s)}(x) = f_{\id}(f_{\id}(x^{(l)} - y^{(l)}))  \\
\intertext{für $j_1 + j_2 + \dots + j_{l-1} + 1 \leq k \leq j_1 + j_2 + \dots + j_l$ und $1 \leq l \leq d$ und} 
& f_{j_1 + j_2 + \dots + j_d + k}^{(s)}(x) = f_{\mathrm{hat},y^{(k)}}(x^{(k)}) \\
\intertext{für $1 \leq k \leq d$ und} 
& f_k^{(s)}(x) = 1 \\
\intertext{für $j_1 + j_2 + \dots + j_d + d + 1 \leq k \leq 2^s.$}
\end{align*} 
Dann erhalten wir für $x \in [-a, a]^d$:
\begin{equation*}
\begin{split}
& \bigg|f_{net,\bj,y}(x) - (x^{(1)} - y^{(1)})^{j_1} \cdots (x^{(d)} - y^{(d)})^{j_d} \prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - y^{(j)}|)_+\bigg| \\
& \leq c \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*}
für eine Konstante $c > 0$.
\end{lem}
  \begin{lem}[\cite{kohler19}, Lemma 8]
  \label{lem:8}
Sei $\beta_n = c_6 \cdot \log(n)$ für eine hinreichend große Konstante $c_6 > 0$. Angenommen die Verteilung von $(X, Y)$ erfüllt 
$$ \E\Big(\mathrm{e}^{c_4 \cdot |Y|^2}\Big) < \infty$$
für eine Konstante $c_4 > 0$ und dass der Betrag der Regressionsfunktion $m$ beschränkt ist. Sei $\mathcal{F}_n$ eine Menge von Funktionen $f\colon \R^d \to \R$ und wir nehmen an, dass der Schätzer $m_n$ 
$$m_n = T_{\beta_n}\tilde{m}_n$$ 
erfüllt, mit 
$$\tilde{m}_n(\cdot) = \tilde{m}_n(\cdot,(X_1, Y_1),\dots,(X_n, Y_n)) \in \mathcal{F}_n$$
und 
$$\frac{1}{n} \sum_{i = 1}^n |Y_i - \tilde{m}_n(X_i)|^2 \leq \min_{l \in \Theta_n}\bigg(\frac{1}{n}\sum_{i = 1}^n |Y_i - g_{n,l}(X_i)|^2 + pen_n(g_n,l)\bigg)$$
mit einer nichtleeren Parametermenge $\Theta_n$, zufällige Funktionen $g_{n,l}\colon \R^d \to \R$ und deterministischen penalty Terme $pen_n(g_{n,l}) \geq 0$, wobei die zufälligen Funktionen $g_{n,l}\colon \R^d \to \R$ nur von den Zufallsvariablen
$$\mathbf{b}_1^{(1)},\dots,\mathbf{b}_r^{(1)},\dots,\mathbf{b}_1^{(I_n)},\dots,\mathbf{b}_r^{(I_n)},$$
abhängen, die unabhängig von $(X_1, Y_1), (X_2, Y_2),\dots$ sind.
Dann erfüllt $m_n\colon$
\begin{equation*}
\begin{split}
& \E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \frac{c_{13} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \cdot \E\bigg(\min_{l \in \Theta_n} \int |g_{n,l}(x) - m(x)|^2 \mathds{P}_X(dx) + pen_n(g_{n,l})\bigg),
\end{split}
\end{equation*}
für $n > 1$ und einer Konstante $c_{13} > 0$ welche nicht von $n$ abhängt.
  \end{lem}
Das nächste Lemma benötigen wir um eine Schranke für die Überdeckungszahl $\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)$ zu finden.
\begin{lem}[\cite{kohler19}, Lemma 9]
\label{lem:9}
Sei $a > 0$ und $d, N, J_n \in \N$ so, dass $J_n \leq n^{c_{14}}$ und setze $\beta_n = c_6 \cdot \log(n).$
Sei $\sigma$ 2-zulässig nach Definition~\ref{nzulässig}. Sei $\mathcal{F}$ die Menge aller Funktionen die durch (\ref{networkarch}) definiert sind mit $k_1 = k_2 = \cdots = k_L = 24 \cdot (N + d)$ und dass der Betrag der Gewichte durch $c_{15} \cdot n^{c_{16}}$ beschränkt ist. Sei
$$ \mathcal{F}^{(J_n)} = \biggl\{\sum_{j = 1}^{J_n} a_j \cdot f_j : f_j \in \mathcal{F} \quad \text{und} \quad \sum_{j = 1}^{J_n} a_j^2 \leq c_{17} \cdot n^{c_{18}}\biggr\}.$$
Dann gilt für $n > 1:$
$$\log\bigg(\sup_{x_1^n\in[-a,a]^{d \cdot n}} \mathcal{N}_1\bigg(\frac{1}{n \cdot \beta_n}, \mathcal{F}^{(J_n)},x_1^n\bigg)\bigg) \leq c_{19} \cdot \log(n) \cdot J_n,$$
für eine Konstante $c_{19}$ die nur von $L, N, a$ und $d$ abhängt.
\end{lem}
Mit diesen Hilfsresultaten können wir nun unser Hauptresultat beweisen.
\begin{proof}[Beweis von Satz~\ref{optstop}]
Nach Voraussetzung wissen wir, dass $supp(\mathds{P}_X)$ beschränkt ist, daher nehmen wir ohne Beschränkung der Allgemeinheit an, dass $supp(X) = \{x \mid \mathds{P}_X(x) > 0\} \subseteq [-a_n, a_n]^d$ ist, da $\mathds{P}(X \in supp\mathds{P}_X)) = 1$ gilt und wir ansonsten die Zufallsvariable $X$ auf Nullmengen abändern können. Zudem haben wir angenommen, dass $m$ $(p,C)$-glatt und damit insbesondere hölderstetig mit $q = 0$ ist. Daraus können wir auf die gleichmäßige Stetigkeit von $m$ schließen \cite{Storch2018}. Da wir nur über den beschränkten $supp(\mathds{P}_X)$ integrieren, wissen wir, dass $m$ als gleichmäßig stetige Funktion auf einer beschränkten Menge auch beschränkt ist \cite{Storch2018}. Wir können daher ohne Beschränkung der Allgemeinheit folgern, dass $\|m\|_{\infty} \leq \beta_n$ ist, da aufgrund der Beschränktheit von $m$, ebenfalls $|m|$ beschränkt ist und wir daher ansonsten eine Skalierung von $m$ nehmen können.

Sei $\mathcal{F}$ die durch \ref{networkarch} definierte Menge von Funktionen mit $L = s + 2 = \lceil\log_2(N + d)\rceil + 2,$ mit $k_1 = k_2 = \cdots = k_L = 24 \cdot (N + d)$ und der Eigenschaft, dass der Betrag der Gewichte durch $n^{c_{20}}$ beschränkt ist. Sei 
$$ \mathcal{F}^{(J_n)} = \biggl\{\sum_{j = 1}^{J_n} a_j \cdot f_j \mid f_j \in \mathcal{F} \quad \text{und} \quad \sum_{j = 1}^{J_n} a_j^2 \leq c_{21} \cdot n \biggr\}$$
wobei $c_{21}$ in (\ref{constantc21}) gewählt wird und
$$J_n = (M_n + 1)^d \cdot \big|\{\bj \mid \bj \in [N]^d, |\bj|_1 \leq N\}\big|,$$ 
die Kardinalität der Menge $\mathcal{F}^{(J_n)}$ ist. 
Ohne die Restriktion $|\bj|_1 \leq N$ lässt sich $$\big|\{\bj \mid \bj \in [N]^d\}\big|$$ durch eine Analogie zu  einem Urnenexperiment bestimmten. Wir betrachten die Anzahl an Möglichkeiten, wie man $d$-Mal mit Zurücklegen (da auch mehrere Komponenten den gleichen Wert haben können) und mit Beachtung der Reihenfolge (da wir einen Vektor betrachten und die Komponenten nicht vertauschen können) aus $(N + 1)$ Kugeln ziehen kann. Durch das Weglassen der Restriktion $|\bj|_1 \leq N$ erhalten wir:
\begin{equation}
\label{jn}
J_n \leq (M_n + 1)^d \cdot (N + 1)^d.
\end{equation}
Da nach Voraussetzung $m$ $(p,C)$-glatt ist, und $x_{\mathbf{i}}   \in \R^d$ für alle $\bi \in [M]^d$, folgt:
\begin{equation}
\label{bound}
\max_{\bi \in [M]^d,\, \bj \in [q]^d,\, |\bj|_1 \leq q} \big|\partial^{\bj}m(x_{\mathbf{i}})\big| < \infty,
\end{equation}
denn das größte Elemente muss auch beschränkt sein, wenn der Abstand zweier beliebiger Elemente beschränkt ist.
Da wir uns in der nichtparametrischen Regressionsschätzung befinden und dafür als Bedingung $\E[Y^2] < \infty$ gelten muss, wählen wir mit dem bisher gezeigten:
\begin{equation}
\label{constantc21}
\begin{split}
c_{21} = \max\Biggl\{\frac{1 + \E[Y^2]}{c_3}, & (N + 1)^d \cdot \max\biggl\{\bigg| \frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\bigg|^2 \mid  \bj \in [q]^d,\, |\bj|_1 \leq q\biggr\}\Biggr\}.
\end{split}
\end{equation}
Sei 
$$g_n(x) = \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}}) \cdot f_{\net,\bj,\mathbf{i}}(x).$$
Da nach Konstruktion $f_{\net,\bj,\mathbf{i}} \in \mathcal{F}$ ist, folgt mit (\ref{constantc21}), dass $g_n$ in $\mathcal{F}^{(J_n)}$ liegt. 
Sei $A_n$ das Event, dass 
\begin{equation}
\label{event}
\frac{1}{n} \sum_{i = 1}^n Y_i^2 \leq 1 + \E[Y^2]
\end{equation}
gilt.
Wir wissen, dass aufgrund der Unabhängigkeit der $\R^d \times \R$-wertigen Zufallsvariablen  $(X, Y), (X_1, Y_1), (X_2, Y_2), \dots$ mit 
\begin{equation*}
\begin{split}
\mathds{P}(Y_1 \in \R, \dots, Y_n \in R) & = \mathds{P}((X_1, Y_1) \in \R^d \times \R, \dots, (X_n, Y_n) \in \R^d \times \R) \\
& = \mathds{P}((X_1, Y_1) \in \R^d \times \R) \cdots \mathds{P}((X_n, Y_n) \in \R^d \times \R) \\
& = \mathds{P}(Y_1 \in \R) \cdots \mathds{P}(Y_n \in R) ,
\end{split}
\end{equation*}
und durch
\begin{equation*}
\begin{split}
\mathds{P}(Y_1 \in \R, \dots, Y_n \in R) & = \mathds{P}((X_1, Y_1) \in \R^d \times \R, \dots, (X_n, Y_n) \in \R^d \times \R) \\
& = \mathds{P}((X_1, Y_1) \in \R^d \times \R) \cdots \mathds{P}((X_n, Y_n) \in \R^d \times \R) \\
& = \mathds{P}((X, Y) \in \R^d \times \R)^n \\
& = \mathds{P}(Y \in \R)^n
\end{split}
\end{equation*}
dass die Zufallsvariablen $Y_1,\dots,Y_n$ auch unabhängig und identisch verteilt sind. Daraus folgern wir $\E\big[\frac{1}{n} \sum_{i = 1}^n Y_i^2\big] = \E[Y^2]$ mit der Linearität des Erwartungswerts.
Mit Hilfe der Monotonie und Homogenität der Wahrscheinlichkeitsfunktion $\mathds{P}$ und der Chebyshev'sche Ungleichung für $\epsilon = 1$ (\cite{Klenke2013}, Satz 5.11) erhalten wir:
\begin{equation}
\label{tscheby}
\begin{split}
\mathds{P}(A_n^{\mathsf{c}}) & = \mathds{P}(\frac{1}{n}\sum_{i=1}^n Y_i^2 - \E[Y^2] \geq 1) \\
& \leq \mathds{P}(\Big|\frac{1}{n}\sum_{i=1}^n Y_i^2 - \E[Y^2]\Big| \geq 1) \\
& \leq \mathds{V}[\frac{1}{n}\sum_{i = 1}^nY_i^2] \\
& = \frac{n \cdot  \mathds{V}[Y^2]}{n^2} \\
& = \frac{\mathds{V}[Y^2]}{n} \\
& \leq \frac{c_{22}}{n},
\end{split}
\end{equation}
wobei wir bei der letzten Gleichheit die identische Verteiltheit der $Y_1,\dots,Y_n$ und Rechenregeln für die Varianz verwendet haben welche wir unter anderem aufgrund der Unabhängigkeit der $Y_1,\dots,Y_n$ verwenden durften.
Sei $\hat{m}_n = T_{\beta_n}\tilde{m}_n = m_n$ im Falle dass Ereignis $A_n$ gilt und andernfalls $\hat{m}_n = T_{\beta_n}g_n$. Durch die Unabhängigkeit von $A_n$ zu den Zufallsvariablen $X, X_1, \dots, X_n$ und der Jensenschen Ungleichung (\cite{Klenke2013}, Satz $7.9$) erhalten wir:
\begin{equation}
\label{anc}
\begin{split}
 \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n^{\mathsf{c}}}\bigg] & =  \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \bigg] \cdot \mathds{P}(A_n^{\mathsf{c}}) \\
 & \leq \E \bigg[2m_n(x)^2 + 2m(x)^2 \mathds{P}_X(dx)\bigg] \cdot \mathds{P}(A_n^{\mathsf{c}})\\
 & \leq \E \bigg[2\beta_n^2 + 2\beta_n^2 \mathds{P}_X(dx)\bigg] \cdot \mathds{P}(A_n^{\mathsf{c}})\\
 & = 4\beta_n^2 \cdot \mathds{P}(A_n^{\mathsf{c}}),
\end{split}
\end{equation}
wobei wir bei der letzten Ungleichung verwendet haben dass wir anfangs angenommen haben, dass $\|m\|_{\infty} < \beta_n$ und damit für $c_6$ und $n$ hinreichend groß auch $\tilde{m}_n \leq \beta_n$ und nach der Definition von $m_n$ zudem $m_n \leq \beta_n$ gilt. Bei der letzten Gleichung habe wir dann schließlich noch verwendet dass $\beta_n$ deterministisch und $\mathds{P}(X \in supp(\mathds{P}_X)) = 1$ ist.
Durch unsere Definition von $\hat{m}_n$ erhalten wir durch die Monotonie des Erwartungswert und der Abschätzung durch den ganzen Raum:
\begin{equation}
\label{an}
\begin{split}
\E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] & = \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] \\
& \leq \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg].
\end{split}
\end{equation}
Zusammen mit (\ref{tscheby}), (\ref{anc}), (\ref{an}) und der Linearität des Erwartungswerts erhalten wir dann:
\begin{equation}
\label{originaleq}
\begin{split}
\E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) & = \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \cdot (\mathds{1}_{A_n^{\mathsf{c}}} + \mathds{1}_{A_n})\bigg] \\
& = \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n^{\mathsf{c}}}\bigg] \\
& \quad + \E \bigg[\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \mathds{1}_{A_n}\bigg] \\
& \leq 4\beta_n^2 \cdot \mathds{P}(A_n^{\mathsf{c}}) + \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg] \\
& \leq \frac{4 \cdot c_{22} \cdot \beta_n^2}{n} + \E \bigg[\int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx)\bigg].
\end{split}
\end{equation}
Nach (\ref{estimate}) können wir unseren Schätzer $\tilde{m}_n$ darstellen durch:
$$\tilde{m}_n(x) = \sum_{j = 1}^{J_n}\hat{a}_j \cdot f_j$$
für geeignete $f_j \in \mathcal{F}$ und $\hat{a}_j$ welche 
\begin{equation*}
\begin{split}
\frac{c_3}{n}\sum_{j = 1}^{J_n} \hat{a}_j^2 & = \frac{c_3}{n} \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q}} a_{\mathbf{i},\bj}^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q}} a_{\mathbf{i},\bj}^2 \\
& \leq \sum_{i = 1}^n Y_i^2,
\end{split}
\end{equation*}
erfüllen, wobei wir bei der letzten Ungleichung wie in (\ref{min}) die minimierende Eigenschaft von $a_{\mathbf{i},\bj}$ verwendet haben und zum Schluss die Koeffizienten Null gesetzt haben. Da $c_3 > 0$ ist, erhalten wir dass die Koeffizienten $\hat{a}_j$ die Eigenschaft
$$\sum_{j = 1}^{J_n} \hat{a}_j^2  \leq \frac{1}{n}\sum_{i = 1}^n Y_i^2 \cdot \frac{n}{c_3}$$
erfüllen müssen.
Auf $A_n$ erhalten wir dann:
$$\sum_{j = 1}^{J_n}\hat{a}_j^2 \stackrel{(\ref{event})}{\leq} \frac{1 + \E[Y^2]}{c_3} \cdot n \stackrel{(\ref{constantc21})}{\leq} c_{21} \cdot n,$$
woraus durch $f_j \in \mathcal{F}$ dann $\tilde{m}_n \in \mathcal{F}^{(J_n)}$ folgt.
Deswegen nehmen wir nun ohne Beschränkung der Allgemeinheit $\hat{m}_n = T_{\beta_n}\bar{m}_n$ für $\bar{m}_n \in \{\tilde{m}_n, g_n\} \subseteq \mathcal{F}^{(J_n)}$ an.
Da $c_3 > 0$ ist, erhalten wir:
\begin{equation}
\begin{split}
\label{g}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q}} \bigg|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\bigg|^2 \\
& \stackrel{(\ref{constantc21})}{\leq} \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 +  \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} c_{21} \\
& = \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + \frac{c_3 \cdot c_{21} \cdot (M_n + 1)^d}{n} \\
& = \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + c_{23} \cdot \frac{(M_n + 1)^d}{n}.
\end{split}
\end{equation}
Die Funktionen $\tilde{m}_n$ und $g_n$ unterscheiden sich in den Vorfaktoren von $f_j$. Da wir die Koeffizienten $a_{\mathbf{i},\bj}$ von $\tilde{m}_n$ durch Minimierung von (\ref{min}) erhalten haben und nach Voraussetzung $N \geq q$ ist, damit dann $\{0,\dots,q\} \subseteq \{0,\dots,N\}$ und wir bei der Minimierung daher auch insbesondere die Koeffizienten von $g_n$ betrachtet haben, erhalten wir:
\begin{equation}
\begin{split}
\label{tilde}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{ \bj \in [N]^d \\ |\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2 \\
& \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \bigg|\frac{1}{\bj!} \cdot \partial^{\bj}m(x_{\mathbf{i}})\bigg|^2 \\
& = \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + c_{23} \cdot \frac{(M_n + 1)^d}{n}
\end{split}
\end{equation}
Mit (\ref{g}) und (\ref{tilde}) erhalten wir zusammen:
\begin{equation}
\label{bed}
\frac{1}{n} \sum_{i = 1}^n|Y_i - \bar{m}_n(X_i)|^2 \leq \frac{1}{n} \sum_{i = 1}^n|Y_i - g_n(X_i)|^2 + c_{23} \cdot \frac{(M_n + 1)^d}{n}.
\end{equation}
Da $g_n$ nach Definition deterministisch, damit also unabhängig von $(X_1, Y_1), (X_2, Y_2),\dots$ ist, sind mit $|\Theta_n| = 1$, $g_{n,1} = g_n$, der Abschätzung (\ref{bed}) für $\hat{m}_n = \T_{\beta_n}\bar{m}_n$ mit $\hat{m}_n \in \mathcal{F}^{(J_n)}$ und dem penalty Term $pen_n(g_{n,1}) = c_{23} \cdot \frac{(M_n + 1)^d}{n} > 0$ die Voraussetzungen für Lemma~\ref{lem:8} erfüllt und wir erhalten durch dessen Anwendung:
\begin{equation*}
\begin{split}
& \E \int |\hat{m}_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \frac{c_{23} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \cdot \E\bigg(\min_{l \in \Theta_n} \int |g_{n,l}(x) - m(x)|^2 \mathds{P}_X(dx) + c_{23} \cdot \frac{(M_n + 1)^d}{n}\bigg) \\
& = \frac{c_{23} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \int |g_{n}(x) - m(x)|^2 \mathds{P}_X(dx) + 2 \cdot c_{23} \cdot \frac{(M_n + 1)^d}{n},
\end{split}
\end{equation*}
wobei wir bei der letzten Gleichheit verwendet haben, dass der letzte Summand deterministisch ist. Zudem wissen wir, dass $c_{23}$ unabhängig von $n$ ist und $n > 1$, da wir $n$ hinreichend groß wählen.
Als nächstes überprüfen wir die Voraussetzungen von Lemma~\ref{lem:9} um damit dann die letzte Gleichung weiter abzuschätzen.
Nach Voraussetzung ist $\beta_n = c_6 \cdot \log(n)$ und $a_n = (\log n)^{1/(6(N + d))}.$ Damit ist $a_n > 0$ für hinreichend großes $n$. Nach Voraussetzung ist zudem $d, N, J_n \in \N$ und es gilt nach (\ref{jn}) 
$$J_n \leq (M_n + 1)^d \cdot (N + 1)^d \leq n^{c_{14}},$$
für hinreichend großes $n$. Wir betrachten hier den logistischen squasher welcher nach Lemma~\ref{lem:logsquasher} insbesondere 2-zulässig nach Definition~\ref{nzulässig} ist. Da die hier betrachtete Menge von Funktionen $\mathcal{F}^{(J_n)}$ identisch mit der aus Lemma~\ref{lem:9} ist, sind nun alle Voraussetzungen erfüllt. Da wir ohne Beschränkung der Allgemeinheit angenommen haben, dass $supp(X) \subseteq [-a_n, a_n]^d$ ist, erhalten wir mit Lemma~\ref{lem:9} für hinreichend großes $n$:
\begin{equation}
\label{lem9sol}
\begin{split}
& \frac{c_{23} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \leq \frac{c_{23} \cdot \log(n)^2 \cdot \big(\big(c_{19} \cdot \log(n) \cdot (M_n + 1)^d \cdot (N + 1)^d \big) + 1\big)}{n} \\
& \leq \frac{c_{23} \cdot \log(n)^2 \cdot \big(2 \cdot c_{19} \cdot \log(n) \cdot (M_n + 1)^d \cdot (N + 1)^d\big)}{n} \\
& \leq c_{24} \cdot \frac{\log(n)^3 \cdot (M_n + 1)^d \cdot (N + 1)^d}{n}.
\end{split}
\end{equation}
Sei 
\begin{equation*}
\begin{split}
P_n(x) = \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj} m (x_{\mathbf{i}}) \cdot (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+. 
\end{split}
\end{equation*}
Für zwei beliebige reelle Zahlen $u, v \in \R$ gilt durch $0 \leq (u - v)^2 = u^2 + v^2 - 2uv$:
$$v^2 + v^2 \geq 2uv$$ und zusammen mit einer Nulladdition, der Linearität des Integrals und der Supremumseigenschaft erhalten wir:
\begin{equation}
\label{gnmx}
\begin{split}
& \int |g_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = \int |g_n(x) - P_n(x) + P_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = \int |g_n(x) - P_n(x)|^2 + 2(g_n - P_n(x))(P_n(x) - m(x)) + |P_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \int 2 |g_n(x) - P_n(x)|^2 + 2 |P_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = 2 \int \sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_n(x)|^2 \mathds{P}_X(dx) + 2 \int \sup_{x \in [-a_n, a_n]^d} |P_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& = 2 \sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_n(x)|^2 + 2 \sup_{x \in [-a_n, a_n]^d} |P_n(x) - m(x)|^2, 
\end{split}
\end{equation}
wobei wir im letzten Schritt $supp(X) \subseteq [-a_n, a_n]^d$ und $\mathds{P}(X \in supp(\mathds{P}_X)) = 1$ verwendet haben. Um die letzten beiden Summanden weiterhin abzuschätzen möchten wir Lemma~\ref{lem:5} anwenden. Dafür überprüfen wir ob dafür alle Voraussetzungen erfüllt sind. Wir betrachten den logistischen squasher, welcher nach Lemma~\ref{lem:2} insbesondere 2-zulässig ist. Zudem ist für hinreichend großes $n$ die Bedingung für $R$ erfüllt und da unser neuronales Netz (\ref{fnet}) mit $x_{\mathbf{i}} \in [-a_n, a_n]^d$ identisch mit der Definition aus Lemma~\ref{lem:5} ist, sind alle Voraussetzungen erfüllt. 
Wir erhalten damit für $x \in [-a_n ,a_n]^d$ und $n$ hinreichend groß:
\begin{equation}
\label{1stsum}
\begin{split}
 |g_n(x) - P_n(x)| 
& = \bigg| \sum_{\bi \in [M_n]^d} \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 \leq q}} \frac{1}{\bj!} \cdot \partial^{\bj} m (x_{\mathbf{i}}) \bigg| \cdot \bigg| f_{\net,\bj,\mathbf{i}}(x) - (x - x_{\mathbf{i}})^{\bj} \cdot \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \bigg| \\
& \leq (M_n + 1)^d \cdot (q + 1)^d \cdot e \cdot \bigg| f_{\net,\bj,\mathbf{i}}(x) - (x - x_{\mathbf{i}})^{\bj} \cdot \prod_{j = 1}^d(1 - \frac{M_n}{2a_n} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \bigg| \\
& \leq (M_n + 1)^d \cdot (q + 1)^d \cdot e \cdot \hat{c}_{12} \cdot 3^{3 \cdot 3^s} \cdot a_n^{3 \cdot 2^s} \cdot M_n^3 \cdot \frac{1}{R_n} \\
& \leq (M_n + 1)^d \cdot (q + 1)^d \cdot c_{25}\cdot a_n^{3 \cdot (N + d) \cdot 2} \cdot \frac{M_n^3}{R_n} \\
& =  (M_n + 1)^d \cdot (q + 1)^d \cdot c_{25}\cdot \log(n) \cdot \frac{M_n^3}{R_n},
\end{split}
\end{equation}
wobei wir unter anderem verwendet haben, dass für hinreichend großes $n$:
$$a_n^{2^{\lceil\log_2(N + d)\rceil}} \leq a_n^{2^{\log_2(N + d) + 1}} = a_n^{(N + d) \cdot 2},$$ gilt. Im letzten Schritt haben wir dann noch die Definition von $a_n$ eingesetzt. 
Da nach Konstruktion $a > 0$, $m$ $(p, C)$-glatt und $P_n(x)$ nach Lemma~\ref{lem:loccon} eine lokale Konvexkombination von Taylorpolynomen von $m$ ist, erhalten wir mit Lemma~\ref{lem:pcsmooth}:
\begin{equation}
\label{2ndsum}
\begin{split}
|P_n(x) - m(x)| \leq c_{26} \cdot \frac{a_n^p}{M_n^p} \leq c_{26} \cdot \log(n) \cdot \frac{1}{M_n^p} , 
\end{split}
\end{equation}
wobei wir verwendet haben, dass für $p = q + s$ für hinreichend großes $n$ gilt:
$$a_n^p = a_n^{q + s} \leq a_n^{N + d} \leq a_n^{6 \cdot (N + d)} = \log(n),$$
da nach Voraussetzung $N \geq q$ und $d \geq s$ mit $s \in (0, 1]$ ist.
Durch Quadrieren bleiben die Ungleichungen auch erhalten und da die rechten Seiten von Ungleichung~(\ref{1stsum}) und (\ref{2ndsum}) nicht von $x$ abhängen, gelten die Ungleichung ebenfalls für das Supremum.   Durch einsetzen der Definitionen von $M_n$ und $R_n$ erhalten wir für $n$ hinreichend groß:
\begin{equation}
\label{3rdsum}
\begin{split}
\sup_{x \in [-a_n, a_n]^d} |g_n(x) - P_n(x)|^2 & \leq \bigg((M_n + 1)^d \cdot (q + 1)^d \cdot c_{25}\cdot \log(n) \cdot \frac{M_n^3}{R_n}\bigg)^2 \\
& \leq c_{25}^2 \cdot (M_n + 1)^{2d} \cdot \log(n)^2 \cdot \frac{M_n^6}{R_n^2} \\
& \leq  c_{25}^2 \cdot (M_n + 1)^{2d} \cdot \log(n)^2 \cdot \frac{(M_n + 1)^{6d}}{R_n^2} \\
& \leq  c_{25}^2 \cdot \log(n)^2 \cdot \frac{(M_n + 1)^{8d}}{R_n^2} \\
& \leq c_{25}^2 \cdot \frac{n^{\frac{8d}{2p + d}}}{n^{2d + 8}} \cdot \log(n)^2 \\
& = c_{25}^2 \cdot n^{\frac{8d}{2p + d} - 2d -8} \cdot \log(n)^2 \\
& \leq c_{25}^2 \cdot n^{\frac{8d}{2p + d}  -8\frac{2p + d}{2p + d}} \cdot \log(n)^2 \\
& = c_{25}^2 \cdot n^{-\frac{16p}{2p + d}} \cdot \log(n)^2 \\
& \leq c_{25}^2 \cdot n^{-\frac{2p}{2p + d}} \cdot \log(n)^3,
\end{split}
\end{equation}
wobei wir bei der letzten Ungleichung verwendet haben, dass $\frac{16p}{2p + d} > \frac{2p}{2p + d}$, da $p > 0$ ist und $\log(n)^2 < \log(n)^3$ für $n$ hinreichend groß. Ebenfalls erhalten wir:
\begin{equation}
\label{4thsum}
\begin{split}
\sup_{x \in [-a_n, a_n]^d} |P_n(x) - m(x)|^2 & \leq \bigg(c_{26} \cdot \log(n) \cdot \frac{1}{M_n^p}\bigg)^2 \\
& \leq c_{26}^2 \cdot \log(n)^2 \cdot c_{5}^{-2p} \cdot n^{-\frac{2p}{2p + d}} \\
& \leq (c_{16}^2 \cdot c_{5}^{-2p}) \cdot \log(n)^3 \cdot n^{-\frac{2p}{2p + d}}.
\end{split}
\end{equation}
Mit analogem Vorgehen erhalten wir für (\ref{lem9sol}):
\begin{equation}
\begin{split}
& \frac{c_{23} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}^{(J_n)},x_1^n\big)\big) + 1\big)}{n} \\
& \leq c_{24} \cdot \frac{\log(n)^3 \cdot (M_n + 1)^d \cdot (N + 1)^d}{n} \\
& \leq \hat{c}_{24} \cdot \log(n)^3 \cdot \frac{(c_5 \cdot n^{\frac{1}{2p + d}} + 2)^d}{n} \\
& \leq \hat{c}_{24} \cdot \log(n)^3 \cdot \frac{c_5^d\cdot n^{\frac{d}{2p + d}}}{n} \\
& = \tilde{c}_{24} \cdot \log(n)^3 \cdot n^{\frac{d}{2p + d} - 1} \\
& = \tilde{c}_{24} \cdot \log(n)^3 \cdot n^{-\frac{2p}{2p + d}}.
\end{split}
\end{equation}
Zudem erhalten wir, da für $n$ hinreichend groß $\log(n)^3 > 1$ gilt, mit analogem Vorgehen:
\begin{equation}
\label{5thsum}
\begin{split}
c_{23} \cdot \frac{(M_n + 1)^d}{n} & \leq \tilde{c}_{23} \cdot \frac{n^{\frac{d}{2p + d}}}{n} \\
& \leq \tilde{c}_{23} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}} 
\end{split}
\end{equation} 
und 
\begin{equation}
\label{6thsum}
\begin{split}
\frac{4 \cdot c_{22} \cdot \beta_n^2}{n} & \leq \tilde{c}_{22} \cdot \log(n)^3 \cdot n^{-1} \\
& \leq \tilde{c}_{22} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}}.  
\end{split}
\end{equation} 
Nun haben wir alle Summanden von ($\ref{originaleq}$) abgeschätzt und erhalten schließlich mit mit (\ref{1stsum}) - (\ref{6thsum}):
\begin{equation*}
\E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \leq c_{fin} \cdot \log(n)^3 \cdot n^{- \frac{2p}{2p + d}},
\end{equation*}
mit 
$$c_{fin} = \tilde{c}_{22} + 2 \cdot \tilde{c}_{23} + \tilde{c}_{24} + 2 \cdot (2 \cdot c_{16}^2 \cdot c_{5}^{-2p} + 2 \cdot  c_{25}^2),$$
wobei $c_{fin}$ als Summe nichtnegativer oder positiver Konstanten, die unabhängig von $n$ sind, nichtnegativ und unabhängig von $n$ ist.
Damit haben wir unser Hauptresultat bewiesen.
\end{proof}