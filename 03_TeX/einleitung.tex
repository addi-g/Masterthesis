\chapter*{Einleitung}\index{Einleitung}
\addcontentsline{toc}{chapter}{Einleitung}\index{Einleitung}

Erfinder träumen schon lange davon, Maschinen zu schaffen, die denken. Dieser Wunsch geht zumindest auf die Zeit des antiken Griechenlands zurück. Die mythischen Figuren Pygmalion, Daedalus und Hephaestus können alle als legendäre Erfinder interpretiert werden, und Galatea, Talos und Pandora können alle als künstliches Leben betrachtet werden (Ovid und Martin, 2004; Sparkes, 1996; Tandy, 1997).
Als programmierbare Computer zum ersten Mal konzipiert wurden, fragten sich die Menschen, ob solche Maschinen intelligent werden könnten, mehr als hundert Jahre bevor man sie baute (Lovelace, 1842). Heute ist die künstliche Intelligenz (KI) ein blühendes Feld mit vielen praktischen Anwendungen und aktiven Forschungsthemen. 
Künstliche Intelligenz ist längst in unserem Alltag präsent und dringt in immer mehr Bereiche vor. Sprachassistenten etwa sind bereits als Helfer auf dem Smartphone, im Auto oder zu Hause Normalität geworden. Fortschritte im Bereich der KI beruhen vor allem auf der Verwendung Neuronaler Netze. Vergleichbar mit der Funktionsweise des menschlichen Gehirns verknüpfen sie mathematisch definierte Einheiten miteinander.

Es besteht eine große Lücke zwischen den Schätzungen, für die schönen Konvergenzergebnisse die in der Theorie nachgewiesen wurden, und den Schätzungen, die in der Praxis verwendet werden können.

Ziel dieser Arbeit ist es, die folgende Frage genauer zu betrachten: Wenn wir eine Regressionsschätzung des neuronalen Netzes theoretisch genau so definieren, wie sie in der Praxis umgesetzt wird, welches Konvergenzergebnis können wir dann für diese Schätzung vorweisen? 

Als erstes werden wir in Kapitel \ref{chap:1} grundlegende Definition und Hilfsresultate für den weiteren Verlauf der Arbeit sammeln.
Anschließend definieren wir in Kapitel \ref{chap:2} eine neue Regressionsschätzung für neuronale Netze, bei der die meisten Gewichte unabhängig von den Daten gewählt werden, die durch einige neuere Approximationsergebnisse für neuronale Netze motiviert sind, und die daher leicht zu implementieren ist. In Kapitel \ref{chap:3} zeigen wir dann unser Hauptresultat, dass wir für diese Schätzung Konvergenzraten ableiten können, falls die Regressionsfunktion glatt ist. 

Mit diesem Hauptergebnis ist es nun möglich einfach zu implementierende Regressionsverfahren für neuronale Netze zu definieren, die die gleiche Konvergenzrate wie lineare Regressionsschätzungen (z.B. Kernel- oder Spline-Schätzungen) erreichen, d.h. sie erreichen (bis zu einem logarithmischen Faktor) die optimale Minimax-Konvergenzrate $n-2p/(2p+d)$ im Falle einer ($p,C$)-glatten Regressionsfunktion, für jedes $p > 0$.
Abschließend wird die Leistung unseres neu vorgeschlagenen Schätzers für simulierte Daten in Kapitel \ref{chap:4} veranschaulicht. Diese Arbeit orientiert sich an \cite{kohler19}.
