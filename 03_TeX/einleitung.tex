\chapter*{Einleitung}\index{Einleitung}
\addcontentsline{toc}{chapter}{Einleitung}\index{Einleitung}

Erfinder träumen schon lange davon, Maschinen zu schaffen, die denken. Dieser Wunsch geht zumindest auf die Zeit des antiken Griechenlands zurück. Die mythischen Figuren Pygmalion, Daedalus und Hephaestus können alle als legendäre Erfinder interpretiert werden, und Galatea, Talos und Pandora können alle als künstliches Leben betrachtet werden (Ovid und Martin, 2004; Sparkes, 1996; Tandy, 1997).
Als programmierbare Computer zum ersten Mal konzipiert wurden, fragten sich die Menschen, ob solche Maschinen intelligent werden könnten, mehr als hundert Jahre bevor man sie baute (Lovelace, 1842). Heute ist die künstliche Intelligenz (KI) ein blühendes Feld mit vielen praktischen Anwendungen und aktiven Forschungsthemen. 
Künstliche Intelligenz ist längst in unserem Alltag präsent und dringt in immer mehr Bereiche vor. Sprachassistenten etwa sind bereits als Helfer auf dem Smartphone, im Auto oder zu Hause Normalität geworden. Fortschritte im Bereich der KI beruhen vor allem auf der Verwendung Neuronaler Netze. Vergleichbar mit der Funktionsweise des menschlichen Gehirns verknüpfen sie mathematisch definierte Einheiten miteinander.

Es besteht eine große Lücke zwischen den Schätzungen, für die schönen Konvergenzergebnisse die in der Theorie nachgewiesen wurden, und den Schätzungen, die in der Praxis verwendet werden können.

Maschinelle Lernverfahren können als Lernen einer Funktion $(f)$ zusammengefasst werden, die Eingangsvariablen $(X)$ auf Ausgangsvariablen $(Y$) abbildet.
$$Y = f(x)$$
Ein Algorithmus lernt diese Zielabbildungsfunktion aus Trainingsdaten.
Die Form der Funktion ist unbekannt, so dass es unsere Aufgabe als Praktiker des maschinellen Lernens ist, verschiedene Algorithmen des maschinellen Lernens zu evaluieren und zu sehen, welcher die zugrunde liegende Funktion besser annähert.
Unterschiedliche Algorithmen machen unterschiedliche Annahmen oder Vorurteile über die Form der Funktion und die Art und Weise, wie sie gelernt werden kann.

Algorithmen, die keine starken Annahmen über die Form der Abbildungsfunktion treffen, werden als nichtparametrische Algorithmen des maschinellen Lernens bezeichnet. Indem sie keine Annahmen treffen, können sie jede beliebige Funktionsform aus den Trainingsdaten lernen.
Nichtparametrische Methoden sind gut, wenn Sie über viele Daten und keine Vorkenntnisse verfügen und wenn Sie sich nicht allzu sehr um die Auswahl der richtigen Funktionen kümmern wollen. (Referenz - Artificial Intelligence: A Modern Approach, Seite 757)

Ziel dieser Arbeit ist es, die folgende Frage genauer zu betrachten: Wenn wir eine Regressionsschätzung des neuronalen Netzes theoretisch genau so definieren, wie sie in der Praxis umgesetzt wird, welches Konvergenzergebnis können wir dann für diese Schätzung vorweisen? 

Als erstes werden wir in Kapitel \ref{chap:1} grundlegende Definition und Hilfsresultate für den weiteren Verlauf der Arbeit sammeln.
Anschließend definieren wir in Kapitel \ref{chap:2} eine neue Regressionsschätzung für neuronale Netze, bei der die meisten Gewichte unabhängig von den Daten gewählt werden, die durch einige neuere Approximationsergebnisse für neuronale Netze motiviert sind, und die daher leicht zu implementieren ist. In Kapitel \ref{chap:3} zeigen wir dann unser Hauptresultat, dass wir für diese Schätzung Konvergenzraten ableiten können, falls die Regressionsfunktion glatt ist. 

Mit diesem Hauptergebnis ist es nun möglich einfach zu implementierende Regressionsverfahren für neuronale Netze zu definieren, die die gleiche Konvergenzrate wie lineare Regressionsschätzungen (z.B. Kernel- oder Spline-Schätzungen) erreichen, d.h. sie erreichen (bis zu einem logarithmischen Faktor) die optimale Minimax-Konvergenzrate $n-2p/(2p+d)$ im Falle einer ($p,C$)-glatten Regressionsfunktion, für jedes $p > 0$.
Abschließend wird die Leistung unseres neu vorgeschlagenen Schätzers für simulierte Daten in Kapitel \ref{chap:4} veranschaulicht. Diese Arbeit orientiert sich an \cite{kohler19}.
