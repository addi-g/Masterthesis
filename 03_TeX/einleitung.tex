\chapter*{Einleitung}\index{Einleitung}
\addcontentsline{toc}{chapter}{Einleitung}\index{Einleitung}

%Erfinder träumen schon lange davon, Maschinen zu erschaffen, die denken. Dieser Wunsch geht zumindest auf die Zeit des antiken Griechenlands zurück. Die mythischen Figuren Pygmalion, Daedalus und Hephaestus können alle als legendäre Erfinder interpretiert werden, und Galatea, Talos und Pandora können alle als künstliches Leben betrachtet werden (siehe \cite{2005metamorphoses}, \cite{sparkes1996red}, \cite{hesiod1996works}).
Als programmierbare Computer zum ersten Mal konzipiert wurden, fragten sich die Menschen, ob solche Maschinen intelligent werden könnten, mehr als hundert Jahre bevor man sie baute. Heute ist die künstliche Intelligenz (\emph{KI}) ein blühendes Feld mit vielen praktischen Anwendungen und aktiven Forschungsthemen. 
Künstliche Intelligenz ist längst in unserem Alltag präsent und dringt in immer mehr Bereiche vor. Sprachassistenten etwa sind bereits als Helfer auf dem Smartphone, im Auto oder zu Hause Normalität geworden. Fortschritte im Bereich der KI beruhen vor allem auf der Verwendung \emph{neuronaler Netze}. Vergleichbar mit der Funktionsweise des menschlichen Gehirns verknüpfen sie mathematisch definierte Einheiten miteinander \cite{goodfellow2016deep}.

In vielen Anwendungen der KI geht es darum, aus einer Menge von Daten eine allgemeine Regel abzuleiten (\emph{maschinelles Lernen}). Maschinelles Lernen kann \emph{als Lernen einer Funktion} $f$ zusammengefasst werden, die Eingangsvariablen~$X$ auf Ausgangsvariablen~$Y$ abbildet, sodass die Abbildungsvorschrift~$f(X) = Y$ entsteht.

Ein Algorithmus lernt diese Zielabbildungsfunktion aus Trainingsdaten.
Die Form der Funktion ist unbekannt, sodass es unsere Aufgabe als Praktiker des maschinellen Lernens ist, verschiedene Algorithmen des maschinellen Lernens zu evaluieren und zu sehen, welcher die zugrunde liegende Funktion besser annähert.
Unterschiedliche Algorithmen machen unterschiedliche Annahmen über die Form der Funktion und die Art und Weise, wie sie gelernt werden kann.

Algorithmen, die keine Annahmen über die Form der Abbildungsfunktion treffen, werden als nichtparametrische Algorithmen des maschinellen Lernens bezeichnet. Indem sie keine Annahmen treffen, können sie jede beliebige Funktionsform aus den Trainingsdaten lernen.
Nichtparametrische Methoden sind gut, wenn viele Daten verfügbar sind und man sich nicht allzu sehr um die Auswahl der richtigen Funktionen kümmern will \cite[Seite~757]{AI}. Mathematisch führt dies zu einem Approximationsproblem. 

Im Kontext der KI wurden hierzu unter anderem neuronale Netze vorgeschlagen, die als universale Funktionsapproximatoren (\emph{Schätzer}) eingesetzt werden können, jedoch insbesondere bei vielen verdeckten Schichten schwer zu analysieren sind. 
Dies führt unter anderem dazu, dass eine große Lücke zwischen den Schätzungen, für die schöne Konvergenzergebnisse in der Theorie nachgewiesen wurden, und den Schätzungen entsteht, die in der Praxis verwendet werden können.

Ziel dieser Arbeit ist es, die folgende Frage genauer zu betrachten: Wenn wir eine Regressionsschätzung des neuronalen Netzes theoretisch genau so definieren, wie sie in der Praxis umgesetzt wird, welches Konvergenzergebnis können wir dann für diese Schätzung vorweisen? 

Als Erstes werden wir in Kapitel~\ref{chap:1} Grundlagen zu neuronalen Netzen für den weiteren Verlauf der Arbeit sammeln.
Anschließend definieren wir in Kapitel~\ref{chap:2} eine neue, leicht zu implementierende Neuronale-Netze-Regressionsschätzung. Die Besonderheit des dem Schätzer zugrunde liegenden neuronalen Netzes besteht darin, dass die meisten Gewichte unabhängig von den Daten gewählt werden.
%die durch einige neuere Approximationsergebnisse für neuronale Netze motiviert sind. 
In Kapitel~\ref{chap:3} zeigen wir, dass wir für diesen Schätzer Konvergenzraten ableiten können, falls die Regressionsfunktion bestimmte Glattheitsvoraussetzungen erfüllt. 
%Mit diesem Hauptergebnis ist es nun möglich, einfach zu implementierende Regressionsverfahren für neuronale Netze zu definieren, die die gleiche Konvergenzrate wie lineare Regressionsschätzungen (z.B. Kernel- oder Spline-Schätzungen) erreichen, d.h. sie erreichen (bis zu einem logarithmischen Faktor) die optimale Minimax-Konvergenzrate $n-2p/(2p+d)$ im Falle einer ($p,C$)-glatten Regressionsfunktion, für jedes $p > 0$.
Abschließend untersuchen wir in Kapitel~\ref{chap:4} die Leistung unseres Neuronale-Netze-Regressionsschätzers aus Kapitel~\ref{chap:2} anhand von Anwendungsbeispielen auf simulierte Daten. Diese Arbeit orientiert sich an \cite{kohler19}.