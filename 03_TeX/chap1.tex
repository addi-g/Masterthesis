\chapter{Grundlagen für neuronale Netze}
\label{chap:1}

Das Ziel dieses Kapitels ist es, auf die in dieser Arbeit verwendeten neuronalen Netze einzugehen, die als Bausteine für unseren einfach berechenbaren Neuronale-Netze-Regressionsschätzer verwendet werden. Weiterhin werden wir Approximationsresultate darstellen und beweisen, welche wir für das Resultat der Konvergenzgeschwindigkeit des einfach berechenbaren Neuronale-Netze-Regressionsschätzers benötigen werden. Wenn wir in dieser Arbeit von \emph{unserem Neuronale-Netze-Regressionsschätzer} sprechen, beziehen wir uns immer auf den \emph{einfach berechenbaren Neuronale-Netze-Regressionsschätzer} aus \cite{kohler19}. 
In dieser Arbeit bezeichnen wir mit $d \geq 1$ immer eine natürliche Zahl.

Da wir uns in dieser Arbeit mit neuronalen Netzen beschäftigen, ist es hilfreich zu wissen, was man darunter versteht. Ein neuronales Netz ist nichts anderes als eine Ansammlung von Neuronen, welche als Informationsverarbeitungseinheiten dienen, die schichtweise in einer Architektur angeordnet sind. Beginnend mit der Eingabeschicht (\textit{Input Layer}) fließen Informationen über eine oder mehrere verborgene Schichten (\textit{Hidden Layer}) bis hin zur Ausgabeschicht (\textit{Output Layer)}. Die Informationsweitergabe der Neuronen verläuft wie folgt: Für jedes Neuron $j$ werden die Eingaben $x_1,\dots,x_n$ mit $w_{1_j}, \dots, w_{n_j}$ gewichtet an eine Aktivierungsfunktion $\sigma$ übergeben, welche die Neuronenaktivierung berechnet. Die Eingaben können aus dem beobachteten Prozess resultieren oder andererseits aus den Ausgaben anderer Neuronen stammen. 
%Die Neuronen erhalten Eingaben $x_1,\dots,x_n$ die sie   Die Informationsweitergabe der Neuronen verläuft so, dass sie die Eingaben $x_1,\dots,x_n$, die einerseits aus dem beobachteten Prozess resultieren können, dessen Werte dem Neuron übergeben werden, oder andererseits aus den Ausgaben anderer Neuronen stammen und verarbeiten.
% und entsprechend über eine Aktivierung reagieren. 
%Dazu werden für ein Neuron $j$ die Eingaben mit $w_{1_j}, \dots, w_{n_j}$ gewichtet an eine Aktivierungsfunktion $\sigma$ übergeben, welche die Neuronenaktivierung berechnet. 
Der Endpunkt des Informationsflusses in einem neuronalen Netz ist die Ausgabeschicht, die hinter den verborgenen Schichten liegt. Sie bildet damit die letzte Schicht in einem neuronalen Netz. Die Ausgabeschicht enthält somit das Ergebnis der Informationsverarbeitung durch das Netz.  
%\begin{figure}
%    \centering
%    \begin{tikzpicture}[
%        % define styles    
%        init/.style={ 
%             draw, 
%             circle, 
%             inner sep=2pt,
%             font=\Huge,
%             join = by -latex
%        },
%        squa/.style={ 
%            font=\Large,
%            join = by -latex
%        }
%    ]
%        
%        % Top chain x1 to w1
%        \begin{scope}[start chain=1]
%            \node[on chain=1] at (0,1.5cm)  (x1) {$x_1$};
%            \node[on chain=1,join=by o-latex] (w1) {$w_1$};
%        \end{scope}
%        
%        % Middle chain x2 to output
%        \begin{scope}[start chain=2]
%            \node[on chain=2] (x2) {$x_2$};
%            \node[on chain=2,join=by o-latex] {$w_2$};
%            \node[on chain=2,init] (sigma) {$\displaystyle\Sigma$};
%            \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation\\ function}}]   {$f_{act}$};
%            \node[on chain=2,squa,label=above:Output,join=by -latex] {$y_{out}$};
%        \end{scope}
%        
%        % Bottom chain x3 to w3
%        \begin{scope}[start chain=3]
%            \node[on chain=3] at (0,-1.5cm) 
%            (x3) {$x_3$};
%            \node[on chain=3,label=below:Weights,join=by o-latex]
%            (w3) {$w_3$};
%        \end{scope}
%        
%        % Bias
%        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
%        
%        % Arrows joining w1, w3 and b to sigma
%        \draw[-latex] (w1) -- (sigma);
%        \draw[-latex] (w3) -- (sigma);
%        \draw[o-latex] (b) -- (sigma);
%        
%        % left hand side brace
%        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
%        
%    \end{tikzpicture}
%    
%    \caption{Neural network pipeline}
%    
%\end{figure}
Zwei wichtige Charakteristika, die neuronale Netze aufweisen können, sind:
\begin{itemize}
\item Wenn in einem neuronalen Netz die Information von der Eingabeschicht über die verborgenen Schichten bis hin zur Ausgabeschicht in eine Richtung (\glqq vorwärts\grqq) weitergereicht wird, spricht man von einem \textit{feedforward} neuronalen Netz.
\item Ein neuronales Netz wird als ein \textit{Fully-connected} (\glqq vollständig verbundenes\grqq\@) neuronales Netz bezeichnet, wenn sämtliche Neuronen einer Schicht mit allen der darauffolgenden verbunden sind. Da man bei Fully-connected neuronalen Netzen die Gewichte der Verbindungen zwischen zwei Neuronen auf Null setzen kann, unterscheiden wir hier nicht mehr zwischen neuronalen Netzen die Fully-connected sind oder nicht.
%Jedes Neuron der verborgenen Schicht ist also mit allen Neuronen der Eingabeschicht verbunden und ebenso ist jedes Neuron der Ausgabeschicht mit allen der letzten verborgenen Schicht verbunden.
\end{itemize}
%Für ein neuronales Netz sind die Aktivierungsfunktionen von großer Bedeutung, da sie dabei helfen, da es nichtlineare komplexe funktionale Mapping zwischen den Eingangsdaten und den abhängigen Ergebnissen zu verstehen.
%\begin{defn}
%Eine \emph{Aktivierungsfunktion} ist eine Transformation, die wir über die Eingabe durchführen, bevor wir sie an die nächste Schicht von Neuronen senden oder sie als Ausgabe finalisieren.
%\end{defn}
%\begin{defn}
%Ein \emph{neuronales Netz} ist eine Funktion $f\colon X \to Y$, wobei $f(x)$ für $x \in X$, definiert ist als Komposition von weiteren Funktionen $g_i \colon X \to Y$, die weiter in andere Funktionen zerlegt werden können.
%Die Funktion $f$ ist eine nichtlinear gewichtete Summe, mit $f(x) =  \sigma(\sum_iw_ig_i(x))$, wobei $w_i$ die Gewichte sind und $\sigma$ eine Aktivierungsfunktion ist.
%\end{defn}
%Als nächstes Definieren wir, was wir unter einer Netzwerkarchitektur verstehen um damit anschließend eine Definition eines mehrschichtigen feedforward neuronalen Netzes zu konstruieren.
%\begin{defn}
%Eine \emph{Netzwerkarchitektur $(L, \bk)$} ist eine Klasse aller neuronaler Netze $f$ für die gilt:
%\begin{itemize}
%\item Das neuronale Netz hat $L \in \N_0$ verborgene Schichten.
%\item Die Anzahl an Neuronen in jeder verborgenen Schicht wird durch den Vektor $\bk = (k_1,\dots,k_L) \in \N^L$ angegeben.
%\end{itemize}
%Mit dieser Definition können wir nun sagen, dass eine Netzwerkarchitektur wie ein Grundgerüst für die Konstruktion weiterer neuronale Netze ist.
%\end{defn}
Als Nächstes kommen wir zur mathematischen Definition eines neuronalen Netzes, welche wir in dieser Form im weiteren Verlauf dieser Arbeit benötigen werden.
\begin{defn}
\label{def:nn}
Sei $L \in \N$, $\bk \in \N^L$ und $\sigma\colon \R \to \R$. Ein \emph{mehrschichtiges feedforward neuronales Netz mit Architektur $(L, \bk)$ und Aktivierungsfunktion} $\sigma$, ist eine reellwertige Funktion $f\colon \R^d \to \R$ definiert durch:
\begin{align*}
%\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{mit Gewichten $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ mit $i = 1,\dots,k_L$ rekursiv definiert durch:} 
f_i^{(r)}(x) & = \sigma_r\bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg)\\
\intertext{mit Gewichten $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R$ mit $r = 2,\dots, L$, wobei $\sigma_r \in \{\sigma, \operatorname{id}\}$ und:}
f_i^{(1)}(x) & = \sigma_1 \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber
\end{align*} 
für die Gewichte $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$, wobei $\sigma_1 \in \{\sigma, \operatorname{id}\}$.
Weiterhin sei $\mathfrak{N}(L, \bk, \sigma)$ die Klasse aller mehrschichtigen feedforward neuronalen Netze mit Architektur $(L, \bk)$ und Aktivierungsfunktion $\sigma$.
\end{defn}
Wir werden ab jetzt der Einfachheit halber nur noch von \emph{neuronalen Netzen mit Architektur $(L,\bk)$} reden, wenn die Aktivierungsfunktion $\sigma$ aus dem Kontext bekannt ist oder keine Rolle spielt und beziehen uns damit immer auf Definition~\ref{def:nn}. Wenn aus dem Kontext die \emph{Architektur~$(L,\bk)$} und die \emph{Aktivierungsfunktion $\sigma$} bereits bekannt ist, sprechen wir nur noch von einem \emph{neuronalen Netz}, beziehen uns damit aber dennoch auf Definition~\ref{def:nn}.
\begin{bemnumber}
In obiger Definition versteht man unter:
\begin{itemize}
\item $L \in \N$ die Anzahl an verborgenen Schichten von $f$.
\item $\bk = (k_1,\dots,k_L) \in \N^L$ einen Vektor, der die Anzahl an Neuronen in jeder verborgenen Schicht angibt.
\end{itemize}
Ist $f = \sum_{i = 1}^n a_i f_i $ eine Linearkombination von neuronalen Netzen $f_1,\dots,f_n \in \mathfrak{N}(L, \bk, \sigma)$ mit Linearfaktoren $a_1,\dots,a_n \in \R$, so können wir $f$ als ein neuronales Netz mit Architektur~$(L+1,(k_1,\dots,k_L,n))$ betrachten, wobei in diesem Fall $\sigma_L = \operatorname{id}$ gilt.
\end{bemnumber}
Da wir bei Neuronale-Netze-Regressionsschätzern Funktionswerte schätzen möchten, haben wir in der Ausgabeschicht nur ein Neuron. Damit erreichen wir einen eindimensionalen Output. Wie wir an der Konstruktion des neuronalen Netzes in Definition~\ref{def:nn} erkennen können, nehmen wir in der Ausgabeschicht die Identität als Aktivierungsfunktion.
%Es wäre auch möglich für jede Schicht eine andere Aktivierungsfunktion zu wählen. In dieser Arbeit beschränken wir uns nur auf den Fall, in welchem wir in allen verborgenen Schichten die selbe Aktivierungsfunktion verwenden.

Abbildung~\ref{fig:DNN} zeigt schematisch ein mehrschichtiges feedforward neuronales Netz, welches aus einer Eingabeschicht (\textit{Input-Feature 1} - \textit{Input-Feature $n_{\mathrm{in}}$}), $n_{\mathrm{lyr}}$~verborgenen Schichten und einer Ausgabeschicht (\textit{Output 1} - \textit{Output $n_{\mathrm{out}}$}) besteht.
\begin{center}
    \begin{figure}
        \begin{tikzpicture}[
            plain/.style={
              draw=none,
              fill=none,
              },
            dot/.style={draw,shape=circle,minimum size=3pt,inner sep=0,fill=black
              },
            net/.style={
              matrix of nodes,
              nodes={
                draw,
                circle,
                inner sep=8.5pt
                },
              nodes in empty cells,
              column sep=0.1cm,
              row sep=-11pt
              },
            >=latex
            ]
            \matrix[net] (mat)
            {
            |[plain]| \parbox{1cm}{\centering \small Input\\layer} 
                        & |[plain]| \parbox{1.2cm}{\centering \small Hidden\\layer 1} 
                                    &  |[plain]| \parbox{0cm}{\centering ...} 
                                                &  |[plain]| \parbox{1.4cm}{\centering \small Hidden\\layer $n_{\mathrm{lyr}}$} 
                                                                & |[plain]| \parbox{1cm}{\centering \small Output\\layer} \\
                        & |[plain]| & |[plain]| & |[plain]|     & |[plain]|                                   \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   & |[plain]|   & |[plain]|  & |[plain]|                \\
                        & |[dot]|  & |[plain]| & |[dot]|    & |[dot]|      \\
            |[plain]|   & |[dot]|   & |[plain]| \parbox{0cm}{\centering ...} & |[dot]|       & |[dot]|     \\
            |[dot]|     & |[dot]|  & |[plain]| & |[dot]|     & |[dot]|      \\
            |[dot]|     & |[plain]|    & |[plain]|   & |[plain]|       & |[plain]|    \\
            |[dot]|     & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]|            \\
            };
            \foreach \ai/\mi in {2/\small Input-Feature 1,4/\small Input-Feature 2,6/ \small Input-Feature 3,12/\small Input-Feature $n_{\mathrm{in}}$}
              \draw[<-] (mat-\ai-1) -- node[above] {\mi} +(-3cm,0);

            \foreach \ai in {2,4,6,12}
            {\foreach \aii/\mii in {3/,11/ }
                \draw[->] (mat-\ai-1) -- (mat-\aii-2) node[yshift=0cm] {\mii};
            }
            \foreach \ai in {3,11}
            {  
                \draw[->] (mat-\ai-4) -- (mat-4-5);
            }
            \draw[->] (mat-4-5) -- node[above] {\small Output 1} +(2.5cm,0);\
            \foreach \ai in {3,11}
            {
                \draw[->] (mat-\ai-4) -- (mat-10-5);
            }
            \draw[->] (mat-10-5) -- node[above] {\small Output $n_{\mathrm{out}}$} +(2.5cm,0);
        \end{tikzpicture}
        \caption{Mehrschichtiges feedforward neuronales Netz mit einer Eingabeschicht mit $n_{\mathrm{in}}$ Neuronen, $n_{\mathrm{lyr}}$ vielen verborgenen Schichten, deren Anzahl an Neuronen variieren kann und einer Ausgabeschicht bestehend aus $n_{\mathrm{out}}$ Neuronen.}
        \label{fig:DNN}
    \end{figure}
\end{center}

Wie zuvor erwähnt ist einer der Ausgangspunkte für die Definition eines neuronalen Netzes die Wahl einer Aktivierungsfunktion $\sigma\colon \R \to \R$. Wir kommen nun zu einer häufig vorausgesetzten Eigenschaft an Aktivierungsfunktionen.
\begin{defn}
\label{nzulässig}
Sei $N \in \N_0$. Eine Funktion $\sigma\colon \R \to [0, 1]$ wird \emph{$N$-zulässig} genannt, wenn sie monoton wachsend und lipschitzstetig ist und wenn zusätzlich die folgenden drei Bedingungen erfüllt sind:
\begin{itemize}
\item[(i)] Die Funktion $\sigma$ ist $(N + 1)$-mal stetig differenzierbar mit beschränkten Ableitungen.
\item[(ii)] Es existiert ein Punkt $t_{\sigma} \in \R$ mit $$\sigma^{(i)}(t_{\sigma}) \neq 0 \text{ für } i = 0,\dots,N.$$
\item[(iii)] Wenn $y > 0$ ist, gilt $|\sigma(y) - 1| \leq \frac{1}{y}$. Wenn $y < 0$ ist, gilt $|\sigma(y)| \leq \frac{1}{|y|}$.
\end{itemize}  
\end{defn}  

%Wir haben uns in dieser Arbeit für eine sogenannte \textit{Squashing Function} entschieden, welche eine monoton wachsende Funktion ist, für die $\lim_{x \to -\infty}\sigma(x) = 0$ und $\lim_{x \to \infty}\sigma(x) = 1$ gilt. In dieser Arbeit verwenden wir als Squashing Function immer den sogenannten \emph{sigmoidal} bzw.\@ \emph{logistischen Squasher}
Ein in dieser Arbeit wichtiges Beispiel für eine Aktivierungsfunktion bildet der sogenannte \emph{sigmoidal} bzw.\@ \emph{logistische Squasher}:
\begin{align}
\label{logsquasher}
\sigma(x) = \frac{1}{1 + \exp(-x)} \quad (x \in \R).
\end{align}
\begin{lem}
\label{lem:polynom}
Sei $n \in \N$. Dann existiert zur $n$-ten Ableitung~$\sigma^{(n)}$ des logistischen Squashers~$\sigma$ aus Gleichung~\eqref{logsquasher} ein Polynom $P_n$ vom Grad $(n + 1)$, sodass $P_n(\sigma) = \sigma^{(n)}$ gilt. Insbesondere ist $P_n$ ungleich dem Nullpolynom für alle $n \in \N$.
\end{lem}
\begin{proof}
    Wir zeigen die Aussage per Induktion über $n$.
    
    \emph{Induktionsanfang} (IA): 
    Die Funktion $\sigma$ erfüllt die gewöhnliche Differentialgleichung $\sigma^{(1)} = (1 - \sigma) \cdot \sigma,$ da:
\begin{equation*}
\begin{split}
\sigma^{(1)}(x) &= -\frac{1}{(1 + \exp(-x))^2} \cdot (-\exp(-x)) 
 = \frac{\exp(-x)}{1 + \exp(-x)} \cdot \frac{1}{1 + \exp(-x)} \\[0.5em]
& = \bigg(1 - \frac{1}{1 + \exp(-x)}\bigg) \cdot \frac{1}{1 + \exp(-x)} 
 = (1 - \sigma(x)) \cdot \sigma(x).
\end{split}
\end{equation*}
    Damit gilt $\sigma^{(1)} = P_1(\sigma)$ mit $P_1(x) = x - x^2.$ Es gilt $\operatorname{deg}(P_1) = 2$, da der führende Koeffizient von $P_1$ ungleich Null ist.
    
\emph{Induktionshypothese} (IH): Die Aussage gelte für ein beliebiges aber festes $n \in \N.$

\emph{Induktionsschritt} (IS): Nach der Induktionshypothese existiert ein Polynom $P_{n - 1}$ mit $P_{n - 1}(x) = \sum_{k = 0}^{n} a_k\cdot x^k$ und $\sigma^{(n - 1)} = P_{n - 1}(\sigma)$.
Mit der Kettenregel aus der Differentialrechnung erhalten wir:
\begin{align}
\sigma^{(n)} & = (\sigma^{(n - 1)})' \overset{\text{(IH)}}{=} \Big(\sum_{k = 0}^{n} a_k\cdot \sigma^k\Big)' = \sum_{k = 1}^{n} a_k \cdot k \cdot \sigma^{k- 1} \cdot \sigma' 
 \overset{\text{(IA)}}{=}  \sum_{k = 1}^{n} a_k \cdot k \cdot \sigma^{k- 1} \cdot \sigma \cdot (1 - \sigma) \nonumber \\[0.5em] 
& = \sum_{k = 1}^{n} a_k \cdot k \cdot \sigma^{k} - \sum_{k = 1}^{n} a_k \cdot k \cdot \sigma^{k + 1}. \label{eq:kette}
\end{align} 
Gleichung \eqref{eq:kette} definiert ein Polynom $P_n$ vom Grad $(n + 1)$ mit $\sigma^{(n)} = P_n(\sigma)$.
Damit haben wir die Aussage für alle $n \in \N$ bewiesen.
\end{proof}
%Als Nächstes stellen wir ein Resultat vor, welches eine wichtige Eigenschaft für die Aktivierungsfunktion der hier betrachteten neuronalen Netze darstellt.
\begin{lem}
\label{lem:logsquasher}
Sei $N \in \N_0$ beliebig, dann ist der logistische Squasher $\sigma$ aus Gleichung~\eqref{logsquasher} $N$-zulässig.
\end{lem}
\begin{proof}
Sei $N \in \N_0$ beliebig. Dadurch, dass $0 \leq \frac{1}{1 + \exp(-x)} \leq 1$ für alle $x \in \R$ gilt, erhalten wir schließlich auch $\sigma: \R \to [0,1]$. Wir wissen, dass $\sigma$ monoton wachsend ist, da für beliebige $s, t \in \R$ mit $s \leq t$ gilt:
$$\sigma(s) = \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} = \sigma(t),$$
wobei wir verwendet haben, dass aufgrund der Monotonie der Exponentialfunktion für $s \leq t$ auch $\exp(-s) \geq \exp(-t)$ gilt.
Zudem ist $\sigma$ als Komposition glatter Funktionen insbesondere $(N + 1)$-mal stetig differenzierbar.

Mit Lemma~\ref{lem:polynom} wissen wir, dass alle Ableitungen von $\sigma$, Polynome in $\sigma$ sind. Dadurch folgt Bedingung (i) aus Definition~\ref{nzulässig}, da $\sigma$ nach Voraussetzung beschränkt ist und die Ableitungen von $\sigma$ als endliche Summe von Produkten beschränkter Faktoren ebenfalls beschränkt sind. Da hiermit auch insbesondere die erste Ableitung von $\sigma$ beschränkt ist, wissen wir, dass $\sigma$ lipschitzstetig ist.

Nun kommen wir zum Beweis von Bedingung (ii).
Sei $\mathcal{N}$ die Menge aller Nullstellen der Polynome $P_1,\dots,P_n$ aus Lemma~\ref{lem:polynom}. Da keines dieser Polynome das Nullpolynom ist, wissen wir, dass $|\mathcal{N}| < \infty$ gilt. Insbesondere ist $(0,1)\setminus \mathcal{N}$ nicht leer. Sei $y \in (0,1) \setminus \mathcal{N}$. Da $\sigma$ surjektiv nach $(0,1)$ ist, existiert ein $t_{\sigma} \in \R$ mit $\sigma(t_{\sigma}) = y.$ Per Konstruktion ist $\sigma^{(n)}(t_{\sigma}) \neq 0.$
Damit ist Bedingung (ii) ebenfalls erfüllt.

Nun zu Bedingung (iii). Wir wissen, dass für $x \in \R$ und damit insbesondere für ein beliebiges $x > 0$ die Ungleichung $$ x \leq \exp(x) + 1$$ gilt.
Daraus folgt die Ungleichung $x \cdot \exp(-x) \leq 1 + \exp(-x)$ und schließlich $\frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x}$ für alle $x > 0$. Damit erhalten wir nun
\begin{equation}
\label{sigma}
|\sigma(x) - 1| = 1 - \frac{1}{1 + \exp(-x)} = \frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x}
\end{equation}
für $x > 0$.
%Daraus erhalten wir mit Umformungen da $x > 0$ und $1 + \exp(-x) > 0$ ist:
%\begin{equation*}
%\begin{split}
%& \quad x \leq \exp(x) + 1 \\
% \Leftrightarrow & \quad x \cdot \exp(-x) \leq 1 + \exp(-x) \\
% \Leftrightarrow & \quad \frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x} \\
% \Leftrightarrow & \quad 1 - \frac{1}{1 + \exp(-x)} \leq \frac{1}{x} \\
% \Leftrightarrow & \quad |\sigma(x) - 1| \leq \frac{1}{x}.
%\end{split}
%\end{equation*}
%Wobei die letzte Ungleichung aus der Eigenschaft des Betrags kommt, da $\frac{1}{1 + \exp(-x)} - 1< 0$ ist, weil $1 + \exp(-x) > 1$, da $\exp(-x) > 0$. 
Dies zeigt den ersten Teil von Definition~\ref{nzulässig} (iii).

Der zweite Teil folgt auf die gleiche Art und Weise. Sei dazu $x < 0$. Aus
\begin{equation}
\label{eq:sym}
\frac{1}{1 + \exp(x)} - \frac{1}{2}= \sigma(0 - x) - \frac{1}{2} = -\sigma(0 + x) + \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2}
\end{equation}
wissen wir, dass $\sigma$ punktsymmetrisch zum Punkt $(0, \frac{1}{2})$ ist. 
Gleichung~(\ref{eq:sym}) folgt aus $\frac{1}{1 + \exp(x)} + \frac{1}{1 + \exp(-x)} = \frac{2 + \exp(-x) + \exp(x)}{2 + \exp(-x) + \exp(x)} = 1$.
Aus der Punktsymmetrie aus Gleichung~(\ref{eq:sym}) folgt zunächst $$\sigma(- x) - 1 = \frac{1}{1 + \exp(x)} - 1 = -\frac{1}{1 + \exp(-x)} = -\sigma(x)$$ für $x < 0$. Da $-x > 0$ ist, folgt mit Gleichung~\eqref{sigma}:  
$$|\sigma(x)| = |-\sigma(x)| = |\sigma(- x) - 1| \leq \frac{1}{-x} = \frac{1}{|x|}.$$
Damit haben wir alle drei Bedingungen aus Definition~\ref{nzulässig} gezeigt und unsere Aussage bewiesen.
\end{proof}
Als Nächstes stellen wir ein Resultat zur Taylorformel mit Rest vor, welches wir im weiteren Verlauf der Arbeit benötigen werden.

\begin{lem}[Lagrangesche Form des Restglieds, {\cite[§22, Satz 2]{forster2016}}]
\label{lem:lagrange} \ \\
Sei $I \subseteq \R$ ein Intervall und $f \colon I \to \R$ eine $(N + 1)$-mal stetig differenzierbare Funktion und $u, x \in I$. Dann existiert ein $\xi$ zwischen $u$ und $x$, so dass $$f(x) = \sum_{k = 0}^N \frac{f^{(k)}(u)}{k!}(x - u)^k + \frac{f^{(N + 1)}(\xi)}{(N + 1)!}(x - u)^{N + 1},$$
wobei hier $f^{(k)}$ für $k = 0,\dots,N+1$ die $k$-te Ableitung von $f$ bezeichnet.
\end{lem}
Es folgen nun drei Approximationsresultate, welche wir in Kapitel~\ref{chap:2} für die Konstruktion unseres Neuronale-Netze-Regressionsschätzers benötigen werden.

Wir betrachten als Erstes eine Approximation der Identität durch das neuronale Netz $f_{\id}\colon \R \to \R$ mit Architektur~$(1,(1))$, welches in Abbildung~\ref{fig:fid} veranschaulicht wird und anschließend die Approximation eines Quadrats auf einem Intervall durch das neuronale Netz $f_{\sq}\colon \R \to \R$ mit Architektur~$(1,(2))$.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
& & \\
};
\foreach \ai [count=\mi ]in {2}
  \draw[<-] (mat-\ai-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {2}
{\foreach \aii in {2}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2}
  \draw[->] (mat-\ai-2) -- (mat-2-3);
\draw[->] (mat-2-3) -- node[above] {$f_{\id}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\id}(x)$ mit Architektur $(1,(1))$.}
\label{fig:fid}
\end{figure}
 
\begin{lem}
  \label{lem:1}
  Sei $\sigma \colon \R \to \R$ eine beschränkte Funktion, $R$ und $a > 0.$
  \begin{itemize}
  \item[a)] Angenommen $\sigma$ ist zweimal stetig differenzierbar und $t_{\sigma,\id} \in \R$ so, dass $\sigma'(t_{\sigma, \id}) \neq 0$ ist. Dann gilt für das neuronale Netz
  $$ f_{\id}(x) = \frac{R}{\sigma'(t_{\sigma, \id})} \cdot \left(\sigma\left(\frac{x}{R} + t_{\sigma, \id}\right) - \sigma(t_{\sigma, \id})\right)$$
  für beliebiges $x \in [-a, a]\colon$ 
  $$ |f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R}.$$
  \item[b)] Angenommen $\sigma$ ist dreimal stetig differenzierbar und $t_{\sigma,\sq} \in \R$ so, dass $\sigma''(t_{\sigma, \sq}) \neq 0$ ist. Dann gilt für das neuronale Netz
  $$ f_{\sq}(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\sigma\bigg(\frac{2 \cdot x}{R} + t_{\sigma, \sq}\bigg) - 2 \cdot \sigma\bigg(\frac{x}{R} + t_{\sigma, \sq}\bigg)+ \sigma(t_{\sigma, \sq})\right)$$
  für beliebiges $x \in [-a, a]\colon$ 
  $$ |f_{\sq}(x) - x^2| \leq \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}.$$
  \end{itemize}
\end{lem}
\begin{proof}
	\begin{itemize}
  	\item[a)] Wir wissen, dass $f_{\id}$  zweimal differenzierbar ist, da $\sigma$ nach Voraussetzung zweimal stetig differenzierbar ist. Damit folgt mit Lemma~\ref{lem:lagrange} für $f = f_{\id}$, $N = 1$, $u = 0$ und $I = [-a, a]$, dass ein $\xi$ zwischen $0$ und $x$ existiert, mit
$$f_{\id}(x) = \sum_{k = 0}^1 \frac{f_{\id}^{(k)}(0)}{k!}x^k + \frac{f_{\id}^{(2)}(\xi)}{2!}x^{2}.$$
Es gilt:
$$
f_{\id}'(x) = \frac{1}{\sigma'(t_{\sigma,\id})} \cdot \sigma'\left(\frac{x}{R} + t_{\sigma,\id}\right).
$$
Mit $f_{\id}(0) = 0$ und $f_{\id}'(0) = 1$ erhalten wir:
  	\begin{equation*}
  	\begin{split}
  	 |f_{\id}(x) -  & x| \\
  	& = \bigg|0 + 1 \cdot x + \frac{1}{2} \cdot \frac{1}{R \cdot \sigma'(t_{\sigma,\id})} \sigma''\bigg(\frac{\xi}{R} + t_{\sigma,\id}\bigg) \cdot x^2 - x \bigg| \\[0.5em]
  	& = \bigg| \frac{\sigma''\left(\frac{\xi}{R} + t_{\sigma,\id}\right)  \cdot x^2}{2 \cdot R \cdot \sigma'(t_{\sigma, \id})} + x - x\bigg| \\[0.5em]
  	& \leq \frac{\| \sigma'' \|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R},
  	\end{split}
  	\end{equation*}
  	wobei sich die letzte Ungleichung aus den Eigenschaften der Supremumsnorm und $x^2 \leq a^2$ ergibt, was aus $x \in [-a,a]$ folgt. Daraus erhalten wir die Behauptung.
  	\item[b)]  Die Funktion $f_{\sq}$ ist dreimal differenzierbar, da $\sigma$ nach Voraussetzung dreimal stetig differenzierbar ist. Wie in a) folgt durch Lemma~\ref{lem:lagrange} mit $f = f_{\sq}$, $N = 2$, $u = 0$ und $I = [-a, a]$, dass ein $\xi$ zwischen $0$ und $x$ existiert, so dass:
$$f_{\sq}(x) = \sum_{k = 0}^2 \frac{f_{\sq}^{(k)}(0)}{k!}x^k + \frac{f_{\sq}^{(3)}(\xi)}{3!}x^{3}.$$
Es gilt:
$$
f_{\sq}'(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\frac{2}{R} \cdot\sigma'\bigg(\frac{2x}{R} + t_{\sigma, \sq}\bigg) - \frac{2}{R} \cdot \sigma'\bigg(\frac{x}{R} + t_{\sigma, \sq}\bigg)\right)
$$
und 
$$
f_{\sq}''(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \Bigg(\frac{4}{R^2} \cdot\sigma''\bigg(\frac{2x}{R} + t_{\sigma, \sq}\bigg) - \frac{2}{R^2} \cdot \sigma''\bigg(\frac{x}{R} + t_{\sigma, \sq}\bigg)\Bigg).
$$
Mit $f_{\sq}(0) = 0$, $f_{\sq}'(0) = 0$ und $f_{\sq}''(0) = 2$ erhalten wir:
\begin{equation*}
  	\begin{split}
  	 & |f_{\sq}(x) - x^2| \\
  	& \; = \Bigg|x^2 + \frac{1}{6} \cdot \frac{R^2}{\sigma''(t_{\sigma,\sq})} \Bigg(\frac{8}{R^3}\sigma'''\bigg(\frac{2\xi}{R} + t_{\sigma,\sq}\bigg) - \frac{2}{R^3} \sigma'''\bigg(\frac{\xi}{R} + t_{\sigma,\sq}\bigg)\Bigg) \cdot x^3 - x^2\Bigg| \\[0.5em]
  	& \; \leq \frac{a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \Bigg(8 \cdot \Bigg|\sigma'''\bigg(\frac{2\xi}{R} + t_{\sigma,\sq}\bigg)\Bigg| + 2 \cdot \bigg|\sigma'''\bigg(\frac{\xi}{R} + t_{\sigma,\sq}\bigg)\bigg|\Bigg) \\[0.5em]
  	& \; \leq \frac{10 \cdot a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \|\sigma'''\|_{\infty} \\[0.5em]
  	& \; = \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}. 
  	\end{split}
  	\end{equation*}
 	\end{itemize}
 	Daraus folgt die Behauptung.
\end{proof}
Wir betrachten als Nächstes die Approximation einer Multiplikation zweier Werte auf einem Intervall durch das neuronale Netz $f_{\mult}\colon \R^2 \to \R$ mit Architektur~$(1,(4))$, welches in Abbildung~\ref{fig:fmult} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| \\
& & |[plain]| \\
|[plain]| & |[plain]| & \\
& & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| \\
|[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-4-1) -- node[above] {$x$} +(-1cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$y$} +(-1cm,0);
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-5-3);
\draw[->] (mat-5-3) -- node[above] {$f_{\mult}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mult}(x, y)$ mit Architektur $(1,(4))$.}
\label{fig:fmult}
\end{figure}

\begin{lem}
  \label{lem:2}
  Sei $\sigma \colon \R \to [0, 1]$ 2-zulässig. Zudem sei $R > 0$ und $a > 0$ beliebig. Dann gilt für das neuronale Netz
  \begin{equation*}
  	\begin{split}
  	f_{\mult}(x, y) = \frac{R^2}{4 \cdot \sigma''(t_{\sigma})} \cdot & \bigg(\sigma\Big(\frac{2 \cdot (x + y)}{R} + t_{\sigma} \Big) - 2 \cdot \sigma\Big(\frac{x + y}{R} + t_{\sigma}\Big) \\[0.5em]
  	& - \sigma \Big(\frac{2 \cdot (x - y)}{R} + t_{\sigma} \Big) + 2 \cdot \sigma \Big(\frac{x - y}{R} + t_{\sigma} \Big) \bigg)
  	\end{split}
  	\end{equation*}
  	für beliebige $x, y \in [-a, a]$ folgende Ungleichung:
  	$$|f_{\mult}(x, y) - x \cdot y| \leq \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Durch Einsetzen von $f_{\sq}$ in $f_{\mult}$ erhalten wir $$f_{\mult}(x,y) = \frac{1}{4}(f_{\sq}(x + y) - f_{\sq}(x - y))$$ und durch Umformungen $$x \cdot y = \frac{1}{4}\big((x + y)^2 - (x - y)^2\big).$$
  Aus diesen beiden Gleichungen folgt durch Ausklammern von $\frac{1}{4}$, der Homogenität des Betrags und der Dreiecksungleichung:
  \begin{equation*}
  \begin{split}
  |f_{\mult}(x, y) - x \cdot y| & = \frac{1}{4} \cdot \big|f_{\sq}(x + y) - f_{\sq}(x - y) - (x + y)^2 + (x - y)^2\big| \\[0.5em]
  & \leq \frac{1}{4} \cdot \big|f_{\sq}(x + y) - (x + y)^2\big| + \frac{1}{4}\cdot\big|f_{\sq}(x - y) - (x - y)^2\big| \\[0.5em]
  & \leq 2 \cdot \frac{1}{4} \cdot \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \\[0.5em]
  & = \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R},
  \end{split}
\end{equation*}   
wobei wir bei der letzten Ungleichung Lemma~\ref{lem:1}~b) mit $x + y, x - y \in [-2a, 2a]$ verwendet haben. Daraus folgt die Behauptung.
  \end{proof}
Wir betrachten als Nächstes die Approximation der Maximumsfunktion $\max\{x, 0\}$ für $x$ aus einem beschränkten Intervall durch das neuronale Netz $f_{\ReLU}\colon \R \to \R$ mit Architektur~$(2,(2,4))$, welches in Abbildung~\ref{fig:frelu} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-5-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {5}
{\foreach \aii in {4,6}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-3) -- (mat-5-4);
\draw[->] (mat-5-4) -- node[above] {$f_{\ReLU}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\ReLU}(x)$ mit Architektur $(2,(2,4))$.}
\label{fig:frelu}
\end{figure}

  \begin{lem}
  \label{lem:3}
  Sei $\sigma\colon \R \to [0, 1]$ 2-zulässig und $a \geq 1$. Sei $f_{\id}$ das neuronale Netz aus Lemma~\ref{lem:1} und $f_{\mult}$ das neuronale Netz aus Lemma~\ref{lem:2}. Angenommen es gelte die Ungleichung 
\begin{equation}
\label{lem:3:Vor1}   
 R \geq \frac{\|\sigma''\|_{\infty} \cdot a}{2 \cdot |\sigma'(t_{\sigma})|}.
\end{equation}
  Dann erfüllt das neuronale Netz 
  \begin{equation}
  \label{lem:3:Vor2}
  \begin{split}
  f_{\ReLU}(x) & = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) 
  \end{split}
  \end{equation}
 für alle $x \in [-a, a]$ folgende Ungleichung:
 $$|f_{\ReLU}(x) - \max\{x, 0\}| \leq 56 \cdot \frac{\max\{\|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Da $\sigma$ nach Voraussetzung 2-zulässig ist, gilt für $R \geq 0$ und $x \in \R\setminus\{0\}\colon$ $$|\sigma(R \cdot x) - 1| \leq \frac{1}{R\cdot x} \text{\, für \,} x > 0$$ und $$|\sigma(R \cdot x)| \leq \frac{1}{|R \cdot x|} \text{\, für \,} x < 0.$$
Damit folgt aus der Homogenität des Betrags für alle $x \neq 0$: 
\begin{equation}
\label{lem:3:Bed1} 
|\sigma(R \cdot x) - \1_{[0, \infty)}(x)| \leq \frac{1}{|R \cdot x|} = \frac{1}{R \cdot |x|}.
\end{equation} 
Nach Lemma~\ref{lem:1} gilt:
\begin{equation}
\label{lem:3:Bed2} 
|f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x \in [-a, a]
\end{equation}
 und nach Lemma~\ref{lem:2}:
\begin{equation}
\label{lem:3:Bed3}
 |f_{\mult}(x, y) - x \cdot y| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x, y \in [-2a, 2a].
 \end{equation} 
Da nach Voraussetzung $a \geq 1$ ist, gilt insbesondere $[0, 1] \subseteq [-2a, 2a]$ und daher gilt insbesondere $\sigma(Rx) \in [0, 1]\subseteq [-2a, 2a].$ 
Zudem erhalten wir durch eine Nulladdition und die Dreiecksungleichung:
\begin{equation}
\label{eq:id1}
\begin{split}
|f_{\id}(x)| & = |f_{\id}(x) - x + x| \leq |f_{\id}(x) -x| + |x|.
\end{split}
\end{equation}
Aus Gleichung~(\ref{lem:3:Bed2}) und Voraussetzung~(\ref{lem:3:Vor1}) folgt
\begin{equation}
\label{eq:id2}
\begin{split}
|f_{\id}(x) -x| & \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R}
\leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{2 \cdot |\sigma'(t_{\sigma})|}{\|\sigma''\|_{\infty} \cdot a}
= a.
\end{split}
\end{equation}
Aus $x \in [-a, a]$ folgt schließlich mit den Ungleichungen~(\ref{eq:id1}) und (\ref{eq:id2}):
\begin{equation*}
|f_{\id}(x)| \leq |f_{\id}(x) -x| + |x| \leq 2 \cdot a.
\end{equation*}
Daraus folgt insbesondere $f_{\id}(x) \in [-2a, 2a].$
Mithilfe von $\max\{x, 0 \} = x \cdot \1_{[0, \infty)}(x)$, der Definition des Netzes in Gleichung~(\ref{lem:3:Vor2}), zweier Nulladditionen und der Dreiecksungleichung erhalten wir:
\begin{equation}
\label{eq:relu1}
\begin{split}
|f_{\ReLU}(x) - & \max\{x, 0\}|  \\
& = \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - x \cdot \1_{[0, \infty)}(x)\big| \\[0.5em]
& \leq \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \\[0.5em]
& \qquad + \big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| + \big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big|. 
\end{split}
\end{equation}
Wenden wir nun auf den ersten Summanden in Gleichung~(\ref{eq:relu1}) die Ungleichung~(\ref{lem:3:Bed3}) an, erhalten wir:
\begin{equation}
\label{eq:relu2}
\big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.
\end{equation}
Klammern wir im nächsten Schritt im zweiten Summanden in Gleichung~(\ref{eq:relu1}) den Faktor $\sigma(R \cdot x) \in [0, 1]$ aus und wenden Ungleichung~(\ref{lem:3:Bed2}) an, erhalten wir:
\begin{equation}
\label{eq:relu3}
\big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \cdot 1 \leq \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R},
\end{equation}
wobei wir verwendet haben, dass nach der Voraussetzung $a \geq 1$ und damit $a^2 \leq a^3$ gilt.

Wenn wir schließlich im dritten Summanden in Gleichung~(\ref{eq:relu1}) den Faktor $x$ durch die Homogenität des Betrags ausklammern und Ungleichung~(\ref{lem:3:Bed1}) anwenden, erhalten wir:
\begin{equation}
\label{eq:relu4}
\big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big| \leq \frac{1}{R}.
\end{equation}
Setzen wir nun Ungleichungen~(\ref{eq:relu2})-~(\ref{eq:relu4}) in Ungleichung~(\ref{eq:relu1}) ein, ergibt sich durch Ausklammern von $\frac{1}{R}$ und weiteren Abschätzungen:
\begin{equation*}
\begin{split}
|f_{\ReLU}(x) - \max\{x, 0\}| & \leq \bigg(\frac{160}{3} \cdot \frac{\|\sigma'''\|_{\infty} \cdot a ^3}{|\sigma''(t_{\sigma})|} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} + \frac{a^3}{a^3} \bigg) \cdot \frac{1}{R} \\[0.5em]
& \leq \bigg(\frac{160 \cdot\|\sigma'''\|_{\infty} \cdot a ^3 + 3 \cdot \|\sigma''\|_{\infty} \cdot a^3 + 3 \cdot a^3}{3 \cdot \min\{ 2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot \frac{1}{R}\\[0.5em]
& \leq \frac{166}{3} \cdot \bigg(\frac{\max\{\|\sigma'''\|_{\infty}, \|\sigma''\|_{\infty} , 1\}}{\min\{ 2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot a^3 \cdot  \frac{1}{R}.
\end{split}
\end{equation*}
Da $\frac{166}{3} \leq 56$ gilt, folgt schließlich die Behauptung.
  \end{proof}
Wir betrachten als Nächstes die Approximation des Positivteils einer Funktion auf einem Intervall durch das neuronale Netz $f_{\mathrm{hat},y}\colon \R \to \R$ mit Architektur $(2, (6, 12))$, welches in Abbildung~\ref{fig:fhat} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-14-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {14}
{\foreach \aii in {4,6,13,15,22,24}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {13,15}
{\foreach \aii in {11,13,15,17}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {22,24}
{\foreach \aii in {20,22,24,26}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8,11,13,15,17,20,22,24,26}
  \draw[->] (mat-\ai-3) -- (mat-14-4);
\draw[->] (mat-14-4) -- node[above] {$f_{\mathrm{\mathrm{hat}},y}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mathrm{\mathrm{hat}},y}(x)$ mit Architektur $(2, (6, 12))$.}
\label{fig:fhat}
\end{figure} 
  \begin{lem}
  \label{lem:4}
  Sei $M \in \N$ und sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $a > 0$ und $$R \geq \frac{\|\sigma''\|_{\infty} \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma})|},$$ sei $y \in [-a, a]$ und $f_{\ReLU}$ das neuronale Netz aus Lemma~\ref{lem:3}. Dann erfüllt das neuronale Netz 
  \begin{equation*}
  \begin{split}
  f_{\mathrm{hat},y}(x) = f_{\ReLU}& \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) \\[0.5em] &+ f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg)
  \end{split}
  \end{equation*}
  für alle $x \in [-a ,a]$ mit $z_+ = \max\{0, z\} \, (z \in \R):$   
  $$\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Für $x \in \R$ gilt die Gleichung:
\begin{equation}
\label{lem:4:eq}
 \bigg(1 - \frac{M}{2a} \cdot |x|\bigg)_+ = \max\bigg\{\frac{M}{2a} \cdot x + 1, 0\bigg\} - 2 \cdot \max\bigg\{\frac{M}{2a} \cdot x, 0\bigg\} + \max\bigg\{\frac{M}{2a} \cdot x - 1, 0\bigg\}, 
 \end{equation}  
die wir im zweiten Teil dieses Beweises zeigen werden. Damit beweisen wir das Resultat mit Hilfe von Lemma~\ref{lem:3}, denn mit der Definition von $f_{\mathrm{hat},y}(x)$ und der Dreiecksungleichung folgt:
\begin{equation*}
\begin{split}
\bigg|f_{\mathrm{hat},y}(x) - & \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \\[1em]
& \leq \bigg|f_{\ReLU} \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - \max\bigg\{\frac{M}{2a} \cdot (x - y) + 1, 0\bigg\}\bigg| \\[0.5em]
& \quad + 2 \cdot \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) - \max\bigg\{\frac{M}{2a} \cdot (x - y), 0\bigg\}\bigg| \\[0.5em]
& \quad + \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg) - \max\bigg\{\frac{M}{2a} \cdot (x - y) - 1, 0\bigg\}\bigg|\\[1em] 
& \leq 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*} 
wobei die letzte Ungleichung daraus folgt, dass wir auf jeden Summanden Lemma~\ref{lem:3} mit $$1 \leq a = M + 1$$ angewendet haben, da aus $x , y \in [-a, a]$ folgt, dass $\frac{M}{2a}\cdot (x - y) \in [-M, M]$ gilt. Schließlich haben wir
$$ (M + 1)^3 \leq (2M)^3 = 8M^3$$ verwendet, da $M \geq 1$ ist. $\hfill(\square)$

Um Gleichung~(\ref{lem:4:eq}) zu zeigen unterscheiden wir vier Fälle.
  \begin{itemize}
  \item[Fall 1] ($x < 0$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) nach der Definition des Betrags die Gestalt $$\max\bigg\{1 + \frac{M}{2a} \cdot x, 0\bigg\}$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\max\bigg\{\frac{M}{2a} \cdot x + 1, 0\bigg\} - 2 \cdot 0 + 0,$$ da $x < 0$ und damit die letzten zwei Summanden Null sind. Es erfordert hier eine weitere Fallunterscheidung.
 \item[Fall 1.1] ($0 > x \geq -\frac{2a}{M}$) In diesem Fall gilt für die linke und rechte Seite von Gleichung~(\ref{lem:4:eq}):
 $$\max\bigg\{1 + \frac{M}{2a} \cdot x, 0\bigg\} = 1 + \frac{M}{2a} \cdot x.$$
 \item[Fall 1.2] ($x < -\frac{2a}{M}$) In diesem Fall sind beide Seiten gleich Null, da $1 + \frac{M}{2a} \cdot x \leq 0$ ist. $\hfill(\square)$
  \item[Fall 2] ($x \geq 0$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) nach der Definition des Betrags die Gestalt $$\max\bigg\{1 - \frac{M}{2a} \cdot x, 0\bigg\}$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\max\bigg\{\frac{M}{2a} \cdot x + 1, 0\bigg\} - 2 \cdot \max\bigg\{\frac{M}{2a} \cdot x, 0\bigg\} + \max\bigg\{\frac{M}{2a} \cdot x - 1, 0\bigg\}$$ und erfordert daher eine weitere Fallunterscheidung.
 \item[Fall 2.1] ($0 \leq x < \frac{2a}{M}$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) die Gestalt $$1 - \frac{M}{2a} \cdot x$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + 0 = 1 - \frac{M}{2a} \cdot x$$ und stimmt daher mit der linken Seite überein.
 \item[Fall 2.2] ($x \geq \frac{2a}{M}$) In diesem Fall ist die linke Seite von Gleichung~(\ref{lem:4:eq}) gleich 0, da wir wissen, dass $1 - \frac{M}{2a} \cdot x < 0$ ist und die rechte Seite die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + \frac{M}{2a} \cdot x - 1 = 0$$
 besitzt. $\hfill(\square)$
\end{itemize} 
Durch diese Fallunterscheidung wurde die Gleichung~(\ref{lem:4:eq}) bewiesen und damit ist der Beweis vollständig.
  \end{proof}
Im nächsten Kapitel werden wir die hier eingeführten neuronalen Netze als Bausteine verwenden, um daraus unseren Neuronale-Netze-Regressionsschätzer zu konstruieren.