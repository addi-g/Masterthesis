\chapter{Grundlagen und Hilfsresultate}
\label{chap:1}

Der Zweck dieses Kapitels ist es, grundlegende Definitionen zu sammeln, die in den folgenden Kapiteln verwendet werden. Weiterhin werden wir Hilfsresultate darstellen und beweisen, welche wir für das Resultat der Konvergenzgeschwindigkeit des einfach berechenbaren Neuronale-Netze-Regressionsschätzer benötigen werden.

\section{Definitionen}
Es ist bekannt, dass man Glattheitsvoraussetzungen an die Regressionsfunktion haben muss, um nichttriviale Konvergenzresultate für nichtparametrische Regressionsschätzer herzuleiten. Dafür verwenden wir die folgende Definition.
\begin{defn}[($p,C$)-Glattheit]
\label{def:pc}
   Sei $p = q + s$ mit $q \in \N_0$ und $s \in (0,1]$ (also $p \in (0, \infty)$ und sei $C > 0$. Eine Funktion $f\colon \R^d \to \R$ heißt \emph{($p, C$)-glatt}, falls für alle $\alpha = (\alpha_1,\dots,\alpha_d) \in \N_0^d$ mit $\sum_{j = 1}^{d}\alpha_j = q$ die partielle Ableitung 
   $$ \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}$$
   existiert und falls für alle $x, z \in \R^d$ die Abschätzung 
   $$ \bigg|\frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(x) - \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(z) \bigg| \leq C \cdot \|x - z\|^r,$$
   gilt, wobei $\|\cdot\|$ die euklidische Norm ist.  
\end{defn}
\begin{bemnumber}
Im Falle von $p \leq 1$ ist eine Funktion genau dann ($p, C$)-glatt, wenn sie hölderstetig ist mit \emph{Exponent $p$} und \emph{Hölderkonstante $C$}.
\end{bemnumber}

Da wir uns in dieser Arbeit mit neuronalen Netzen beschäftigen, ist es hilfreich zu wissen was man darunter versteht. Ein neuronales Netz ist nichts anderes als eine Ansammlung von Neuronen, welche als Informationsverarbeitungseinheiten dienen, die schichtweise in einer Netzwerkarchitektur angeordnet sind. Beginnend mit der Eingabeschicht (\textit{Input Layer}) fließen Informationen über eine oder mehrere verborgene Schichten (\textit{Hidden Layer}) bis hin zur Ausgabeschicht (\textit{Output Layer)}. Die Informationsweitergabe der Neuronen verläuft so, dass sie die Eingaben $x_1,\dots,x_n$, die einerseits aus dem beobachteten Prozess resultieren können, dessen Werte dem Neuron übergeben werden, oder wiederum aus den Ausgaben anderer Neuronen stammen, verarbeiten und entsprechend über seine Aktivierung reagieren. Dazu werden für ein künstliches Neuron $j$ die Eingaben mit $w_{1_j}, \dots, w_{n_j}$ gewichtet an eine Aktivierungsfunktion $\sigma$ übergeben, welche die Neuronenaktivierung berechnet. Der Endpunkt des Informationsflusses in einem neuronalen Netz ist die Ausgabeschicht, die hinter den verborgenen Schichten liegt. Sie bildet damit die letzte Schicht in einem neuronalen Netz. Die Ausgabeschicht enthält somit das Ergebnis der Informationsverarbeitung durch das Netz.  
%\begin{figure}
%    \centering
%    \begin{tikzpicture}[
%        % define styles    
%        init/.style={ 
%             draw, 
%             circle, 
%             inner sep=2pt,
%             font=\Huge,
%             join = by -latex
%        },
%        squa/.style={ 
%            font=\Large,
%            join = by -latex
%        }
%    ]
%        
%        % Top chain x1 to w1
%        \begin{scope}[start chain=1]
%            \node[on chain=1] at (0,1.5cm)  (x1) {$x_1$};
%            \node[on chain=1,join=by o-latex] (w1) {$w_1$};
%        \end{scope}
%        
%        % Middle chain x2 to output
%        \begin{scope}[start chain=2]
%            \node[on chain=2] (x2) {$x_2$};
%            \node[on chain=2,join=by o-latex] {$w_2$};
%            \node[on chain=2,init] (sigma) {$\displaystyle\Sigma$};
%            \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation\\ function}}]   {$f_{act}$};
%            \node[on chain=2,squa,label=above:Output,join=by -latex] {$y_{out}$};
%        \end{scope}
%        
%        % Bottom chain x3 to w3
%        \begin{scope}[start chain=3]
%            \node[on chain=3] at (0,-1.5cm) 
%            (x3) {$x_3$};
%            \node[on chain=3,label=below:Weights,join=by o-latex]
%            (w3) {$w_3$};
%        \end{scope}
%        
%        % Bias
%        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
%        
%        % Arrows joining w1, w3 and b to sigma
%        \draw[-latex] (w1) -- (sigma);
%        \draw[-latex] (w3) -- (sigma);
%        \draw[o-latex] (b) -- (sigma);
%        
%        % left hand side brace
%        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
%        
%    \end{tikzpicture}
%    
%    \caption{Neural network pipeline}
%    
%\end{figure}
Zwei wichtige Charakteristika, die neuronale Netze aufweisen können, sind:
\begin{itemize}
\item Wenn in einem neuronalen Netz die Information von der Eingabeschicht über die verborgenen Schichten bis hin zur Ausgabeschicht in eine Richtung (\glqq vorwärts\grqq) weitergereicht wird, spricht man von einem \textit{feedforward} neuronalen Netz.
\item Ein neuronales Netz wird als ein \textit{fully connected} (\glqq vollständig verbundenes\grqq\@) neuronales Netz bezeichnet, wenn sämtliche Neuronen einer Schicht mit allen der darauffolgenden verbunden sind. Jedes Neuron der verborgenen Schicht ist also mit allen Neuronen der Eingabeschicht verbunden und ebenso ist jedes Neuron der Ausgabeschicht mit allen der letzten verborgenen Schicht verbunden.
\end{itemize}
Für ein neuronales Netz sind die Aktivierungsfunktionen von großer Bedeutung, da sie dabei helfen, das wirklich komplizierte und nichtlineare komplexe funktionale Mapping zwischen den Eingangsdaten und den abhängigen Ergebnissen zu lernen und zu verstehen.
\begin{defn}
Eine \emph{Aktivierungsfunktion} ist eine Transformation, die wir über die Eingabe durchführen, bevor wir sie an die nächste Schicht von Neuronen senden oder sie als Ausgabe finalisieren.
\end{defn}
\begin{defn}
Ein \emph{neuronales Netz} ist eine Funktion $f\colon X \to Y$, wobei $f(x)$ für $x \in X$, definiert ist als Komposition von weiteren Funktionen $g_i \colon X \to Y$, die weiter in andere Funktionen zerlegt werden können.
Die Funktion $f$ ist eine nichtlinear gewichtete Summe, mit $f(x) =  \sigma(\sum_iw_ig_i(x))$, wobei $w_i$ die Gewichte sind und $\sigma$ eine Aktivierungsfunktion ist.
\end{defn}
Als nächstes Definieren wir, was wir unter einer Netzwerkarchitektur verstehen um damit anschließend eine Definition eines mehrschichtigen feedforward neuronalen Netzes zu konstruieren.
\begin{defn}
Eine \emph{Netzwerkarchitektur $(L, \bk)$} ist eine Klasse aller neuronaler Netze $f$ für die gilt:
\begin{itemize}
\item Das neuronale Netz hat $L \in \N_0$ verborgenen Schichten.
\item Die Anzahl an Neuronen in jeder verborgenen Schicht wird durch den Vektor $\bk = (k_1,\dots,k_L) \in \N^L$ angegeben.
\end{itemize}
Mit dieser Definition können wir nun sagen, dass eine Netzwerkarchitektur wie ein Grundgerüst für die Konstruktion weiterer neuronale Netze ist.
\end{defn}
\begin{defn}
\label{def:nn}
Ein \emph{mehrschichtiges feedforward neuronales Netz mit Architektur $(L, \bk)$ und Aktivierungsfunktion} $\sigma$, ist eine reellwertige Funktion $f\colon \R^d \to \R$ definiert durch:
\begin{align}
\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{mit Gewichten $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ mit $i = 1,\dots,k_L$ rekursiv definiert durch:} 
f_i^{(r)}(x) & = \sigma \bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg)\\
\intertext{mit Gewichten $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R$ mit $r = 2,\dots, L$ und:}
f_i^{(1)}(x) & = \sigma \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber
\end{align} 
für die Gewichte $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$.
\end{defn}
Da wir bei Neuronale-Netze-Regressionsschätzern Funktionswerte schätzen möchten, haben wir in der Ausgabeschicht nur ein Neuron. Damit erreichen wir einen eindimensionalen Output und nehmen als Aktivierungsfunktion die Identität.
Man könnte auch für jede Schicht eine andere Aktivierungsfunktion wählen aber beschränkten uns in dieser Arbeit nur auf den Fall dass wir in allen verborgenen Schichten die selbe Aktivierungsfunktion verwenden.

Abbildung~\ref{fig:DNN} zeigt schematisch ein mehrschichtiges fully connected feedforward neuronales Netz, welches aus einer Eingabeschicht (\textit{Input feature 1} - \textit{Input feature $n_{\mathrm{in}}$}), $n_{\mathrm{lyr}}$ verborgenen Schichten und einer Ausgabeschicht (\textit{Output 1} - \textit{Output $n_{\mathrm{out}}$}) besteht.
\begin{center}
    \begin{figure}
        \begin{tikzpicture}[
            plain/.style={
              draw=none,
              fill=none,
              },
            dot/.style={draw,shape=circle,minimum size=3pt,inner sep=0,fill=black
              },
            net/.style={
              matrix of nodes,
              nodes={
                draw,
                circle,
                inner sep=8.5pt
                },
              nodes in empty cells,
              column sep=0.1cm,
              row sep=-11pt
              },
            >=latex
            ]
            \matrix[net] (mat)
            {
            |[plain]| \parbox{1cm}{\centering \small Input\\layer} 
                        & |[plain]| \parbox{1.2cm}{\centering \small Hidden\\layer 1} 
                                    &  |[plain]| \parbox{0cm}{\centering ...} 
                                                &  |[plain]| \parbox{1.4cm}{\centering \small Hidden\\layer $n_{\mathrm{lyr}}$} 
                                                                & |[plain]| \parbox{1cm}{\centering \small Output\\layer} \\
                        & |[plain]| & |[plain]| & |[plain]|     & |[plain]|                                   \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   & |[plain]|   & |[plain]|  & |[plain]|                \\
                        & |[dot]|  & |[plain]| & |[dot]|    & |[dot]|      \\
            |[plain]|   & |[dot]|   & |[plain]| \parbox{0cm}{\centering ...} & |[dot]|       & |[dot]|     \\
            |[dot]|     & |[dot]|  & |[plain]| & |[dot]|     & |[dot]|      \\
            |[dot]|     & |[plain]|    & |[plain]|   & |[plain]|       & |[plain]|    \\
            |[dot]|     & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]|            \\
            };
            \foreach \ai/\mi in {2/\small Input feature 1,4/\small Input feature 2,6/ \small Input feature 3,12/\small Input feature $n_{\mathrm{in}}$}
              \draw[<-] (mat-\ai-1) -- node[above] {\mi} +(-3cm,0);

            \foreach \ai in {2,4,6,12}
            {\foreach \aii/\mii in {3/,11/ }
                \draw[->] (mat-\ai-1) -- (mat-\aii-2) node[yshift=0cm] {\mii};
            }
            \foreach \ai in {3,11}
            {  
                \draw[->] (mat-\ai-4) -- (mat-4-5);
            }
            \draw[->] (mat-4-5) -- node[above] {\small Output 1} +(2.5cm,0);\
            \foreach \ai in {3,11}
            {
                \draw[->] (mat-\ai-4) -- (mat-10-5);
            }
            \draw[->] (mat-10-5) -- node[above] {\small Output $n_{\mathrm{out}}$} +(2.5cm,0);
        \end{tikzpicture}
        \caption{Fully connected feedforward neuronales Netz mit einer Eingabeschicht mit $n_{\mathrm{in}}$ Neuronen, $n_{\mathrm{lyr}}$ vielen verborgenen Schichten dessen Anzahl an Neuronen variieren kann und einer Ausgabeschicht bestehend aus $n_{\mathrm{out}}$ Neuronen.}
        \label{fig:DNN}
    \end{figure}
\end{center}
Eines der Ausgangspunkte für die Definition eines neuronalen Netzes ist die Wahl einer Aktivierungsfunktion $\sigma\colon \R \to \R$. Wir haben uns in dieser Arbeit für die sogenannten \textit{squashing functions} entschieden, welche eine monoton wachsend Funktion ist, für die $\lim_{x \to -\infty}\sigma(x) = 0$ und $\lim_{x \to \infty}\sigma(x) = 1$ gilt. Ein Beispiel für eine squashing function ist der sogenannte \emph{sigmoidal} bzw.\@ \emph{logistische squasher}
\begin{align}
\label{logsquasher}
\sigma(x) = \frac{1}{1 + \exp(-x)} \quad (x \in \R).
\end{align}

\begin{defn}
\label{nzulässig}
Sei $N \in \N_0$. Eine Funktion $\sigma\colon \R \to [0, 1]$ wird \emph{$N$-zulässig} genannt, wenn sie monoton wachsend und lipschitzsteig \cite{forster2016} ist und wenn zusätzlich die folgenden drei Bedingungen erfüllt sind:
\begin{itemize}
\item[(i)] Die Funktion $\sigma$ ist $N + 1$ mal stetig differenzierbar mit beschränkten Ableitungen.
\item[(ii)] Es existiert ein Punkt $t_{\sigma} \in \R$, in welchem alle Ableitungen bis hin zur $N$-ten Ableitung von $\sigma$ ungleich Null sind.
\item[(iii)] Wenn $y > 0$ ist, gilt $|\sigma(y) - 1| \leq \frac{1}{y}$. Wenn $y < 0$ ist, gilt $|\sigma(y)| \leq \frac{1}{|y|}$.
\end{itemize}  
\end{defn}  

In Lemma~\ref{lem:logsquasher} werden wir zudem zeigen, dass der logistische squasher~(\ref{logsquasher}) $N$-zulässig ist für beliebiges $N \in \N_0.$ 

Als geben wir eine Definition von wir Überdeckungszahlen an, da wir im Beweis für unser Hauptresultat eine Abschätzung einer $L_p\text{-}\epsilon$-Überdeckungszahl anwenden.
\begin{defn}
\label{ueberdeckung}
$(X, d)$ sei ein halbmetrischer Raum \cite{forster2016} . Für $x \in X$ und $\epsilon > 0$ sei:
$$U_{\epsilon}(x) = \{z \in X : d(x, z) < \epsilon\}$$
die Kugel um $x$ mit Radius $\epsilon$.
\begin{itemize}
\item[a)] $\{z_1,\dots,z_N\} \subseteq X$ heißt $\epsilon$\textit{-Überdeckung} einer Menge $A \subseteq X$, falls gilt:
$$A \subseteq \bigcup_{k = 1}^N U_{\epsilon}(z_k).$$
\item[b)] Ist $A \subseteq X$ und $\epsilon > 0$, so ist die sogenannte $\epsilon$\textit{-Überdeckungszahl} von $A$ in $(X,d)$ definiert als:
$$\mathcal{N}_{(X,d)}(\epsilon, A) = \inf\big\{|U| : U \subseteq X \text{ ist } \epsilon\text{-Überdeckung von } A\big\}.$$   
\end{itemize}
\end{defn} 
\begin{defn}
\label{lpe}
Sei $\mathcal{F}$ eine Menge von Funktionen $f\colon \R^d \to \R$, sei $\epsilon > 0$, $1 \leq p < \infty$ und seien $x_1,\dots,x_n \in \R^d$ und $x_1^n = (x_1,\dots,x_n).$ Dann ist die $L_p$-$\epsilon$\textit{-Überdeckungszahl} von $\mathcal{F}$ auf $x_1^n$ definiert durch:
$$\mathcal{N}_p(\epsilon, \mathcal{F}, x_1^n) \coloneqq \mathcal{N}_{(X,d)}(\epsilon, \mathcal{F}),$$
wobei der halbmetrische Raum $(X, d)$ gegeben ist durch
\begin{itemize}
\item $X = $ Menge aller Funktionen $f\colon \R^d \to \R$,
\item $d(f, g) = d_p(f, g) = (\frac{1}{n}\sum_{i = 1}^n |f(x_i) - g(x_i)|^p)^{1/p} .$
\end{itemize}
\end{defn}
In anderen Worten: $\mathcal{N}_p(\epsilon, \mathcal{F}, x_1^n)$ ist das minimal $N \in \N$, so dass Funktionen $f_1,\dots,f_N\colon \R^d \to \R$ existieren mit der Eigenschaft, dass für jedes $f \in \mathcal{F}$ gilt:
$$\min_{j = 1,\dots,N}(\frac{1}{n}\sum_{i = 1}^n|f(x_i) - f_j(x_i)|^p)^{1/p} < \epsilon.$$
In Lemma~\ref{lem:9} liefern wir ein Resultat zur Abschätzung einer $L_p$-$\epsilon$-Überdeckungszahl.

\section{Hilfsresultate}
\begin{lem}
\label{lem:logsquasher}
Sei $N \in \N_0$ beliebig, dann erfüllt der logistische squasher $\sigma\colon \R \to [0, 1], \sigma(x) = \frac{1}{1 + \exp(-x)}$ die Bedingungen aus Definition~\ref{nzulässig}.  
\end{lem}
\begin{proof}
Sei $N \in \N_0$ beliebig. Wir wissen, dass $\sigma$ monoton wachsend ist, da für beliebige $s, t \in \R$ mit $s \leq t$ gilt:
$$\sigma(s) = \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} = \sigma(t),$$
wobei wir bei der Ungleichung die Monotonie der Exponentialfunktion verwendet haben und die obige Ungleichung aus
\begin{equation*}
\begin{split}
& \quad \exp(s) \leq \exp(t) \\
 \Leftrightarrow & \quad \exp(-s) \geq \exp(-t) \\
 \Leftrightarrow & \quad 1 + \exp(-s) \geq 1 + \exp(-t) \\
 \Leftrightarrow & \quad \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} \\
\end{split}
\end{equation*}
folgt. Zudem ist $\sigma$ als Komposition glatter Funktionen, $N + 1$ mal stetig differenzierbar ist. Die Ableitungen von $\sigma$ haben die Form:
\begin{equation*}
\begin{split}
\frac{\partial \sigma}{\partial x}(x) &= -\frac{1}{(1 + \exp(-x))^2} \cdot (-\exp(-x)) \\
& = \frac{\exp(-x)}{1 + \exp(-x)} \cdot \frac{1}{1 + \exp(-x)} \\
& = \bigg(1 - \frac{1}{1 + \exp(-x)}\bigg) \cdot \frac{1}{1 + \exp(-x)} \\
& = (1 - \sigma(x)) \cdot \sigma(x).
\end{split}
\end{equation*}
Da wir bei weiterem Ableiten die Produktregel wiederholt anwenden sind alle Ableitungen von $\sigma$, Polynome in $\sigma$. Dadurch folgt Bedingung (i) aus Definition~\ref{nzulässig}, da $\sigma$ nach Voraussetzung durch $0$ und $1$ beschränkt ist, und die Ableitungen von $\sigma$ als Produkt von beschränkten Faktoren daher auch. Da hiermit auch die erste Ableitung von $\sigma$ beschränkt ist wissen wir, dass $\sigma$ lipschitzstetig ist.

Nun kommen wir zum Beweis von Bedingung (ii). Polynome, die nicht das $0$-Polynom sind, haben nach Satz ... (REFERENZ) auf $(0, 1)$ endlich viele Nullstellen und $\sigma$ bildet nach Voraussetzung in das Intervall $[0, 1] \supseteq (0, 1)$ ab. Da die Ableitungen von $\sigma$, als Zusammensetzung von Polynomen in $\sigma$, wieder Polynome sind für die die obere Eigenschaft ebenfalls gilt, existiert ein $t_{\sigma} \in \R$ mit $\sigma(t_{\sigma}) \neq 0$, sodass alle Ableitungen bis zum Grad $N$ von $\sigma$, aufgrund ihrer Struktur ungleich $0$ sind. Daher ist Bedingung (ii) ebenfalls erfüllt.

Wir wissen, dass für $x \in R$ und damit insbesondere für ein beliebiges $x > 0$: $$ x \leq \exp(x) + 1$$ gilt. Daraus erhalten wir mit Umformungen da $x > 0$ und $1 + \exp(-x) > 0$ ist:
\begin{equation*}
\begin{split}
& \quad x \leq \exp(x) + 1 \\
 \Leftrightarrow & \quad x \cdot \exp(-x) \leq 1 + \exp(-x) \\
 \Leftrightarrow & \quad \frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x} \\
 \Leftrightarrow & \quad 1 - \frac{1}{1 + \exp(-x)} \leq \frac{1}{x} \\
 \Leftrightarrow & \quad |\sigma(x) - 1| \leq \frac{1}{x}.
\end{split}
\end{equation*}
Wobei die letzte Ungleichung aus der Eigenschaft des Betrags kommt, da $\frac{1}{1 + \exp(-x)} - 1< 0$ ist, weil $1 + \exp(-x) > 1$, da $\exp(-x) > 0$. Dies zeigt die erste Relation aus Bedingung (iii).
Die zweite Relation folgt durch die gleiche Art und Weise, da wir durch $$\frac{1}{1 + \exp(x)} - \frac{1}{2}= \sigma(0 - x) - \frac{1}{2} = -\sigma(0 + x) + \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2}$$ 
wissen, dass $\sigma$ punktsymmetrisch in $(0, \frac{1}{2})$ ist. Die obige Gleichheit folgt aus
\begin{equation*}
\begin{split}
 & \quad \frac{1}{1 + \exp(x)} - \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2} \\
 \Leftrightarrow  & \quad \frac{1}{1 + \exp(x)} - \frac{1}{2} +\frac{1}{1 + \exp(-x)} = \frac{1}{2}\\
 \Leftrightarrow & \quad  \frac{1 + \exp(-x) + 1 + \exp(x)}{(1 + \exp(x)) \cdot (1 + \exp(-x))} - \frac{1}{2} = \frac{1}{2} \\
 \Leftrightarrow & \quad \frac{2 + \exp(-x) + \exp(x)}{2 + \exp(-x) + \exp(x)} - \frac{1}{2} = \frac{1}{2} \\
 \Leftrightarrow & \quad 1 - \frac{1}{2} = \frac{1}{2}.
\end{split}
\end{equation*}
Aus dieser Eigenschaft folgt mit $$\sigma(- x) - 1 = \frac{1}{1 + \exp(x)} - 1 = -\frac{1}{1 + \exp(-x)} = -\sigma(x)$$ für $x < 0$ aus der ersten Relation, da $-x > 0 $ ist:  
$$|\sigma(x)| = |-\sigma(x)| = |\sigma(- x) - 1| \leq \frac{1}{-x} = \frac{1}{|x|}.$$
Damit haben wir alle drei Bedingungen aus Definition~\ref{nzulässig} gezeigt und unsere Aussage bewiesen.
\end{proof}
\begin{lem}[Langrangesche Form des Restglieds \cite{forster2016}]
\label{lem:lagrange}
Sei $f \colon I \to \R$ eine $(N + 1)$-mal stetig differenzierbare Funktion und $u, x \in I$. Dann existiert ein $\xi \in [u, x],$ so dass $$f(x) = \sum_{k = 0}^N \frac{f^{(k)}(u)}{k!}(x - u)^k + \frac{f^{(N + 1)}(\xi)}{(N + 1)!}(x - u)^{N + 1}.$$
\end{lem}
\begin{lem}
  \label{lem:1}
  Sei $\sigma \colon \R \to \R$ eine Funktion und $R$, $a > 0$.
  \begin{itemize}
  \item[a)] Angenommen $\sigma$ ist zwei mal stetig differenzierbar und $t_{\sigma,\id} \in \R$ so, dass $\sigma'(t_{\sigma, \id}) \neq 0$ ist. Dann gilt mit
  $$ f_{\id}(x) = \frac{R}{\sigma'(t_{\sigma, \id})} \cdot \left(\sigma\left(\frac{x}{R} + t_{\sigma, \id}\right) - \sigma(t_{\sigma, \id})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R}.$$
  \item[b)] Angenommen $\sigma$ ist drei mal stetig differenzierbar und $t_{\sigma,\sq} \in \R$ so, dass $\sigma''(t_{\sigma, \sq}) \neq 0$ ist. Dann gilt mit
  $$ f_{\sq}(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\sigma\left(\frac{2 \cdot x}{R} + t_{\sigma, \sq}\right) - 2 \cdot \sigma(\frac{x}{R} + t_{\sigma, \sq})+ \sigma(t_{\sigma, \sq})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\sq}(x) - x^2| \leq \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}.$$
  \end{itemize}
\end{lem}
\begin{proof}
	\begin{itemize}
  	\item[a)] Wir wissen, dass $f_{\id}$ 2-mal differenzierbar ist, da nach Vorraussetzung $\sigma$ 2-mal stetig differenzierbar ist. Damit folgt mit Lemma~\ref{lem:lagrange} für $f = f_{\id}, N = 1$ und $I = [-a, a]$, dass ein $\xi \in [0, x]$ existiert, so dass:
$$f_{\id}(x) = \sum_{k = 0}^N \frac{f_{\id}^{(k)}(0)}{k!}x^k + \frac{f_{\id}^{(N + 1)}(\xi)}{(N + 1)!}x^{N + 1}.$$
Mit $f_{\id}(0) = 0$ und $f_{\id}'(0) = 1$ erhalten wir:
  	\begin{equation*}
  	\begin{split}
  	 |f_{\id}(x) -  & x| \\
  	& = \bigg|0 + 1 \cdot x + \frac{1}{2} \cdot \frac{1}{R \cdot \sigma'(t_{\sigma,\id})} \sigma''(\frac{\xi}{R} + t_{\sigma,\id}) \cdot x^2 - x \bigg| \\
  	& = \bigg| \frac{\sigma''(\frac{\xi}{R} + t_{\sigma,\id})  \cdot x^2}{2R \cdot \sigma'(t_{\sigma, \id})} + x - x\big| \\
  	& \leq \frac{\| \sigma'' \|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R},  
  	\end{split}
  	\end{equation*}
  	Wobei sich die letzte Ungleichung aus den Eigenschaften der Supremumsnorm ergibt und zudem aus $x \in [-a,a] \Leftrightarrow -a \leq x \leq a$ durch Quadrieren der Ungleichung folgt, dass $x^2 \leq a^2$ ist.
  	\item[b)]  Die Funktion $f_{\sq}$ die hier nun 3-mal differenzierbar ist, da $\sigma$ nach Voraussetzung  3-mal stetig differenzierbar ist. Es wie in a) durch Lemma~\ref{lem:lagrange} mit $f = f_{\sq}, N = 2$ und $I = [-a, a]$, dass ein $\xi \in [0, x]$ existiert, so dass:
$$f_{\sq}(x) = \sum_{k = 0}^N \frac{f_{\sq}^{(k)}(0)}{k!}x^k + \frac{f_{\sq}^{(N + 1)}(\xi)}{(N + 1)!}x^{N + 1}.$$
Mit $f_{\sq}(0) = 0$, $f_{\sq}'(0) = 0$ und $f_{\sq}''(0) = 2$ erhalten wir:
\begin{equation*}
  	\begin{split}
  	 |f_{\sq}(x) -  & x^2| \\
  	& = \bigg|x^2 + \frac{1}{6} \cdot \frac{R^2}{\sigma''(t_{\sigma,\sq})} \left(\frac{8}{R^3}\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq}) - \frac{2}{R^3} \sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})\right) \cdot x^3 - x^2\bigg| \\
  	& \leq \frac{a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \left(8 \cdot |\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq})| + 2 |\sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})|\right) \\
  	& \leq \frac{10 \cdot a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \|\sigma'''\|_{\infty} \\
  	& = \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}. 
  	\end{split}
  	\end{equation*}
 	\end{itemize}
\end{proof}

\begin{lem}
  \label{lem:2}
  Sei $\sigma \colon \R \to [0, 1]$ 2-zulässig. Zudem sei $R > 0$ und $a > 0$ beliebig. Dann gilt für das neuronale Netz
  \begin{equation*}
  	\begin{split}
  	f_{\mult}(x, y) = \frac{R^2}{4 \cdot \sigma''(t_{\sigma})} \cdot & \bigg(\sigma\Big(\frac{2 \cdot (x + y)}{R} + t_{\sigma} \Big) - 2 \cdot \sigma\Big(\frac{x + y}{R} + t_{\sigma}\Big) \\
  	& - \sigma \Big(\frac{2 \cdot (x - y)}{R} + t_{\sigma} \Big) + 2 \cdot \sigma \Big(\frac{x - y}{R} + t_{\sigma} \Big) \bigg)
  	\end{split}
  	\end{equation*}
  	für beliebige $x, y \in [-a, a]$ die folgende Ungleichung:
  	$$|f_{\mult}(x, y) - x \cdot y| \leq \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Durch Einsetzen erhalten wir $$f_{\mult}(x,y) = \frac{1}{4}(f_{\sq}(x + y) - f_{\sq}(x - y))$$ und $$x \cdot y = \frac{1}{4}\big((x + y)^2 - (x - y)^2\big).$$
  Aus diesen beiden Gleichungen folgt durch Ausklammern von $\frac{1}{4}$, der Homogenität des Betrags und der Anwendung der Dreiecksungleichung:
  \begin{equation*}
  \begin{split}
  |f_{\mult}(x, y) - x \cdot y| & = \frac{1}{4} \cdot \big|f_{\sq}(x + y) - f_{\sq}(x - y) - (x + y)^2 + (x - y)^2\big| \\
  & \leq \frac{1}{4} \cdot \big|f_{\sq}(x + y) - (x + y)^2\big| + \frac{1}{4}\cdot\big| (x - y)^2 - f_{\sq}(x - y)\big| \\
  & \leq 2 \cdot \frac{1}{4} \cdot \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \\
  & = \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R},
  \end{split}
\end{equation*}   
wobei wir bei der letzten Ungleichung verwendet haben, dass $a > 0$ nach Lemma~\ref{lem:1}b) beliebig gewählt wurde und daher insbesondere für beliebiges $x \in [-2a,2a]$ $$ |f_{\sq}(x) - x^2| \leq \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}$$ gilt. 
  \end{proof}
  \begin{lem}
  \label{lem:3}
  Sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $f_{\mult}$ das neuronale Netz aus Lemma~\ref{lem:2} und $f_{\id}$ das neuronale Netz aus Lemma~\ref{lem:1}. Angenommen es gelten die Ungleichungen 
\begin{equation}
\label{lem:3:Vor1}   
  a \geq 1 \text{\quad und \quad} R \geq \frac{\|\sigma''\|_{\infty} \cdot a}{2 \cdot |\sigma'(t_{\sigma})|}.
\end{equation}
  Dann erfüllt das neuronale Netz 
  \begin{equation}
  \label{lem:3:Vor2}
  \begin{split}
  f_{\ReLU}(x) & = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) 
  \end{split}
  \end{equation}
 für alle $x \in [-a, a]$ folgende Ungleichung:
 $$|f_{\ReLU}(x) - \max\{x, 0\}| \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Da $\sigma$ nach Voraussetzung 2-zulässig nach Definition 1 ist, gilt für $R \geq 0,$ und $x \in \R\setminus\{0\}\colon$ $$|\sigma(R \cdot x) - 1| \leq \frac{1}{R\cdot x} \text{\, für \,} x > 0$$ und $$|\sigma(R \cdot x)| \leq \frac{1}{|R \cdot x|} \text{\, für \,} x < 0.$$
Damit folgt aus der Homogenität des Betrags für alle $x \neq 0$: 
\begin{equation}
\label{lem:3:Bed1} 
|\sigma(R \cdot x) - \1_{[0, \infty)}(x)| \leq \frac{1}{|R \cdot x|} = \frac{1}{R \cdot |x|}.
\end{equation} 
Nach Lemma~\ref{lem:1} und Lemma~\ref{lem:2} gilt:
\begin{equation}
\label{lem:3:Bed2} 
|f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x \in [-a, a]
\end{equation}
 und 
\begin{equation}
\label{lem:3:Bed3}
 |f_{\mult}(x, y) - x \cdot y| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x, y \in [-2a, 2a].
 \end{equation} 
Da nach Voraussetzung $a \geq 1$ ist, gilt insbesondere $[0, 1] \subseteq [-2a, 2a]$ und daher gilt insbesondere $\sigma(Rx) \in [0, 1]\subseteq [-2a, 2a].$ 
Zudem erhalten wir durch eine Nulladdition, das Anwenden der Dreiecksungleichung, die Verwendung von Lemma~\ref{lem:1} und (\ref{lem:3:Vor1}) für $R$:
\begin{equation*}
\begin{split}
|f_{\id}(x)| & = |f_{\id}(x) - x + x| \\
& \leq |f_{\id}(x) -x| + |x| \\
&  \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} + |x| \\
& \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{2 \cdot |\sigma'(t_{\sigma})|}{\|\sigma''\|_{\infty} \cdot a} + |x| \\
& = a + |x| \\
& \leq 2 \cdot a
\end{split}
\end{equation*}
wobei $x \in [-a, a]$. Daraus folgt insbesondere $f_{\id}(x) \in [-2a, 2a].$
Mithilfe von $\max\{x, 0 \} = x \cdot \1_{[0, \infty)}(x)$, Gleichung (\ref{lem:3:Vor2}), zweier Nulladditionen und dem zweifachen Anwenden der Dreiecksungleichung erhalten wir:
\begin{equation*}
\begin{split}
|f_{\ReLU}(x) - & \max\{x, 0\}|  \\
& = \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - x \cdot \1_{[0, \infty)}(x)\big| \\
& \leq \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \\
& \qquad + \big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| + \big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big|. \\
& \text{Daraus ergibt sich mithilfe von (\ref{lem:3:Bed1}) - (\ref{lem:3:Bed3}), $\sigma(Rx) \in [0, 1]$ und $a^3 \geq 1$}\\
& \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \cdot 1+ \frac{1}{R} \\
& \leq \bigg(\frac{160}{3} \cdot \frac{\|\sigma'''\|_{\infty} \cdot a ^3}{|\sigma''(t_{\sigma})|} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} + \frac{a^3}{a^3} \bigg) \cdot \frac{1}{R} \\ 
& \leq \bigg(\frac{160 \cdot\|\sigma'''\|_{\infty} \cdot a ^3 + 3 \cdot \|\sigma''\|_{\infty} \cdot a^3 + 3 \cdot a^3}{3 \cdot \min\{ 2 \cdot \sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot \frac{1}{R}\\
& \leq \frac{166}{3} \cdot \bigg(\frac{\max\{\|\sigma'''\|_{\infty}, \|\sigma''\|_{\infty} , 1\}}{\min\{ 2 \cdot \sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot a^3 \cdot  \frac{1}{R} \\
& \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.
\end{split}
\end{equation*}
  \end{proof}
  \begin{lem}
  \label{lem:4}
  Sei $M \in \N$ und sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $a > 0$ und $$R \geq \frac{\|\sigma''\|_{}\infty \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma})|},$$ sei $y \in [-a, a]$ und $f_{\ReLU}$ das neuronale Netz aus Lemma~\ref{lem:3}. Dann erfüllt das neuronale Netz 
  \begin{equation*}
  \begin{split}
  f_{\mathrm{hat},y}(x) = f_{\ReLU}& \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) \\ &+ f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg)
  \end{split}
  \end{equation*}
  für alle $x \in [-a ,a]$ mit $z_+ = \max\{0, z\} \, (z \in \R):$   
  $$\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma, \id}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Für $x \in \R$ gilt die Gleichung:
\begin{equation}
\label{lem:4:eq}
 (1 - \frac{M}{2a} \cdot |x|)_+ = \max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}, 
 \end{equation}  
die wir im zweiten Teil dieses Beweises zeigen werden. Damit beweisen wir das Resultat mit Hilfe von Lemma~\ref{lem:3}, denn mit der Definition von $f_{\mathrm{hat},y}(x)$ und zwei mal der Dreiecksungleichung folgt:
\begin{equation*}
\begin{split}
\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq \bigg|&f_{\ReLU} \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) + 1, 0\}\bigg| \\ 
& + 2 \cdot \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) - \max\{\frac{M}{2a} \cdot (x - y), 0\}\bigg| \\
& + \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) - 1, 0\}\bigg|\\ 
& \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*} 
wobei die letzte Ungleichung daraus folgt, dass wir auf jeden Summanden mit $1 \leq a = M + 1$ Lemma~\ref{lem:3} angewendet haben und die Abschätzung 
$$ (M + 1)^3 \leq (2M)^3 = 8M^3$$ verwendet haben, da $M \geq 1$ ist. $\hfill(\square)$

Um Gleichung~(\ref{lem:4:eq}) zu zeigen unterscheiden wir vier Fälle.
  \begin{itemize}
  \item[Fall 1] ($x < 0$) In diesem Fall hat die linke Seite nach der Definition des Betrags die Gestalt $$\max\{1 + \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot 0 + 0,$$ da $x < 0$ und damit die letzten zwei Summanden 0 sind. Es erfordert hier eine weitere Fallunterscheidung.
 \item[Fall 1.1] ($0 > x \geq -\frac{2a}{M}$) In diesem Fall gilt für die linke und rechte Seite:
 $$\max\{1 + \frac{M}{2a} \cdot x, 0\} = 1 + \frac{M}{2a} \cdot x.$$
 \item[Fall 1.2] ($x < -\frac{2a}{M}$) In diesem Fall sind beide Seiten gleich 0, da $1 + \frac{M}{2a} \cdot x \leq 0$ ist. $\hfill(\square)$
  \item[Fall 2] ($x \geq 0$) In diesem Fall hat die linke Seite nach der Definition des Betrags die Gestalt $$\max\{1 - \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}$$ und erfordert daher eine weitere Fallunterscheidung.
 \item[Fall 2.1] ($0 \leq x < \frac{2a}{M}$) In diesem Fall hat die linke Seite die Gestalt $$1 - \frac{M}{2a} \cdot x$$ und die rechte Seite die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + 0 = 1 - \frac{M}{2a} \cdot x.$$ und stimmt daher mit der linken Seite überein.
 \item[Fall 2.2] ($x \geq \frac{2a}{M}$) In diesem ist die linke Seite gleich 0, da $1 - \frac{M}{2a} \cdot x < 0$ ist und die rechte Seite bestitz die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + \frac{M}{2a} \cdot x - 1 = 0.$$ $\hfill(\square)$
\end{itemize} 
Durch diese Fallunterscheidung wurde die Gleichung~(\ref{lem:4:eq}) bewiesen und damit ist der Beweis vollständig.
  \end{proof}
Das nächste Lemma ist ein Kombinatorikargument welches wir in Kapitel~\ref{chap:2} benötigen werden.
\begin{lem}
\label{lem:kombi}
Sei $d, N \in \N$ und $k \in \N_0$, dann gilt:
$$\big|\Bigl\{\bj \in [N]^d : |\bj|_1 = k \Bigr\}\big| = \binom{d + k - 1}{k}.$$
\end{lem}
\begin{proof}
Diese Aussage folgt aus einer Analogie zu einem Urnenexperiment. Wir betrachten eine Urne mit $d$-vielen Kugeln die wir mit $j_1,\dots,j_d$ beschriften. Wir ziehen $k$-Mal aus dieser Urne mit Zurücklegen und ohne Beachtung der Reihenfolge und konstruieren einen Vektor $(j_1,\dots,j_d$ welcher mit jeder Komponente angibt, wie oft welche Kugel gezogen wurde.
\end{proof}