\chapter{Grundlagen und Hilfsresultate}
\label{chap:1}

Der Zweck dieses Kapitels ist es, grundlegende Definitionen zu sammeln, die in den folgenden Kapiteln verwendet werden. Weiterhin werden wir Hilfsresultate darstellen und beweisen, welche wir für das Resultat der Konvergenzgeschwindigkeit des einfach berechenbaren Neuronale-Netze-Regressionsschätzer benötigen werden.

In dieser Arbeit behandeln wir Neuronale-Netze-Regressionsschätzer im Kontext der nichtparametrischen Regression mit zufälligem Design. 

Im Gegensatz zur parametrischen Regression ist bei der nichtparametrischen die Bauart der zu schätzenden Funktion komplett unbekannt, was den Vorteil besitzt, dass weniger Annahmen getroffen werden müssen, man aber dadurch noch mehr Daten benötigt, um eine Funktion zu schätzen.

Bei der nichtparametrischen Regressionsschätzung seien $(X, Y), (X_1, Y_1), (X_2, Y_2), \dots$ unabhängig identisch verteilte (u.\@i.\@v.\@) $\R^d \times \R$-wertige Zufallsvariablen mit $\E[Y^2] < \infty$. Zudem sei $m\colon \R^d \to \R$ definiert durch $m(x) = \E[Y \mid X = x]$ die zugehörige Regressionsfunktion. Ausgehend von der Stichprobe
$$ (X_1, Y_1),\dots,(X_n, Y_n)$$ 
soll $m$ geschätzt werden.

Das Problem der Regressionsschätzung bei zufälligem Design lässt sich wie folgt erläutern: 

In Anwendungsfällen ist üblicherweise die Verteilung von $(X, Y)$ unbekannt, daher kann $m(x) = \E[Y \mid X = x]$ nicht berechnet werden. Oft ist es aber möglich, Werte von $(X, Y)$ zu beobachten. Ziel ist es dann, daraus die Regressionsfunktion $m$ zu schätzen. Im Hinblick auf die Minimierung des $L_2$-Risikos sollte dabei der $L_2$-Fehler der Schätzfunktion möglichst klein sein. 

Für das $L_2$-Risiko einer beliebigen messbaren Funktion $f\colon \R^d \to \R$  gilt$\colon$
$$\E[|f(X) - Y|^2] = \E[|m(X) - Y|^2] + \int_{\R^d}|f(x) - m(x)|^2 \mathds{P}_X (dx),$$
d.h. der mittlere quadratische Vorhersagefehler einer Funktion ist darstellbar als Summe des $L_2$-Risikos der Regressionsfunktion (unvermeidbarer Fehler) und des $L_2$-Fehlers der entsteht aufgrund der Verwendung von $f$ an Stelle von $m$ bei der Vorhersage bzw.\@ Approximation des Wertes von Y.

Formal führt das daher auf folgende Problemstellung$\colon$
$(X, Y), (X_1, Y_1), (X_2, Y_2), \dots$ seien u.\@i.\@v.\@ $\R^d \times \R$ wertige Zufallsvariablen mit $\E[Y^2] < \infty$ und $m\colon\R^d \to \R$ definiert durch $m(x) = \E[Y \mid X = x]$ sei die zugehörige Regressionsfunktion. Gegeben sei die Datenmenge 
$$ \mathcal{D}_n = \{(X_1, Y_1),\dots,(X_n, Y_n)\}.$$
Gesucht ist eine Schätzung 
$$m_n(\cdot) = m_n(\cdot, \mathcal{D}_n)\colon\R^d \to \R $$
von $m$, für die der $L_2$-Fehler 
$$\int |m_n(x) - m(x)|^2 \mathds{P}_X(dx)$$
möglichst \glqq klein\grqq\@ ist. (Referenz Györfi (2002))

\section{Definitionen}
Es ist bekannt, dass man Glattheitsvoraussetzungen an die Regressionsfunktion haben muss, um nichttriviale Konvergenzresultate für nichtparametrische Regressionsschätzer herzuleiten. Dafür verwenden wir die folgende Definition.
\begin{defn}[($p,C$)-Glattheit]
\label{def:pc}
   Sei $p = q + s$ mit $q \in \N_0$ und $s \in (0,1]$ (also $p \in (0, \infty)$ und sei $C > 0$. Eine Funktion $f\colon \R^d \to \R$ heißt ($p, C$)-glatt, falls für alle $\alpha = (\alpha_1,\dots,\alpha_d) \in \N_0^d$ mit $\sum_{j = 1}^{d}\alpha_j = q$ die partielle Ableitung 
   $$ \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}$$
   existiert und falls für alle $x, z \in \R^d$ die Abschätzung 
   $$ \bigg|\frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(x) - \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(z) \bigg| \leq C \cdot \|x - z\|^r,$$
   gilt, wobei $\|\cdot\|$ die euklidische Norm ist.  
\end{defn}
\begin{bemnumber}
Im Falle von $p \leq 1$ ist eine Funktion genau dann ($p, C$)-glatt, wenn sie hölderstetig ist mit Exponent $p$ und Hölderkonstante $C$.
\end{bemnumber}

Da wir uns in dieser Arbeit mit neuronalen Netzen beschäftigen , ist es hilfreich zu wissen was man darunter versteht. Ein neuronales Netz ist nichts anderes als eine Ansammlung von Neuronen, welche als Informationsverarbeitungseinheiten dienen, die schichtweise in einer Netzarchitektur angeordnet sind. Beginnend mit der Eingabeschicht (\textit{Input Layer}) fließen Informationen über eine oder mehrere verborgene Schichten (\textit{Hidden Layer}) bis hin zur Ausgabeschicht (\textit{Output Layer)}. Die Informationsweitergabe der Neuronen verläuft so, dass sie die Eingaben $x_1,\dots,x_n$, die einerseits aus dem beobachteten Prozess resultieren können, dessen Werte dem Neuron übergeben werden, oder wiederum aus den Ausgaben anderer Neuronen stammen, verarbeiten und entsprechend über seine Aktivierung reagieren. Dazu werden für ein künstliches Neuron $j$ die Eingaben mit $w_{1_j}, \dots, w_{n_j}$ gewichtet an eine Aktivierungsfunktion $\sigma$ übergeben, welche die Neuronenaktivierung berechnet. Der Endpunkt des Informationsflusses in einem neuronalen Netz ist die Ausgabeschicht, die hinter den verborgenen Schichten liegt. Sie bildet damit die letzte Schicht in einem neuronalen Netz. Die Ausgabeschicht enthält somit das Ergebnis der Informationsverarbeitung durch das Netz.  
%\begin{figure}
%    \centering
%    \begin{tikzpicture}[
%        % define styles    
%        init/.style={ 
%             draw, 
%             circle, 
%             inner sep=2pt,
%             font=\Huge,
%             join = by -latex
%        },
%        squa/.style={ 
%            font=\Large,
%            join = by -latex
%        }
%    ]
%        
%        % Top chain x1 to w1
%        \begin{scope}[start chain=1]
%            \node[on chain=1] at (0,1.5cm)  (x1) {$x_1$};
%            \node[on chain=1,join=by o-latex] (w1) {$w_1$};
%        \end{scope}
%        
%        % Middle chain x2 to output
%        \begin{scope}[start chain=2]
%            \node[on chain=2] (x2) {$x_2$};
%            \node[on chain=2,join=by o-latex] {$w_2$};
%            \node[on chain=2,init] (sigma) {$\displaystyle\Sigma$};
%            \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation\\ function}}]   {$f_{act}$};
%            \node[on chain=2,squa,label=above:Output,join=by -latex] {$y_{out}$};
%        \end{scope}
%        
%        % Bottom chain x3 to w3
%        \begin{scope}[start chain=3]
%            \node[on chain=3] at (0,-1.5cm) 
%            (x3) {$x_3$};
%            \node[on chain=3,label=below:Weights,join=by o-latex]
%            (w3) {$w_3$};
%        \end{scope}
%        
%        % Bias
%        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
%        
%        % Arrows joining w1, w3 and b to sigma
%        \draw[-latex] (w1) -- (sigma);
%        \draw[-latex] (w3) -- (sigma);
%        \draw[o-latex] (b) -- (sigma);
%        
%        % left hand side brace
%        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
%        
%    \end{tikzpicture}
%    
%    \caption{Neural network pipeline}
%    
%\end{figure}
Zwei wichtige Charakteristika, die neuronale Netze aufweisen können, sind:
\begin{itemize}
\item Wenn in einem neuronalen Netz die Information von der Eingabesicht über die verborgenen Schichten bis hin zur Ausgabeschicht in eine Richtung (\glqq vorwärts\grqq) weitergereicht wird, spricht man von einem \textit{feedforward} neuronalen Netz.
\item Ein neuronales Netz wird als ein \glqq vollständig verbundenes\grqq\@ (\textit{fully connected}) neuronales Netz bezeichnet, wenn sämtliche Neuronen einer Schicht mit allen der darauffolgenden verbunden sind. Jedes Neuron der verborgenen Schicht ist also mit allen Neuronen der Eingabeschicht verbunden und ebenso ist jedes Neuron der Ausgabeschicht mit allen der (letzten) verborgenen Schicht verbunden.
\end{itemize}

Da wir uns in dieser Arbeit mit neuronalen Netzen zur Regressionsschätzung beschäftigen und Funktionswerte schätzen wollen, betrachten wir nur neuronale Netze die in der Ausgabeschicht ein einzelnes Neuron besitzen. 

Abbildung \ref{fig:DNN} zeigt schematisch ein mehrschichtiges fully connected feedforward neuronales Netz, welches aus einer Eingabeschicht (\textit{Input feature 1} - \textit{Input feature $n_{\mathrm{in}}$}), $n_{\mathrm{lyr}}$ verborgenen Schichten und einer Ausgabeschicht (\textit{Output 1} - \textit{Output $n_{\mathrm{out}}$}) besteht.
\begin{center}
    \begin{figure}
        \begin{tikzpicture}[
            plain/.style={
              draw=none,
              fill=none,
              },
            dot/.style={draw,shape=circle,minimum size=3pt,inner sep=0,fill=black
              },
            net/.style={
              matrix of nodes,
              nodes={
                draw,
                circle,
                inner sep=8.5pt
                },
              nodes in empty cells,
              column sep=0.1cm,
              row sep=-11pt
              },
            >=latex
            ]
            \matrix[net] (mat)
            {
            |[plain]| \parbox{1cm}{\centering \small Input\\layer} 
                        & |[plain]| \parbox{1.2cm}{\centering \small Hidden\\layer 1} 
                                    &  |[plain]| \parbox{0cm}{\centering ...} 
                                                &  |[plain]| \parbox{1.4cm}{\centering \small Hidden\\layer $n_{\mathrm{lyr}}$} 
                                                                & |[plain]| \parbox{1cm}{\centering \small Output\\layer} \\
                        & |[plain]| & |[plain]| & |[plain]|     & |[plain]|                                   \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   & |[plain]|   & |[plain]|  & |[plain]|                \\
                        & |[dot]|  & |[plain]| & |[dot]|    & |[dot]|      \\
            |[plain]|   & |[dot]|   & |[plain]| \parbox{0cm}{\centering ...} & |[dot]|       & |[dot]|     \\
            |[dot]|     & |[dot]|  & |[plain]| & |[dot]|     & |[dot]|      \\
            |[dot]|     & |[plain]|    & |[plain]|   & |[plain]|       & |[plain]|    \\
            |[dot]|     & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]|            \\
            };
            \foreach \ai/\mi in {2/\small Input feature 1,4/\small Input feature 2,6/ \small Input feature 3,12/\small Input feature $n_{\mathrm{in}}$}
              \draw[<-] (mat-\ai-1) -- node[above] {\mi} +(-3cm,0);

            \foreach \ai in {2,4,6,12}
            {\foreach \aii/\mii in {3/,11/ }
                \draw[->] (mat-\ai-1) -- (mat-\aii-2) node[yshift=0cm] {\mii};
            }
            \foreach \ai in {3,11}
            {  
                \draw[->] (mat-\ai-4) -- (mat-4-5);
            }
            \draw[->] (mat-4-5) -- node[above] {\small Output 1} +(2.5cm,0);\
            \foreach \ai in {3,11}
            {
                \draw[->] (mat-\ai-4) -- (mat-10-5);
            }
            \draw[->] (mat-10-5) -- node[above] {\small Output $n_{\mathrm{out}}$} +(2.5cm,0);
        \end{tikzpicture}
        \caption{Fully connected feedforward neuronales Netz mit einer Eingabeschicht mit $n_{\mathrm{in}}$ Neuronen, $n_{\mathrm{lyr}}$ vielen verborgenen Schichten dessen Anzahl an Neuronen variieren kann und einer Ausgabeschicht bestehend aus $n_{\mathrm{out}}$ Neuronen.}
        \label{fig:DNN}
    \end{figure}
\end{center}
Der Ausgangspunkt für die Definition eines neuronalen Netzes ist die Wahl einer Aktivierungsfunktion $\sigma\colon \R \to \R$. Wir haben uns in dieser Arbeit für die sogenannten \textit{squashing functions} entschieden, welche eine monoton wachsend Funktion ist, für die $\lim_{x \to -\infty}\sigma(x) = 0$ und $\lim_{x \to \infty}\sigma(x) = 1$ gilt. Ein Beispiel für eine squashing function ist der sogenannte sigmoidal bzw.\@ logistische squasher
\begin{align}
\label{logsquasher}
\sigma(x) = \frac{1}{1 + \exp(-x)} \quad (x \in \R).
\end{align}

\begin{defn}
\label{nzulässig}
Sei $N \in \N_0$. Eine Funktion $\sigma\colon \R \to [0, 1]$ wird $N$-zulässig genannt, wenn sie monoton wachsend und lipschitzsteig \cite{forster2016} ist und wenn zusätzlich die folgenden drei Bedingungen erfüllt sind$\colon$
\begin{itemize}
\item[(i)] Die Funktion $\sigma$ ist $N + 1$ mal stetig differenzierbar mit beschränkten Ableitungen.
\item[(ii)] Es existiert ein Punkt $t_{\sigma} \in \R$, in welchem alle Ableitungen bis hin zur $N$-ten Ableitung von $\sigma$ ungleich Null sind.
\item[(iii)] Wenn $y > 0$ ist, gilt $|\sigma(y) - 1| \leq \frac{1}{y}$. Wenn $y < 0$ ist, gilt $|\sigma(y)| \leq \frac{1}{|y|}$.
\end{itemize}  
\end{defn}  

In Lemma \ref{lem:logsquasher} werden wir zudem zeigen, dass der logistische squasher (\ref{logsquasher}) $N$-zulässig ist für beliebiges $N \in \N.$ 

\section{Hilfsresultate}
\begin{lem}
\label{lem:logsquasher}
Sei $N \in \N_0$ beliebig, dann erfüllt der logistische squasher $\sigma\colon \R \to [0, 1], \sigma(x) = \frac{1}{1 + \exp(-x)}$ die Bedingungen aus Definition \ref{nzulässig}.  
\end{lem}
\begin{proof}
Sei $N \in \N_0$ beliebig. Wir wissen, dass $\sigma$ monoton wachsend ist, da für beliebige $s, t \in \R$ mit $s \leq t$ gilt:
$$\sigma(s) = \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} = \sigma(t),$$
wobei wir bei der Ungleichung die Monotonie der Exponentialfunktion verwendet haben und die obige Ungleichung aus
\begin{equation*}
\begin{split}
& \quad \exp(s) \leq \exp(t) \\
 \Leftrightarrow & \quad \exp(-s) \geq \exp(-t) \\
 \Leftrightarrow & \quad 1 + \exp(-s) \geq 1 + \exp(-t) \\
 \Leftrightarrow & \quad \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} \\
\end{split}
\end{equation*}
folgt. Zudem ist $\sigma$ als Komposition glatter Funktionen, $N + 1$ mal stetig differenzierbar ist. Die Ableitungen von $\sigma$ haben die Form$\colon$
\begin{equation*}
\begin{split}
\frac{\partial \sigma}{\partial x}(x) &= -\frac{1}{(1 + \exp(-x))^2} \cdot (-\exp(-x)) \\
& = \frac{\exp(-x)}{1 + \exp(-x)} \cdot \frac{1}{1 + \exp(-x)} \\
& = \bigg(1 - \frac{1}{1 + \exp(-x)}\bigg) \cdot \frac{1}{1 + \exp(-x)} \\
& = (1 - \sigma(x)) \cdot \sigma(x).
\end{split}
\end{equation*}
Da wir bei weiterem Ableiten die Produktregel wiederholt anwenden sind alle Ableitungen von $\sigma$, Polynome in $\sigma$. Dadurch folgt Bedingung (i) aus Definition \ref{nzulässig}, da $\sigma$ nach Voraussetzung durch $0$ und $1$ beschränkt ist, und die Ableitungen von $\sigma$ als Produkt von beschränkten Faktoren daher auch. Da hiermit auch die erste Ableitung von $\sigma$ beschränkt ist wissen wir nach Satz ... aus (REFERENZ), dass $\sigma$ lipschitzstetig ist.

Nun kommen wir zum Beweis von Bedingung (ii). Polynome, die nicht das $0$-Polynom sind, haben nach Satz ... (REFERENZ) auf $(0, 1)$ endlich viele Nullstellen und $\sigma$ bildet nach Voraussetzung in das Intervall $[0, 1] \supseteq (0, 1)$ ab. Da die Ableitungen von $\sigma$, als Zusammensetzung von Polynomen in $\sigma$, wieder Polynome sind für die die obere Eigenschaft ebenfalls gilt, existiert ein $t_{\sigma} \in \R$ mit $\sigma(t_{\sigma}) \neq 0$, sodass alle Ableitungen bis zum Grad $N$ von $\sigma$, aufgrund ihrer Struktur ungleich $0$ sind. Daher ist Bedingung (ii) ebenfalls erfüllt.

Wir wissen, dass für $x \in R$ und damit insbesondere für ein beliebiges $x > 0$: $$ x \leq \exp(x) + 1$$ gilt. Daraus erhalten wir mit Umformungen da $x > 0$ und $1 + \exp(-x) > 0$ ist:
\begin{equation*}
\begin{split}
& \quad x \leq \exp(x) + 1 \\
 \Leftrightarrow & \quad x \cdot \exp(-x) \leq 1 + \exp(-x) \\
 \Leftrightarrow & \quad \frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x} \\
 \Leftrightarrow & \quad 1 - \frac{1}{1 + \exp(-x)} \leq \frac{1}{x} \\
 \Leftrightarrow & \quad |\sigma(x) - 1| \leq \frac{1}{x}.
\end{split}
\end{equation*}
Wobei die letzte Ungleichung aus der Eigenschaft des Betrags kommt, da $\frac{1}{1 + \exp(-x)} - 1< 0$ ist, weil $1 + \exp(-x) > 1$, da $\exp(-x) > 0$. Dies zeigt die erste Relation aus Bedingung (iii).
Die zweite Relation folgt durch die gleiche Art und Weise, da wir durch $$\frac{1}{1 + \exp(x)} - \frac{1}{2}= \sigma(0 - x) - \frac{1}{2} = -\sigma(0 + x) + \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2}$$ 
wissen, dass $\sigma$ punktsymmetrisch in $(0, \frac{1}{2})$ ist. Die obige Gleichheit folgt aus
\begin{equation*}
\begin{split}
 & \quad \frac{1}{1 + \exp(x)} - \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2} \\
 \Leftrightarrow  & \quad \frac{1}{1 + \exp(x)} - \frac{1}{2} +\frac{1}{1 + \exp(-x)} = \frac{1}{2}\\
 \Leftrightarrow & \quad  \frac{1 + \exp(-x) + 1 + \exp(x)}{(1 + \exp(x)) \cdot (1 + \exp(-x))} - \frac{1}{2} = \frac{1}{2} \\
 \Leftrightarrow & \quad \frac{2 + \exp(-x) + \exp(x)}{2 + \exp(-x) + \exp(x)} - \frac{1}{2} = \frac{1}{2} \\
 \Leftrightarrow & \quad 1 - \frac{1}{2} = \frac{1}{2}.
\end{split}
\end{equation*}
Aus dieser Eigenschaft folgt mit $$\sigma(- x) - 1 = \frac{1}{1 + \exp(x)} - 1 = -\frac{1}{1 + \exp(-x)} = -\sigma(x)$$ für $x < 0$ aus der ersten Relation, da $-x > 0 $ ist$\colon$  
$$|\sigma(x)| = |-\sigma(x)| = |\sigma(- x) - 1| \leq \frac{1}{-x} = \frac{1}{|x|}.$$
Damit haben wir alle drei Bedingungen aus Definition \ref{nzulässig} gezeigt und unsere Aussage bewiesen.
\end{proof}
\begin{lem}[Langrangesche Form des Restglieds \cite{forster2016}]
\label{lem:lagrange}
Sei $f \colon I \to \R$ eine $(N + 1)$-mal stetig differenzierbare Funktion und $u, x \in I$. Dann existiert ein $\xi \in [u, x],$ so dass $$f(x) = \sum_{k = 0}^N \frac{f^{(k)}(u)}{k!}(x - u)^k + \frac{f^{(N + 1)}(\xi)}{(N + 1)!}(x - u)^{N + 1}.$$
\end{lem}
\begin{lem}
  \label{lem:1}
  Sei $\sigma \colon \R \to \R$ eine Funktion und $R$, $a > 0$.
  \begin{itemize}
  \item[a)] Angenommen $\sigma$ ist zwei mal stetig differenzierbar und $t_{\sigma,\id} \in \R$ so, dass $\sigma'(t_{\sigma, \id}) \neq 0$ ist. Dann gilt mit
  $$ f_{\id}(x) = \frac{R}{\sigma'(t_{\sigma, \id})} \cdot \left(\sigma\left(\frac{x}{R} + t_{\sigma, \id}\right) - \sigma(t_{\sigma, \id})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R}.$$
  \item[b)] Angenommen $\sigma$ ist drei mal stetig differenzierbar und $t_{\sigma,\sq} \in \R$ so, dass $\sigma''(t_{\sigma, \sq}) \neq 0$ ist. Dann gilt mit
  $$ f_{\sq}(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\sigma\left(\frac{2 \cdot x}{R} + t_{\sigma, \sq}\right) - 2 \cdot \sigma(\frac{x}{R} + t_{\sigma, \sq})+ \sigma(t_{\sigma, \sq})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\sq}(x) - x^2| \leq \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}.$$
  \end{itemize}
\end{lem}
\begin{proof}
	\begin{itemize}
  	\item[a)] Wir wissen, dass $f_{\id}$ 2-mal differenzierbar ist, da nach Vorraussetzung $\sigma$ 2-mal stetig differenzierbar ist. Damit folgt mit Lemma \ref{lem:lagrange} für $f = f_{\id}, N = 1$ und $I = [-a, a]$, dass ein $\xi \in [0, x]$ existiert, so dass:
$$f_{\id}(x) = \sum_{k = 0}^N \frac{f_{\id}^{(k)}(0)}{k!}x^k + \frac{f_{\id}^{(N + 1)}(\xi)}{(N + 1)!}x^{N + 1}.$$
Mit $f_{\id}(0) = 0$ und $f_{\id}'(0) = 1$ erhalten wir:
  	\begin{equation*}
  	\begin{split}
  	 |f_{\id}(x) -  & x| \\
  	& = \bigg|0 + 1 \cdot x + \frac{1}{2} \cdot \frac{1}{R \cdot \sigma'(t_{\sigma,\id})} \sigma''(\frac{\xi}{R} + t_{\sigma,\id}) \cdot x^2 - x \bigg| \\
  	& = \bigg| \frac{\sigma''(\frac{\xi}{R} + t_{\sigma,\id})  \cdot x^2}{2R \cdot \sigma'(t_{\sigma, \id})} + x - x\big| \\
  	& \leq \frac{\| \sigma'' \|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R},  
  	\end{split}
  	\end{equation*}
  	Wobei sich die letzte Ungleichung aus den Eigenschaften der Supremumsnorm ergibt und zudem aus $x \in [-a,a] \Leftrightarrow -a \leq x \leq a$ durch Quadrieren der Ungleichung folgt, dass $x^2 \leq a^2$ ist.
  	\item[b)]  Die Funktion $f_{\sq}$ die hier nun 3-mal differenzierbar ist, da $\sigma$ nach Voraussetzung  3-mal stetig differenzierbar ist. Es wie in a) durch Lemma \ref{lem:lagrange} mit $f = f_{\sq}, N = 2$ und $I = [-a, a]$, dass ein $\xi \in [0, x]$ existiert, so dass:
$$f_{\sq}(x) = \sum_{k = 0}^N \frac{f_{\sq}^{(k)}(0)}{k!}x^k + \frac{f_{\sq}^{(N + 1)}(\xi)}{(N + 1)!}x^{N + 1}.$$
Mit $f_{\sq}(0) = 0$, $f_{\sq}'(0) = 0$ und $f_{\sq}''(0) = 2$ erhalten wir:
\begin{equation*}
  	\begin{split}
  	 |f_{\sq}(x) -  & x^2| \\
  	& = \bigg|x^2 + \frac{1}{6} \cdot \frac{R^2}{\sigma''(t_{\sigma,\sq})} \left(\frac{8}{R^3}\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq}) - \frac{2}{R^3} \sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})\right) \cdot x^3 - x^2\bigg| \\
  	& \leq \frac{a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \left(8 \cdot |\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq})| + 2 |\sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})|\right) \\
  	& \leq \frac{10 \cdot a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \|\sigma'''\|_{\infty} \\
  	& = \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}. 
  	\end{split}
  	\end{equation*}
 	\end{itemize}
\end{proof}

\begin{lem}
  \label{lem:2}
  Sei $\sigma \colon \R \to [0, 1]$ 2-zulässig. Zudem sei $R > 0$ und $a > 0$ beliebig. Dann gilt für das neuronale Netz
  \begin{equation*}
  	\begin{split}
  	f_{\mult}(x, y) = \frac{R^2}{4 \cdot \sigma''(t_{\sigma})} \cdot & \bigg(\sigma\Big(\frac{2 \cdot (x + y)}{R} + t_{\sigma} \Big) - 2 \cdot \sigma\Big(\frac{x + y}{R} + t_{\sigma}\Big) \\
  	& - \sigma \Big(\frac{2 \cdot (x - y)}{R} + t_{\sigma} \Big) + 2 \cdot \sigma \Big(\frac{x - y}{R} + t_{\sigma} \Big) \bigg)
  	\end{split}
  	\end{equation*}
  	für beliebige $x, y \in [-a, a]$ die folgende Ungleichung$\colon$
  	$$|f_{\mult}(x, y) - x \cdot y| \leq \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Durch Einsetzen erhalten wir $$f_{\mult}(x,y) = \frac{1}{4}(f_{\sq}(x + y) - f_{\sq}(x - y))$$ und $$x \cdot y = \frac{1}{4}\big((x + y)^2 - (x - y)^2\big).$$
  Aus diesen beiden Gleichungen folgt durch Ausklammern von $\frac{1}{4}$, der Homogenität des Betrags und der Anwendung der Dreiecksungleichung$\colon$
  \begin{equation*}
  \begin{split}
  |f_{\mult}(x, y) - x \cdot y| & = \frac{1}{4} \cdot \big|f_{\sq}(x + y) - f_{\sq}(x - y) - (x + y)^2 + (x - y)^2\big| \\
  & \leq \frac{1}{4} \cdot \big|f_{\sq}(x + y) - (x + y)^2\big| + \frac{1}{4}\cdot\big| (x - y)^2 - f_{\sq}(x - y)\big| \\
  & \leq 2 \cdot \frac{1}{4} \cdot \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \\
  & = \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R},
  \end{split}
\end{equation*}   
wobei wir bei der letzten Ungleichung verwendet haben, dass $a > 0$ nach Lemma \ref{lem:1}b) beliebig gewählt wurde und daher insbesondere für beliebiges $x \in [-2a,2a]$ $$ |f_{\sq}(x) - x^2| \leq \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}$$ gilt. 
  \end{proof}
  \begin{lem}
  \label{lem:3}
  Sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $f_{\mult}$ das neuronale Netz aus Lemma \ref{lem:2} und $f_{\id}$ das neuronale Netz aus Lemma \ref{lem:1}. Angenommen es gelten die Ungleichungen 
\begin{equation}
\label{lem:3:Vor1}   
  a \geq 1 \text{\quad und \quad} R \geq \frac{\|\sigma''\|_{\infty} \cdot a}{2 \cdot |\sigma'(t_{\sigma})|}.
\end{equation}
  Dann erfüllt das neuronale Netz 
  \begin{equation}
  \label{lem:3:Vor2}
  \begin{split}
  f_{\ReLU}(x) & = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) 
  \end{split}
  \end{equation}
 für alle $x \in [-a, a]$ folgende Ungleichung:
 $$|f_{\ReLU}(x) - \max\{x, 0\}| \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Da $\sigma$ nach Voraussetzung 2-zulässig nach Definition 1 ist, gilt für $R \geq 0,$ und $x \in \R\setminus\{0\}\colon$ $$|\sigma(R \cdot x) - 1| \leq \frac{1}{R\cdot x} \text{\, für \,} x > 0$$ und $$|\sigma(R \cdot x)| \leq \frac{1}{|R \cdot x|} \text{\, für \,} x < 0.$$
Damit folgt aus der Homogenität des Betrags für alle $x \neq 0$: 
\begin{equation}
\label{lem:3:Bed1} 
|\sigma(R \cdot x) - \1_{[0, \infty)}(x)| \leq \frac{1}{|R \cdot x|} = \frac{1}{R \cdot |x|}.
\end{equation} 
Nach Lemma \ref{lem:1} und Lemma \ref{lem:2} gilt:
\begin{equation}
\label{lem:3:Bed2} 
|f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R} \text{\, für \,} x \in [-a, a]
\end{equation}
 und 
\begin{equation}
\label{lem:3:Bed3}
 |f_{\mult}(x, y) - x \cdot y| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x, y \in [-2a, 2a].
 \end{equation} 
Da nach Voraussetzung $a \geq 1$ ist, gilt insbesondere $[0, 1] \subseteq [-2a, 2a]$ und daher gilt insbesondere $\sigma(Rx) \in [0, 1]\subseteq [-2a, 2a].$ 
Zudem erhalten wir durch eine Nulladdition, das Anwenden der Dreiecksungleichung, die Verwendung von Lemma \ref{lem:1} und (\ref{lem:3:Vor1}) für $R$:
\begin{equation*}
\begin{split}
|f_{\id}(x)| & = |f_{\id}(x) - x + x| \\
& \leq |f_{\id}(x) -x| + |x| \\
&  \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} + |x| \\
& \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{2 \cdot |\sigma'(t_{\sigma})|}{\|\sigma''\|_{\infty} \cdot a} + |x| \\
& = a + |x| \\
& \leq 2 \cdot a
\end{split}
\end{equation*}
wobei $x \in [-a, a]$. Daraus folgt insbesondere $f_{\id}(x) \in [-2a, 2a].$
Mithilfe von $\max\{x, 0 \} = x \cdot \1_{[0, \infty)}(x)$, (\ref{lem:3:Vor2}, zweier Nulladditionen und dem zweifachen Anwenden der Dreiecksungleichung erhalten wir$\colon$
\begin{equation*}
\begin{split}
|f_{\ReLU}(x) - & \max\{x, 0\}|  \\
& = \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - x \cdot \1_{[0, \infty)}(x)\big| \\
& \leq \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \\
& \qquad + \big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| + \big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big|. \\
& \text{Daraus ergibt sich mithilfe von (\ref{lem:3:Bed1}) - \ref{lem:3:Bed3}), $\sigma(Rx) \in [0, 1]$ und $a^3 \geq 1$}\\
& \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \cdot 1+ \frac{1}{R} \\
& \leq \bigg(\frac{160}{3} \cdot \frac{\|\sigma'''\|_{\infty} \cdot a ^3}{|\sigma''(t_{\sigma})|} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} + \frac{a^3}{a^3} \bigg) \cdot \frac{1}{R} \\ 
& \leq \bigg(\frac{160 \cdot\|\sigma'''\|_{\infty} \cdot a ^3 + 3 \cdot \|\sigma''\|_{\infty} \cdot a^3 + 3 \cdot a^3}{3 \cdot \min\{ 2 \cdot \sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot \frac{1}{R}\\
& \leq \frac{166}{3} \cdot \bigg(\frac{\max\{\|\sigma'''\|_{\infty}, \|\sigma''\|_{\infty} , 1\}}{\min\{ 2 \cdot \sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot a^3 \cdot  \frac{1}{R} \\
& \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.
\end{split}
\end{equation*}
  \end{proof}
  \begin{lem}
  \label{lem:4}
  Sei $M \in \N$ und sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $a > 0$ und $$R \geq \frac{\|\sigma''\|_{}\infty \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma})|},$$ sei $y \in [-a, a]$ und $f_{\ReLU}$ das neuronale Netz aus Lemma \ref{lem:3}. Dann erfüllt das neuronale Netz 
  \begin{equation*}
  \begin{split}
  f_{\mathrm{hat},y}(x) = f_{\ReLU}& \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) \\ &+ f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg)
  \end{split}
  \end{equation*}
  für alle $x \in [-a ,a]$ mit $z_+ = \max\{0, z\} \, (z \in \R):$   
  $$\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma, \id}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Für $x \in \R$ gilt die Gleichung:
\begin{equation}
\label{lem:4:eq}
 (1 - \frac{M}{2a} \cdot |x|)_+ = \max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}, 
 \end{equation}  
die wir im zweiten Teil dieses Beweises zeigen werden. Damit beweisen wir das Resultat mit Hilfe von Lemma \ref{lem:3}, denn mit der Definition von $f_{\mathrm{hat},y}(x)$ und zwei mal der Dreiecksungleichung folgt:
\begin{equation*}
\begin{split}
\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq \bigg|&f_{\ReLU} \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) + 1, 0\}\bigg| \\ 
& + 2 \cdot \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) - \max\{\frac{M}{2a} \cdot (x - y), 0\}\bigg| \\
& + \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) - 1, 0\}\bigg|\\ 
& \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*} 
wobei die letzte Ungleichung daraus folgt, dass wir auf jeden Summanden mit $1 \leq a = M + 1$ Lemma \ref{lem:3} angewendet haben und die Abschätzung 
$$ (M + 1)^3 \leq (2M)^3 = 8M^3$$ verwendet haben, da $M \geq 1$ ist. $\hfill(\square)$

Um Gleichung (\ref{lem:4:eq}) zu zeigen unterscheiden wir vier Fälle.
  \begin{itemize}
  \item[Fall 1] ($x < 0$) In diesem Fall hat die linke Seite nach der Definition des Betrags die Gestalt $$\max\{1 + \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot 0 + 0,$$ da $x < 0$ und damit die letzten zwei Summanden 0 sind. Es erfordert hier eine weitere Fallunterscheidung.
 \item[Fall 1.1] ($0 > x \geq -\frac{2a}{M}$) In diesem Fall gilt für die linke und rechte Seite:
 $$\max\{1 + \frac{M}{2a} \cdot x, 0\} = 1 + \frac{M}{2a} \cdot x.$$
 \item[Fall 1.2] ($x < -\frac{2a}{M}$) In diesem Fall sind beide Seiten gleich 0, da $1 + \frac{M}{2a} \cdot x \leq 0$ ist. $\hfill(\square)$
  \item[Fall 2] ($x \geq 0$) In diesem Fall hat die linke Seite nach der Definition des Betrags die Gestalt $$\max\{1 - \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}$$ und erfordert daher eine weitere Fallunterscheidung.
 \item[Fall 2.1] ($0 \leq x < \frac{2a}{M}$) In diesem Fall hat die linke Seite die Gestalt $$1 - \frac{M}{2a} \cdot x$$ und die rechte Seite die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + 0 = 1 - \frac{M}{2a} \cdot x.$$ und stimmt daher mit der linken Seite überein.
 \item[Fall 2.2] ($x \geq \frac{2a}{M}$) In diesem ist die linke Seite gleich 0, da $1 - \frac{M}{2a} \cdot x < 0$ ist und die rechte Seite bestitz die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + \frac{M}{2a} \cdot x - 1 = 0.$$ $\hfill(\square)$
\end{itemize} 
Durch diese Fallunterscheidung wurde die Gleichung (\ref{lem:4:eq}) bewiesen und damit ist der Beweis vollständig.
  \end{proof}
Die nächsten Lemmata benötigen wir für den Beweis unseres Hauptresultats, einer Aussage über die Konvergenzgeschwindigkeit unseres neuronale Netze Schätzers.. Diese Lemmata werden hier nur der Vollständigkeit halber und ohne Beweis aufgeführt.
\begin{lem}[\cite{kohler19}, Lemma 5]
\label{lem:5}
Sei $M \in \N$ und $\sigma\colon \R \to [0, 1]$ $2$-zulässig nach Definition \ref{nzulässig}. (AUF SCHREIBWEISE VON FNET AUFPASSEN)
Sei $a \geq 1$ und 
\begin{equation*}
\begin{split}
R & \geq \max\biggl\{\frac{\|\sigma''\|_{\infty} \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma, \id})|}, \frac{9 \cdot \|\sigma''\|_{\infty} \cdot a}{|\sigma'(t_{\sigma, \id}|}, \\
& \quad \frac{20 \cdot \|\sigma'''\|_{\infty}}{3 \cdot |\sigma''(t_{\sigma})|} \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s}, 1792 \cdot \frac{\max\{\|\sigma''\|_{\infty},\|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma, \id})|, |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \biggr\}
\end{split}
\end{equation*}
und sei $y \in [-a, a]^d.$ Sei $N \in \N$ und $j_1,\dots,j_d \in \N_0$ so, dass $j_1 + \cdots + j_d \leq N$ gilt und wir setzen $s = \lceil\log_2(N + d)\rceil$. Sei $f_{\id}, f_{\mult}$ und $f_{\mathrm{hat}, z}$ (für $z \in \R$) die neuronalen Netze wir in Lemma \ref{lem:1}, Lemma \ref{lem:2} und Lemma \ref{lem:4}. Wir definieren das Netz $f_{net,j_1,\dots,j_d,y}$ durch:
\begin{align*}
\label{fnet}
& f_{net,j_1,\dots,j_d,y}(x) = f_1^{(0)}(x). \\
\intertext{wobei} 
& f_k^{(l)}(x) = f_{\mult}\Big(f_{2k - 1}^{(l + 1)}(x),f_{2k}^{(l + 1)}(x)\Big) \\
\intertext{für $1 \leq k \leq 2^l$ und $0 \leq l \leq s - 1$ und} 
& f_k^{(s)}(x) = f_{\id}(f_{\id}(x^{(l)} - y^{(l)}))  \\
\intertext{für $j_1 + j_2 + \dots + j_{l-1} + 1 \leq k \leq j_1 + j_2 + \dots + j_l$ und $1 \leq l \leq d$ und} 
& f_{j_1 + j_2 + \dots + j_d + k}^{(s)}(x) = f_{\mathrm{hat},y^{(k)}}(x^{(k)}) \\
\intertext{für $1 \leq k \leq d$ und} 
& f_k^{(s)}(x) = 1 \\
\intertext{für $j_1 + j_2 + \dots + j_d + d + 1 \leq k \leq 2^s.$}
\end{align*} 
Dann erhalten wir für $x \in [-a, a]^d$:
\begin{equation*}
\begin{split}
& \bigg|f_{net, y}(x) - (x^{(1)} - y^{(1)})^{j_1} \cdots (x^{(d)} - y^{(d)})^{j_d} \prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - y^{(j)}|)_+\bigg| \\
& \leq c_{12} \cdot 3^{3 \cdot 3^s} \cdot a^{3 \cdot 2^s} \cdot M^3 \cdot \frac{1}{R}.
\end{split}
\end{equation*}
\end{lem}
  \begin{lem}[\cite{kohler19}, Lemma 8]
  \label{lem:8}
Sei $\beta_n = c_6 \cdot \log(n)$ für eine hinreichend große Konstante $c_6 > 0$. Angenommen die Verteilung von $(X, Y)$ erfüllt 
$$ \E\Big(\mathrm{e}^{c_4 \cdot |Y|^2}\Big) < \infty$$
für eine Konstante $c_4 > 0$ und dass der Betrag der Regressionsfunktion $m$ beschränkt ist. Sei $\mathcal{F}_n$ eine Menge von Funktionen $f\colon \R^d \to \R$ und wir nehmen an, dass der Schätzer $m_n$ 
$$m_n = T_{\beta_n}\tilde{m}_n$$ 
erfüllt, mit 
$$\tilde{m}_n(\cdot) = \tilde{m}_n(\cdot,(X_1, Y_1),\dots,(X_n, Y_n)) \in \mathcal{F}_n$$
und 
$$\frac{1}{n} \sum_{i = 1}^n |Y_i - \tilde{m}_n(X_i)|^2 \leq \min_{l \in \Theta_n}\bigg(\frac{1}{n}\sum_{i = 1}^n |Y_i - g_{n,l}(X_i)|^2 + pen_n(g_n,l)\bigg)$$
mit einer nichtleeren Parametermenge $\Theta_n$, zufällige Funktionen $g_{n,l}\colon \R^d \to \R$ und deterministischen penalty Termen $pen_n(g_{n,l}) \geq 0$, wobei die zufälligen Funktionen $g_{n,l}\colon \R^d \to \R$ nur von den Zufallsvariablen
$$\mathbf{b}_1^{(1)},\dots,\mathbf{b}_r^{(1)},\dots,\mathbf{b}_1^{(I_n)},\dots,\mathbf{b}_r^{(I_n)},$$
abhängen, die unabhängig von $(X_1, Y_1), (X_2, Y_2),\dots$ sind.
Dann erfüllt $m_n\colon$
\begin{equation*}
\begin{split}
& \E \int |m_n(x) - m(x)|^2 \mathds{P}_X(dx) \\
& \leq \frac{c_{13} \cdot \log(n)^2 \cdot \big(\log\big(\sup_{x_1^n \in (supp(X))^n}\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)\big) + 1\big)}{n} \\
& \quad + 2 \cdot \E\bigg(\min_{l \in \Theta_n} \int |g_{n,l}(x) - m(x)|^2 \mathds{P}_X(dx) + pen_n(g_{n,l})\bigg),
\end{split}
\end{equation*}
für $n > 1$ und einer Konstante $c_{13} > 0$ welche nicht von $n$ abhängt. (DEFINITION VON LP-e-ÜBERDECKUNGSZAHLEN)
  \end{lem}
Das nächste Lemma benötigen wir um eine Schranke für die Überdeckungszahl $\mathcal{N}_1\big(\frac{1}{n \cdot \beta_n},\mathcal{F}_n,x_1^n\big)$ zu finden.
\begin{lem}[\cite{kohler19}, Lemma 9]
\label{lem:9}
Sei $a > 0$ und $d, N, J_n \in \N$ so, dass $J_n \leq n^{c_{14}}$ und setze $\beta_n = c_6 \cdot \log(n).$
Sei $\sigma$ 2-zulässig nach Definition \ref{nzulässig}. Sei $\mathcal{F}$ die Menge aller Funktionen die durch (\ref{networkarch}) definiert sind mit $k_1 = k_2 = \cdots = k_L = 24 \cdot (N + d)$ und dass der Betrag der Gewichte durch $c_{15} \cdot n^{c_{16}}$ beschränkt ist. Sei
$$ \mathcal{F}^{(J_n)} = \biggl\{\sum_{j = 1}^{J_n} a_j \cdot f_j : f_j \in \mathcal{F} \quad \text{und} \quad \sum_{j = 1}^{J_n} a_j^2 \leq c_{17} \cdot n^{c_{18}}\biggr\}.$$
Dann gilt für $n > 1:$
$$\log\bigg(\sup_{x_1^n\in[-a,a]^{d \cdot n}} \mathcal{N}_1\bigg(\frac{1}{n \cdot \beta_n}, \mathcal{F}^{(J_n)},x_1^n\bigg)\bigg) \leq c_{19} \cdot \log(n) \cdot J_n,$$
für eine Konstante $c_{19}$ die nur von $L, N, a$ und $d$ abhängt.
\end{lem}