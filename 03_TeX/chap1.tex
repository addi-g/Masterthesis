\chapter{Grundlagen und Hilfsresultate}
\label{chap:1}

Der Zweck dieses Kapitels ist es, grundlegende Definitionen zu sammeln, die in den folgenden Kapiteln verwendet werden. Insbesondere gehen wir auf die in dieser Arbeit verwendeten neuronalen Netze ein, die als Baustein für unseren Neuronale-Netze-Regressionsschätzer verwendet werden. Weiterhin werden wir Hilfsresultate darstellen und beweisen, welche wir für das Resultat der Konvergenzgeschwindigkeit des einfach berechenbaren Neuronale-Netze-Regressionsschätzers benötigen werden.
In dieser Arbeit bezeichnen wir mit $d \geq 1$ immer eine natürliche Zahl.

\section{Definitionen}
Es ist bekannt (vgl.\@ \cite[Theorem 7.2 und Problem 7.2]{DevLug96} \cite[Section 3]{DevWag80}), dass man Glattheitsvoraussetzungen an die Regressionsfunktion haben muss, um nichttriviale Konvergenzresultate für nichtparametrische Regressionsschätzer herzuleiten. Dafür verwenden wir die folgende Definition.
\begin{defn}[($p,C$)-Glattheit]
\label{def:pc}
   Sei $p = q + s$ mit $q \in \N_0$ und $s \in (0,1]$ (also $p \in (0, \infty)$ und sei $C > 0$. Eine Funktion $f\colon \R^d \to \R$ heißt \emph{($p, C$)-glatt}, falls für alle $\alpha = (\alpha_1,\dots,\alpha_d) \in \N_0^d$ mit $\sum_{j = 1}^{d}\alpha_j = q$ die partielle Ableitung 
   $$ \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}$$
   existiert und falls für alle $x, z \in \R^d$ die Abschätzung 
   $$ \bigg|\frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(x) - \frac{\partial^qf}{\partial x_1^{\alpha_1}\dots\partial x_d^{\alpha_d}}(z) \bigg| \leq C \cdot \|x - z\|^s,$$
   gilt, wobei $\|\cdot\|$ die euklidische Norm in $\R^d$ bezeichnet.  
\end{defn}

Da wir uns in dieser Arbeit mit neuronalen Netzen beschäftigen, ist es hilfreich zu wissen was man darunter versteht. Ein neuronales Netz ist nichts anderes als eine Ansammlung von Neuronen, welche als Informationsverarbeitungseinheiten dienen, die schichtweise in einer Netzwerkarchitektur angeordnet sind. Beginnend mit der Eingabeschicht (\textit{Input Layer}) fließen Informationen über eine oder mehrere verborgene Schichten (\textit{Hidden Layer}) bis hin zur Ausgabeschicht (\textit{Output Layer)}. Die Informationsweitergabe der Neuronen verläuft so, dass sie die Eingaben $x_1,\dots,x_n$, die einerseits aus dem beobachteten Prozess resultieren können, dessen Werte dem Neuron übergeben werden, oder andererseits aus den Ausgaben anderer Neuronen stammen und verarbeiten.
% und entsprechend über eine Aktivierung reagieren. 
Dazu werden für ein künstliches Neuron $j$ die Eingaben mit $w_{1_j}, \dots, w_{n_j}$ gewichtet an eine Aktivierungsfunktion $\sigma$ übergeben, welche die Neuronenaktivierung berechnet. Der Endpunkt des Informationsflusses in einem neuronalen Netz ist die Ausgabeschicht, die hinter den verborgenen Schichten liegt. Sie bildet damit die letzte Schicht in einem neuronalen Netz. Die Ausgabeschicht enthält somit das Ergebnis der Informationsverarbeitung durch das Netz.  
%\begin{figure}
%    \centering
%    \begin{tikzpicture}[
%        % define styles    
%        init/.style={ 
%             draw, 
%             circle, 
%             inner sep=2pt,
%             font=\Huge,
%             join = by -latex
%        },
%        squa/.style={ 
%            font=\Large,
%            join = by -latex
%        }
%    ]
%        
%        % Top chain x1 to w1
%        \begin{scope}[start chain=1]
%            \node[on chain=1] at (0,1.5cm)  (x1) {$x_1$};
%            \node[on chain=1,join=by o-latex] (w1) {$w_1$};
%        \end{scope}
%        
%        % Middle chain x2 to output
%        \begin{scope}[start chain=2]
%            \node[on chain=2] (x2) {$x_2$};
%            \node[on chain=2,join=by o-latex] {$w_2$};
%            \node[on chain=2,init] (sigma) {$\displaystyle\Sigma$};
%            \node[on chain=2,squa,label=above:{\parbox{2cm}{\centering Activation\\ function}}]   {$f_{act}$};
%            \node[on chain=2,squa,label=above:Output,join=by -latex] {$y_{out}$};
%        \end{scope}
%        
%        % Bottom chain x3 to w3
%        \begin{scope}[start chain=3]
%            \node[on chain=3] at (0,-1.5cm) 
%            (x3) {$x_3$};
%            \node[on chain=3,label=below:Weights,join=by o-latex]
%            (w3) {$w_3$};
%        \end{scope}
%        
%        % Bias
%        \node[label=above:\parbox{2cm}{\centering Bias \\ $b$}] at (sigma|-w1) (b) {};
%        
%        % Arrows joining w1, w3 and b to sigma
%        \draw[-latex] (w1) -- (sigma);
%        \draw[-latex] (w3) -- (sigma);
%        \draw[o-latex] (b) -- (sigma);
%        
%        % left hand side brace
%        \draw[decorate,decoration={brace,mirror}] (x1.north west) -- node[left=10pt] {Inputs} (x3.south west);
%        
%    \end{tikzpicture}
%    
%    \caption{Neural network pipeline}
%    
%\end{figure}
Zwei wichtige Charakteristika, die neuronale Netze aufweisen können, sind:
\begin{itemize}
\item Wenn in einem neuronalen Netz die Information von der Eingabeschicht über die verborgenen Schichten bis hin zur Ausgabeschicht in eine Richtung (\glqq vorwärts\grqq) weitergereicht wird, spricht man von einem \textit{feedforward} neuronalen Netz.
\item Ein neuronales Netz wird als ein \textit{fully connected} (\glqq vollständig verbundenes\grqq\@) neuronales Netz bezeichnet, wenn sämtliche Neuronen einer Schicht mit allen der darauffolgenden verbunden sind. Da man bei fully connected neuronalen Netzen die Gewichte der Verbindungen zwischen zwei Neuronen auf Null setzen kann, unterscheiden wir hier nicht mehr zwischen neuronalen Netzen die fully connected sind oder nicht.
%Jedes Neuron der verborgenen Schicht ist also mit allen Neuronen der Eingabeschicht verbunden und ebenso ist jedes Neuron der Ausgabeschicht mit allen der letzten verborgenen Schicht verbunden.
\end{itemize}
%Für ein neuronales Netz sind die Aktivierungsfunktionen von großer Bedeutung, da sie dabei helfen, da es nichtlineare komplexe funktionale Mapping zwischen den Eingangsdaten und den abhängigen Ergebnissen zu verstehen.
%\begin{defn}
%Eine \emph{Aktivierungsfunktion} ist eine Transformation, die wir über die Eingabe durchführen, bevor wir sie an die nächste Schicht von Neuronen senden oder sie als Ausgabe finalisieren.
%\end{defn}
%\begin{defn}
%Ein \emph{neuronales Netz} ist eine Funktion $f\colon X \to Y$, wobei $f(x)$ für $x \in X$, definiert ist als Komposition von weiteren Funktionen $g_i \colon X \to Y$, die weiter in andere Funktionen zerlegt werden können.
%Die Funktion $f$ ist eine nichtlinear gewichtete Summe, mit $f(x) =  \sigma(\sum_iw_ig_i(x))$, wobei $w_i$ die Gewichte sind und $\sigma$ eine Aktivierungsfunktion ist.
%\end{defn}
%Als nächstes Definieren wir, was wir unter einer Netzwerkarchitektur verstehen um damit anschließend eine Definition eines mehrschichtigen feedforward neuronalen Netzes zu konstruieren.
%\begin{defn}
%Eine \emph{Netzwerkarchitektur $(L, \bk)$} ist eine Klasse aller neuronaler Netze $f$ für die gilt:
%\begin{itemize}
%\item Das neuronale Netz hat $L \in \N_0$ verborgene Schichten.
%\item Die Anzahl an Neuronen in jeder verborgenen Schicht wird durch den Vektor $\bk = (k_1,\dots,k_L) \in \N^L$ angegeben.
%\end{itemize}
%Mit dieser Definition können wir nun sagen, dass eine Netzwerkarchitektur wie ein Grundgerüst für die Konstruktion weiterer neuronale Netze ist.
%\end{defn}
Als nächstes kommen wir zu einer Definition eines neuronalen Netzes welches wir in dieser Form im weiteren Verlauf dieser Arbeit benötigen werden.
\begin{defn}
\label{def:nn}
Sei $L \in \N$, $\bk \in \N^L$ und $\sigma\colon \R \to \R$. Ein \emph{mehrschichtiges feedforward neuronales Netz mit Architektur $(L, \bk)$ und Aktivierungsfunktion} $\sigma$, ist eine reellwertige Funktion $f\colon \R^d \to \R$ definiert durch:
\begin{align}
\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{mit Gewichten $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ mit $i = 1,\dots,k_L$ rekursiv definiert durch:} 
f_i^{(r)}(x) & = \sigma \bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg)\\
\intertext{mit Gewichten $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R$ mit $r = 2,\dots, L$ und:}
f_i^{(1)}(x) & = \sigma \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber
\end{align} 
für die Gewichte $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$,
weiterhin sei $\mathfrak{N}(L, \bk, \sigma)$ die Klasse aller mehrschichtigen feedforward neuronalen Netze mit Architektur $(L, \bk)$ und Aktivierungsfunktion $\sigma$.
\end{defn}
Wir werden ab jetzt der Einfachheit halber nur noch von neuronalen Netzen mit Architektur $(L,\bk)$ reden und beziehen uns damit immer auf Definition~\ref{def:nn}. Wenn aus dem Kontext Netzwerkarchitektur bereits bekannt ist sprechen wir nur noch von einem neuronalen Netz, beziehen uns damit aber dennoch auf Definition~\ref{def:nn}.
\begin{bemnumber}
In obiger Definition versteht man unter:
\begin{itemize}
\item $L \in \N$ die Anzahl an verborgenen Schichten von $f$.
\item $\bk = (k_1,\dots,k_L) \in \N^L$ einen Vektor, der die Anzahl an Neuronen in jeder verborgenen Schicht angibt.
\end{itemize}
\end{bemnumber}
Da wir bei Neuronale-Netze-Regressionsschätzern Funktionswerte schätzen möchten, haben wir in der Ausgabeschicht nur ein Neuron. Damit erreichen wir einen eindimensionalen Output. Wie wir an der Konstruktion des neuronalen Netzes erkennen können, nehmen wir in der Ausgabeschicht die Identität als Aktivierungsfunktion.
%Es wäre auch möglich für jede Schicht eine andere Aktivierungsfunktion zu wählen. In dieser Arbeit beschränken wir uns nur auf den Fall, in welchem wir in allen verborgenen Schichten die selbe Aktivierungsfunktion verwenden.

Abbildung~\ref{fig:DNN} zeigt schematisch ein mehrschichtiges feedforward neuronales Netz, welches aus einer Eingabeschicht (\textit{Input feature 1} - \textit{Input feature $n_{\mathrm{in}}$}), $n_{\mathrm{lyr}}$~verborgenen Schichten und einer Ausgabeschicht (\textit{Output 1} - \textit{Output $n_{\mathrm{out}}$}) besteht.
\begin{center}
    \begin{figure}
        \begin{tikzpicture}[
            plain/.style={
              draw=none,
              fill=none,
              },
            dot/.style={draw,shape=circle,minimum size=3pt,inner sep=0,fill=black
              },
            net/.style={
              matrix of nodes,
              nodes={
                draw,
                circle,
                inner sep=8.5pt
                },
              nodes in empty cells,
              column sep=0.1cm,
              row sep=-11pt
              },
            >=latex
            ]
            \matrix[net] (mat)
            {
            |[plain]| \parbox{1cm}{\centering \small Input\\layer} 
                        & |[plain]| \parbox{1.2cm}{\centering \small Hidden\\layer 1} 
                                    &  |[plain]| \parbox{0cm}{\centering ...} 
                                                &  |[plain]| \parbox{1.4cm}{\centering \small Hidden\\layer $n_{\mathrm{lyr}}$} 
                                                                & |[plain]| \parbox{1cm}{\centering \small Output\\layer} \\
                        & |[plain]| & |[plain]| & |[plain]|     & |[plain]|                                   \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   & |[plain]|   & |[plain]|  & |[plain]|                \\
                        & |[dot]|  & |[plain]| & |[dot]|    & |[dot]|      \\
            |[plain]|   & |[dot]|   & |[plain]| \parbox{0cm}{\centering ...} & |[dot]|       & |[dot]|     \\
            |[dot]|     & |[dot]|  & |[plain]| & |[dot]|     & |[dot]|      \\
            |[dot]|     & |[plain]|    & |[plain]|   & |[plain]|       & |[plain]|    \\
            |[dot]|     & |[plain]| & |[plain]| & |[plain]|     &              \\
            |[plain]|   &           &  |[plain]| \parbox{0cm}{\centering ...}  &               & |[plain]|    \\
                        & |[plain]| & |[plain]|            \\
            };
            \foreach \ai/\mi in {2/\small Input feature 1,4/\small Input feature 2,6/ \small Input feature 3,12/\small Input feature $n_{\mathrm{in}}$}
              \draw[<-] (mat-\ai-1) -- node[above] {\mi} +(-3cm,0);

            \foreach \ai in {2,4,6,12}
            {\foreach \aii/\mii in {3/,11/ }
                \draw[->] (mat-\ai-1) -- (mat-\aii-2) node[yshift=0cm] {\mii};
            }
            \foreach \ai in {3,11}
            {  
                \draw[->] (mat-\ai-4) -- (mat-4-5);
            }
            \draw[->] (mat-4-5) -- node[above] {\small Output 1} +(2.5cm,0);\
            \foreach \ai in {3,11}
            {
                \draw[->] (mat-\ai-4) -- (mat-10-5);
            }
            \draw[->] (mat-10-5) -- node[above] {\small Output $n_{\mathrm{out}}$} +(2.5cm,0);
        \end{tikzpicture}
        \caption{Mehrschichtiges feedforward neuronales Netz mit einer Eingabeschicht mit $n_{\mathrm{in}}$ Neuronen, $n_{\mathrm{lyr}}$ vielen verborgenen Schichten, deren Anzahl an Neuronen variieren kann und einer Ausgabeschicht bestehend aus $n_{\mathrm{out}}$ Neuronen.}
        \label{fig:DNN}
    \end{figure}
\end{center}
Wie zuvor erwähnt ist einer der Ausgangspunkte für die Definition eines neuronalen Netzes die Wahl einer Aktivierungsfunktion $\sigma\colon \R \to \R$. Wir haben uns in dieser Arbeit für eine sogenannte \textit{Squashing Function} entschieden, welche eine monoton wachsende Funktion ist, für die $\lim_{x \to -\infty}\sigma(x) = 0$ und $\lim_{x \to \infty}\sigma(x) = 1$ gilt. In dieser Arbeit verwenden wir als Squashing Function immer den sogenannten \emph{sigmoidal} bzw.\@ \emph{logistischen Squasher}
\begin{align}
\label{logsquasher}
\sigma(x) = \frac{1}{1 + \exp(-x)} \quad (x \in \R).
\end{align}
Wir kommen nun zu einer Eigenschaft die wir für den logistischen Squasher benötigen und in Lemma~\ref{lem:logsquasher} für $N \in \N_0$ zeigen werden.
\begin{defn}
\label{nzulässig}
Sei $N \in \N_0$. Eine Funktion $\sigma\colon \R \to [0, 1]$ wird \emph{$N$-zulässig} genannt, wenn sie monoton wachsend und lipschitzstetig ist und wenn zusätzlich die folgenden drei Bedingungen erfüllt sind:
\begin{itemize}
\item[(i)] Die Funktion $\sigma$ ist $(N + 1)$-mal stetig differenzierbar mit beschränkten Ableitungen.
\item[(ii)] Es existiert ein Punkt $t_{\sigma} \in \R$ mit $$\frac{\partial^i}{\partial x^i}\sigma(t_{\sigma}) \neq 0 \text{ für } i = 0,\dots,N.$$
\item[(iii)] Wenn $y > 0$ ist, gilt $|\sigma(y) - 1| \leq \frac{1}{y}$. Wenn $y < 0$ ist, gilt $|\sigma(y)| \leq \frac{1}{|y|}$.
\end{itemize}  
\end{defn}  

\section{Hilfsresultate}
Als erstes stellen wir ein Resultat vor, welches wir für Approximationsresultate von neuronalen Netzen benötigen um unseren Neuronale-Netze-Schätzer zu konstruieren.
\begin{lem}
\label{lem:logsquasher}
Sei $N \in \N_0$ beliebig, dann ist der logistische Squasher $$\sigma\colon \R \to [0, 1] \text{ mit } \sigma(x) = \frac{1}{1 + \exp(-x)}$$ $N$-zulässig.
\end{lem}
\begin{proof}
Sei $N \in \N_0$ beliebig. Dadurch, dass $0 \leq \frac{1}{1 + exp(-x)} \leq 1$ für alle $x \in \R$ gilt, erhalten wir schließlich auch $\sigma: \R \to [0,1]$. Wir wissen, dass $\sigma$ monoton wachsend ist, da für beliebige $s, t \in \R$ mit $s \leq t$ gilt:
$$\sigma(s) = \frac{1}{1 + \exp(-s)} \leq \frac{1}{1 + \exp(-t)} = \sigma(t),$$
wobei wir verwendet haben, dass aufgrund der Monotonie der Exponentialfunktion für $s \leq t$ auch $\exp(-s) \geq \exp(-t)$ gilt.
Zudem ist $\sigma$ als Komposition glatter Funktionen insbesondere $(N + 1)$-mal stetig differenzierbar. Die Funktion $\sigma$ erfüllt die gewöhnliche Differentialgleichung $\frac{\partial\sigma}{\partial x} = (1 - \sigma) \cdot \sigma,$ da:
\begin{equation*}
\begin{split}
\frac{\partial \sigma}{\partial x}(x) &= -\frac{1}{(1 + \exp(-x))^2} \cdot (-\exp(-x)) \\
& = \frac{\exp(-x)}{1 + \exp(-x)} \cdot \frac{1}{1 + \exp(-x)} \\
& = \bigg(1 - \frac{1}{1 + \exp(-x)}\bigg) \cdot \frac{1}{1 + \exp(-x)} \\
& = (1 - \sigma(x)) \cdot \sigma(x).
\end{split}
\end{equation*}
Mit der Leibnizregel aus der Differentialrechnung erhalten wir:
\begin{equation}
\label{eq:leibniz}
\frac{\partial^n\sigma}{\partial x^n} = \frac{\partial^{n - 1}}{\partial x^{n - 1}}\bigg((1 - \sigma(x)) \cdot \sigma(x)\bigg) = \sum_{k = 0}^{n-1}\binom{n - 1}{k}(1 - \sigma(x))^{(n - 1 - k)}  \sigma(x)^{(k)} 
\end{equation}
für $n = 1,\dots,N$. 
Damit erkennen wir, dass alle Ableitungen von $\sigma$, Polynome in $\sigma$ sind. Dadurch folgt Bedingung (i) aus Definition~\ref{nzulässig}, da $\sigma$ nach Voraussetzung beschränkt ist, und die Ableitungen von $\sigma$ als endliche Summe von Produkten beschränkter Faktoren, ebenfalls beschränkt ist. Da hiermit auch insbesondere die erste Ableitung von $\sigma$ beschränkt ist, wissen wir, dass $\sigma$ lipschitzstetig ist.

Nun kommen wir zum Beweis von Bedingung (ii). 
Da für alle $x \in \R$ $\sigma(x) > 0$ gilt, bleibt dies auch bei Potenzen und positiven Linearkombinationen erhalten. Daher wissen wir durch Gleichung~(\ref{eq:leibniz}), dass ein $t_{\sigma} \in \R$ existiert, mit $$\frac{\partial^i}{\partial x^i}\sigma(t_{\sigma}) \neq 0 \text{ für } i = 0,\dots,N.$$
%Da die Ableitungen von $\sigma$, als Zusammensetzung von Polynomen in $\sigma$, wieder Polynome sind für die die obere Eigenschaft ebenfalls gilt, existiert ein $t_{\sigma} \in \R$ mit $\sigma(t_{\sigma}) \neq 0$, sodass alle Ableitungen bis zum Grad $N$ von $\sigma$, aufgrund ihrer Struktur ungleich $0$ sind. 
Damit ist Bedingung (ii) ebenfalls erfüllt.

Nun zu Bedingung (iii). Wir wissen, dass für $x \in \R$ und damit insbesondere für ein beliebiges $x > 0$ die Ungleichung $$ x \leq \exp(x) + 1$$ gilt. Weiterhin erhalten wir 
\begin{equation}
\label{sigma}
|\sigma(x) - 1| = 1 - \frac{1}{1 + \exp(-x)} = \frac{\exp(-x)}{1 + \exp(-x)}.
\end{equation}
Zudem folgt aus $x \leq \exp(x) + 1$ die Ungleichung $x \cdot \exp(-x) \leq 1 + \exp(-x)$ und damit $\frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x}$. Mithilfe von Gleichung~(\ref{sigma}) erhalten wir nun 
$$
|\sigma(x) - 1| \leq \frac{1}{x}.
$$
%Daraus erhalten wir mit Umformungen da $x > 0$ und $1 + \exp(-x) > 0$ ist:
%\begin{equation*}
%\begin{split}
%& \quad x \leq \exp(x) + 1 \\
% \Leftrightarrow & \quad x \cdot \exp(-x) \leq 1 + \exp(-x) \\
% \Leftrightarrow & \quad \frac{\exp(-x)}{1 + \exp(-x)} \leq \frac{1}{x} \\
% \Leftrightarrow & \quad 1 - \frac{1}{1 + \exp(-x)} \leq \frac{1}{x} \\
% \Leftrightarrow & \quad |\sigma(x) - 1| \leq \frac{1}{x}.
%\end{split}
%\end{equation*}
%Wobei die letzte Ungleichung aus der Eigenschaft des Betrags kommt, da $\frac{1}{1 + \exp(-x)} - 1< 0$ ist, weil $1 + \exp(-x) > 1$, da $\exp(-x) > 0$. 
Dies zeigt den ersten Teil von Definition~\ref{nzulässig} (iii).

Die zweite Teil folgt durch die gleiche Art und Weise, da wir durch 
\begin{equation}
\label{eq:sym}
\frac{1}{1 + \exp(x)} - \frac{1}{2}= \sigma(0 - x) - \frac{1}{2} = -\sigma(0 + x) + \frac{1}{2} = -\frac{1}{1 + \exp(-x)} + \frac{1}{2}
\end{equation}
wissen, dass $\sigma$ punktsymmetrisch in $(0, \frac{1}{2})$ ist. Dies erhalten wir durch $\frac{1}{1 + \exp(x)} - \frac{1}{2} +\frac{1}{1 + \exp(-x)} - \frac{1}{2} = 0$, indem wir die Brüche auf einen gemeinsamen Nenner bringen und erhalten und dadurch
$\frac{2 + \exp(-x) + \exp(x)}{2 + \exp(-x) + \exp(x)} - 1 = 0$ erhalten, woraus  Gleichung~(\ref{eq:sym}) folgt.
Aus der Punktsymmetrie folgt mit $$\sigma(- x) - 1 = \frac{1}{1 + \exp(x)} - 1 = -\frac{1}{1 + \exp(-x)} = -\sigma(x)$$ für $x < 0$ aus dem ersten Teil von Definition~\ref{nzulässig} (iii), da $-x > 0 $ ist:  
$$|\sigma(x)| = |-\sigma(x)| = |\sigma(- x) - 1| \leq \frac{1}{-x} = \frac{1}{|x|}.$$
Damit haben wir alle drei Bedingungen aus Definition~\ref{nzulässig} gezeigt und unsere Aussage bewiesen.
\end{proof}
Als nächstes stellen wir ein Resultat zur Restgliedabschätzung vor, welches wir im weiteren Verlauf der Arbeit benötigen werden.

\begin{lem}[Lagrangesche Form des Restglieds, {\cite[Kapitel 2.4 Seite 67]{Koenigsberger2013}}]
\label{lem:lagrange}

\textbf{\emph{i)}} Sei $U \subseteq \R^d$ und $f\colon U \to \R$ eine $(N + 1)$-mal stetig differenzierbare Funktion und $u, x \in U$ Punkte, deren Verbindungsstrecke in $U$ liegt, so gilt:
$$ f(x) = T_Nf(x;u) + R_{N + 1}(x;u),$$
wobei
$$
T_N(f;u) \coloneqq \sum_{k = 0}^N \frac{f^{(k)}(u)}{k!} (x - u)^k,
$$
das \emph{Taylorpolynom der Ordnung $N$} von $f$ in $a$ ist und das Restglied mit einem geeigneten Punkt $\xi$ auf der Verbindungsstrecke zwischen $u$ und $x$, in der Form 
$$ R_{N + 1}(x;u) = \frac{f^{(N + 1)}(\xi)}{(N + 1)!} (x - u)^{N + 1}$$
dargestellt werden kann.

\textbf{\emph{ii)}} Sei $I \subseteq \R$ ein Intervall und $f \colon I \to \R$ eine $(N + 1)$-mal stetig differenzierbare Funktion und $u, x \in I$. Dann existiert ein $\xi$ zwischen $u$ und $x$, so dass $$f(x) = \sum_{k = 0}^N \frac{f^{(k)}(u)}{k!}(x - u)^k + \frac{f^{(N + 1)}(\xi)}{(N + 1)!}(x - u)^{N + 1}.$$
\end{lem}
Es folgen nun drei Approximationsresultate, welche wir in Kapitel~\ref{chap:2} für die Konstruktion unseres Neuronale-Netze-Regressionschätzers benötigen werden.
Wir betrachten erst eine Approximation der Identität durch das neuronale Netz $f_{\id}\colon \R \to \R$ mit Architektur~$(1,(1))$, welches in Abbildung~\ref{fig:fid} veranschaulicht wird und anschließend die Approximation eines Quadrats auf einem Intervall durch das neuronale Netz $f_{\sq}\colon \R \to \R$ mit Architektur~$(1,(2))$.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
& & \\
};
\foreach \ai [count=\mi ]in {2}
  \draw[<-] (mat-\ai-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {2}
{\foreach \aii in {2}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2}
  \draw[->] (mat-\ai-2) -- (mat-2-3);
\draw[->] (mat-2-3) -- node[above] {$f_{\id}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\id}(x)$ mit Architektur $(1,(1))$.}
\label{fig:fid}
\end{figure}
 
\begin{lem}
  \label{lem:1}
  Sei $\sigma \colon \R \to \R$ eine Funktion, $R, a > 0$ und $\|\cdot\|_{\infty}$ die Supremumsnorm auf $[-a, a]$.
  \begin{itemize}
  \item[a)] Angenommen $\sigma$ ist zweimal stetig differenzierbar und $t_{\sigma,\id} \in \R$ so, dass $\sigma'(t_{\sigma, \id}) \neq 0$ ist. Dann gilt für das neuronale Netz
  $$ f_{\id}(x) = \frac{R}{\sigma'(t_{\sigma, \id})} \cdot \left(\sigma\left(\frac{x}{R} + t_{\sigma, \id}\right) - \sigma(t_{\sigma, \id})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R}.$$
  \item[b)] Angenommen $\sigma$ ist dreimal stetig differenzierbar und $t_{\sigma,\sq} \in \R$ so, dass $\sigma''(t_{\sigma, \sq}) \neq 0$ ist. Dann gilt für das neuronale Netz
  $$ f_{\sq}(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\sigma\left(\frac{2 \cdot x}{R} + t_{\sigma, \sq}\right) - 2 \cdot \sigma\left(\frac{x}{R} + t_{\sigma, \sq}\right)+ \sigma(t_{\sigma, \sq})\right)$$
  für beliebige $x \in [-a, a]\colon$ 
  $$ |f_{\sq}(x) - x^2| \leq \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}.$$
  \end{itemize}
\end{lem}
\begin{proof}
	\begin{itemize}
  	\item[a)] Wir wissen, dass $f_{\id}$  zweimal differenzierbar ist, da nach Voraussetzung $\sigma$ zweimal stetig differenzierbar ist. Damit folgt mit Lemma~\ref{lem:lagrange} für $f = f_{\id}, N = 1, u = 0$ und $I = [-a, a]$, dass ein $\xi$ zwischen $0$ und $x$ existiert, so dass,
$$f_{\id}(x) = \sum_{k = 0}^1 \frac{f_{\id}^{(k)}(0)}{k!}x^k + \frac{f_{\id}^{(2)}(\xi)}{2!}x^{2}.$$
Es gilt:
$$
f_{\id}'(x) = \frac{1}{\sigma'(t_{\sigma,\id})} \cdot \sigma'\left(\frac{x}{R} + t_{\sigma,\id}\right).
$$
Mit $f_{\id}(0) = 0$ und $f_{\id}'(0) = 1$ erhalten wir:
  	\begin{equation*}
  	\begin{split}
  	 |f_{\id}(x) -  & x| \\
  	& = \bigg|0 + 1 \cdot x + \frac{1}{2} \cdot \frac{1}{R \cdot \sigma'(t_{\sigma,\id})} \sigma''(\frac{\xi}{R} + t_{\sigma,\id}) \cdot x^2 - x \bigg| \\
  	& = \bigg| \frac{\sigma''(\frac{\xi}{R} + t_{\sigma,\id})  \cdot x^2}{2R \cdot \sigma'(t_{\sigma, \id})} + x - x\big| \\
  	& \leq \frac{\| \sigma'' \|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma, \id})|} \cdot \frac{1}{R},
  	\end{split}
  	\end{equation*}
  	wobei sich die letzte Ungleichung aus den Eigenschaften der Supremumsnorm und $x^2 \leq a^2$ ergibt, was aus $x \in [-a,a]$ folgt. Daraus erhalten wir die Behauptung.
  	\item[b)]  Die Funktion $f_{\sq}$ ist dreimal differenzierbar, da $\sigma$ nach Voraussetzung dreimal stetig differenzierbar ist. Wie in a) folgt durch Lemma~\ref{lem:lagrange} mit $f = f_{\sq}, N = 2, u = 0$ und $I = [-a, a]$, dass ein $\xi$ zwischen $0$ und $x$ existiert, so dass:
$$f_{\sq}(x) = \sum_{k = 0}^2 \frac{f_{\sq}^{(k)}(0)}{k!}x^k + \frac{f_{\sq}^{(3)}(\xi)}{3!}x^{3}.$$
Es gilt:
$$
f_{\sq}'(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\frac{2}{R} \cdot\sigma'\left(\frac{2x}{R} + t_{\sigma, \sq}\right) - \frac{2}{R} \cdot \sigma'\left(\frac{x}{R} + t_{\sigma, \sq}\right)\right)
$$
und 
$$
f_{\sq}''(x) = \frac{R^2}{\sigma''(t_{\sigma, \sq})} \cdot \left(\frac{4}{R^2} \cdot\sigma''\left(\frac{2x}{R} + t_{\sigma, \sq}\right) - \frac{2}{R^2} \cdot \sigma''\left(\frac{x}{R} + t_{\sigma, \sq}\right)\right).
$$
Mit $f_{\sq}(0) = 0$, $f_{\sq}'(0) = 0$ und $f_{\sq}''(0) = 2$ erhalten wir:
\begin{equation*}
  	\begin{split}
  	 |f_{\sq}(x) -  & x^2| \\
  	& = \bigg|x^2 + \frac{1}{6} \cdot \frac{R^2}{\sigma''(t_{\sigma,\sq})} \left(\frac{8}{R^3}\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq}) - \frac{2}{R^3} \sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})\right) \cdot x^3 - x^2\bigg| \\
  	& \leq \frac{a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \left(8 \cdot |\sigma'''(\frac{2\xi}{R} + t_{\sigma,\sq})| + 2 |\sigma'''(\frac{\xi}{R} + t_{\sigma,\sq})|\right) \\
  	& \leq \frac{10 \cdot a^3}{6 \cdot |\sigma''(t_{\sigma,\sq})|} \cdot \frac{1}{R} \cdot \|\sigma'''\|_{\infty} \\
  	& = \frac{5 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma, \sq})|} \cdot \frac{1}{R}. 
  	\end{split}
  	\end{equation*}
 	\end{itemize}
 	Daraus folgt die Behauptung.
\end{proof}
Wir betrachten als Nächstes die Approximation einer Multiplikation zweier Werte auf einem Intervall durch das neuronale Netz $f_{\mult}\colon \R^2 \to \R$ mit Architektur $(1,(4))$, welches in Abbildung~\ref{fig:fmult} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| \\
& & |[plain]| \\
|[plain]| & |[plain]| & \\
& & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| \\
|[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-4-1) -- node[above] {$x$} +(-1cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$y$} +(-1cm,0);
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-5-3);
\draw[->] (mat-5-3) -- node[above] {$f_{\mult}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mult}(x, y)$ mit Architektur $(1,(4))$.}
\label{fig:fmult}
\end{figure}

\begin{lem}
  \label{lem:2}
  Sei $\sigma \colon \R \to [0, 1]$ 2-zulässig. Zudem sei $R > 0$ und $a > 0$ beliebig. Dann gilt für das neuronale Netz
  \begin{equation*}
  	\begin{split}
  	f_{\mult}(x, y) = \frac{R^2}{4 \cdot \sigma''(t_{\sigma})} \cdot & \bigg(\sigma\Big(\frac{2 \cdot (x + y)}{R} + t_{\sigma} \Big) - 2 \cdot \sigma\Big(\frac{x + y}{R} + t_{\sigma}\Big) \\
  	& - \sigma \Big(\frac{2 \cdot (x - y)}{R} + t_{\sigma} \Big) + 2 \cdot \sigma \Big(\frac{x - y}{R} + t_{\sigma} \Big) \bigg)
  	\end{split}
  	\end{equation*}
  	für beliebige $x, y \in [-a, a]$ folgende Ungleichung:
  	$$|f_{\mult}(x, y) - x \cdot y| \leq \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Durch Einsetzen erhalten wir $$f_{\mult}(x,y) = \frac{1}{4}(f_{\sq}(x + y) - f_{\sq}(x - y))$$ und $$x \cdot y = \frac{1}{4}\big((x + y)^2 - (x - y)^2\big).$$
  Aus diesen beiden Gleichungen folgt durch Ausklammern von $\frac{1}{4}$, der Homogenität des Betrags und der Dreiecksungleichung:
  \begin{equation*}
  \begin{split}
  |f_{\mult}(x, y) - x \cdot y| & = \frac{1}{4} \cdot \big|f_{\sq}(x + y) - f_{\sq}(x - y) - (x + y)^2 + (x - y)^2\big| \\
  & \leq \frac{1}{4} \cdot \big|f_{\sq}(x + y) - (x + y)^2\big| + \frac{1}{4}\cdot\big| (x - y)^2 - f_{\sq}(x - y)\big| \\
  & \leq 2 \cdot \frac{1}{4} \cdot \frac{40 \cdot \|\sigma'''\|_{\infty} \cdot a^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \\
  & = \frac{20 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R},
  \end{split}
\end{equation*}   
wobei wir bei der letzten Ungleichung Lemma~\ref{lem:1}b) mit $x + y, x - y \in [-2a, 2a]$ verwendet haben. Daraus folgt die Behauptung.
  \end{proof}
Wir betrachten als nächstes die Approximation der Maximumsfunktion $\max\{x, 0\}$ für $x$ aus einem Intervall, durch das neuronale Netz $f_{\ReLU}\colon \R \to \R$ mit Architektur $(2,(2,4))$, welches in Abbildung~\ref{fig:frelu} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-5-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {5}
{\foreach \aii in {4,6}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-3) -- (mat-5-4);
\draw[->] (mat-5-4) -- node[above] {$f_{\ReLU}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\ReLU}(x)$ mit Architektur $(2,(2,4))$.}
\label{fig:frelu}
\end{figure}

  \begin{lem}
  \label{lem:3}
  Sei $\sigma\colon \R \to [0, 1]$ 2-zulässig und $a \geq 1$. Sei $f_{\mult}$ das neuronale Netz aus Lemma~\ref{lem:2} und $f_{\id}$ das neuronale Netz aus Lemma~\ref{lem:1}. Angenommen es gelte die Ungleichung 
\begin{equation}
\label{lem:3:Vor1}   
 R \geq \frac{\|\sigma''\|_{\infty} \cdot a}{2 \cdot |\sigma'(t_{\sigma})|}.
\end{equation}
  Dann erfüllt das neuronale Netz 
  \begin{equation}
  \label{lem:3:Vor2}
  \begin{split}
  f_{\ReLU}(x) & = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) 
  \end{split}
  \end{equation}
 für alle $x \in [-a, a]$ folgende Ungleichung:
 $$|f_{\ReLU}(x) - \max\{x, 0\}| \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Da $\sigma$ nach Voraussetzung 2-zulässig ist, gilt für $R \geq 0$ und $x \in \R\setminus\{0\}\colon$ $$|\sigma(R \cdot x) - 1| \leq \frac{1}{R\cdot x} \text{\, für \,} x > 0$$ und $$|\sigma(R \cdot x)| \leq \frac{1}{|R \cdot x|} \text{\, für \,} x < 0.$$
Damit folgt aus der Homogenität des Betrags für alle $x \neq 0$: 
\begin{equation}
\label{lem:3:Bed1} 
|\sigma(R \cdot x) - \1_{[0, \infty)}(x)| \leq \frac{1}{|R \cdot x|} = \frac{1}{R \cdot |x|}.
\end{equation} 
Nach Lemma~\ref{lem:1} und Lemma~\ref{lem:2} gilt:
\begin{equation}
\label{lem:3:Bed2} 
|f_{\id}(x) - x| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x \in [-a, a]
\end{equation}
 und 
\begin{equation}
\label{lem:3:Bed3}
 |f_{\mult}(x, y) - x \cdot y| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R} \text{\, für \,} x, y \in [-2a, 2a].
 \end{equation} 
Da nach Voraussetzung $a \geq 1$ ist, gilt insbesondere $[0, 1] \subseteq [-2a, 2a]$ und daher gilt insbesondere $\sigma(Rx) \in [0, 1]\subseteq [-2a, 2a].$ 
Zudem erhalten wir durch eine Nulladdition und der Dreiecksungleichung:
\begin{equation}
\label{eq:id1}
\begin{split}
|f_{\id}(x)| & = |f_{\id}(x) - x + x| \leq |f_{\id}(x) -x| + |x|.
\end{split}
\end{equation}
Aus Gleichung~(\ref{lem:3:Bed2}) und Voraussetzung~(\ref{lem:3:Vor1}), folgt
\begin{equation}
\label{eq:id2}
\begin{split}
|f_{\id}(x) -x| & \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R}
\leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{2 \cdot |\sigma'(t_{\sigma})|}{\|\sigma''\|_{\infty} \cdot a}
= a.
\end{split}
\end{equation}
Aus $x \in [-a, a]$ folgt schließlich aus den Ungleichungen~(\ref{eq:id1}) und (\ref{eq:id2}):
\begin{equation*}
|f_{\id}(x)| \leq |f_{\id}(x) -x| + |x| \leq 2 \cdot a.
\end{equation*}
Daraus folgt insbesondere $f_{\id}(x) \in [-2a, 2a].$
Mithilfe von $\max\{x, 0 \} = x \cdot \1_{[0, \infty)}(x)$, der Definition des Netzes~(\ref{lem:3:Vor2}), zweier Nulladditionen und zweimal der Dreiecksungleichung erhalten wir:
\begin{equation}
\label{eq:relu1}
\begin{split}
|f_{\ReLU}(x) - & \max\{x, 0\}|  \\
& = \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - x \cdot \1_{[0, \infty)}(x)\big| \\
& \leq \big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \\
& \qquad + \big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| + \big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big|. 
\end{split}
\end{equation}
Wenden wir nun auf den ersten Summanden in Gleichung~(\ref{eq:relu1}) die Ungleichung~(\ref{lem:3:Bed3}) an, erhalten wir:
\begin{equation}
\label{eq:relu2}
\big| f_{\mult}(f_{\id}(x), \sigma(R \cdot x)) - f_{\id}(x)\cdot\sigma(R \cdot x)\big| \leq \frac{160 \cdot \|\sigma'''\|_{\infty} \cdot a ^3}{3 \cdot |\sigma''(t_{\sigma})|} \cdot \frac{1}{R}.
\end{equation}
Klammern wir nun im zweiten Summanden in Gleichung~(\ref{eq:relu1}) den Faktor $\sigma(R \cdot x) \in [0, 1]$ aus und wenden Ungleichung~(\ref{lem:3:Bed2}) an, erhalten wir nach Voraussetzung mit $a^2 \leq a^3$:
\begin{equation}
\label{eq:relu3}
\big| f_{\id}(x)\cdot\sigma(R \cdot x) - x \cdot \sigma(R \cdot x)\big| \leq \frac{\|\sigma''\|_{\infty} \cdot a^2}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R} \cdot 1 \leq \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} \cdot \frac{1}{R}.
\end{equation}
Wenn wir nun im dritten Summanden in Gleichung~(\ref{eq:relu1}), den Faktor $|x|$ durch die Homogenität des Betrag ausklammern und Ungleichung~(\ref{lem:3:Bed1}) anwenden, erhalten wir:
\begin{equation}
\label{eq:relu4}
\big| x \cdot \sigma(R \cdot x) - x \cdot \1_{[0, \infty)}(x)\big| \leq \frac{1}{R} = \frac{a^3}{a^3} \cdot \frac{1}{R}.
\end{equation}
Setzen wir nun Ungleichungen~(\ref{eq:relu2})-~(\ref{eq:relu4}) in Ungleichung~(\ref{eq:relu1}) ein, ergibt sich durch Ausklammern von $\frac{1}{R}$ und Umformungen:
\begin{equation*}
\begin{split}
|f_{\ReLU}(x) - \max\{x, 0\}| & \leq \bigg(\frac{160}{3} \cdot \frac{\|\sigma'''\|_{\infty} \cdot a ^3}{|\sigma''(t_{\sigma})|} + \frac{\|\sigma''\|_{\infty} \cdot a^3}{2 \cdot |\sigma'(t_{\sigma})|} + \frac{a^3}{a^3} \bigg) \cdot \frac{1}{R} \\ 
& \leq \bigg(\frac{160 \cdot\|\sigma'''\|_{\infty} \cdot a ^3 + 3 \cdot \|\sigma''\|_{\infty} \cdot a^3 + 3 \cdot a^3}{3 \cdot \min\{ 2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot \frac{1}{R}\\
& \leq \frac{166}{3} \cdot \bigg(\frac{\max\{\|\sigma'''\|_{\infty}, \|\sigma''\|_{\infty} , 1\}}{\min\{ 2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}}\bigg) \cdot a^3 \cdot  \frac{1}{R} \\
& \leq 56 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma})|, |\sigma''(t_{\sigma})|, 1\}} \cdot a^3 \cdot \frac{1}{R}.
\end{split}
\end{equation*}
Daraus folgt die Behauptung.
  \end{proof}
Wir betrachten als nächstes die Approximation des Positivteils einer Funktion auf einem Intervall durch das neuronale Netz $f_{\mathrm{hat},y}\colon \R \to \R$ mit Architektur $(2, (6, 12))$, welches in Abbildung~\ref{fig:fhat} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-14-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {14}
{\foreach \aii in {4,6,13,15,22,24}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {13,15}
{\foreach \aii in {11,13,15,17}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {22,24}
{\foreach \aii in {20,22,24,26}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8,11,13,15,17,20,22,24,26}
  \draw[->] (mat-\ai-3) -- (mat-14-4);
\draw[->] (mat-14-4) -- node[above] {$f_{\mathrm{\mathrm{hat}},y}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mathrm{\mathrm{hat}},y}(x)$ mit Architektur $(2, (6, 12))$.}
\label{fig:fhat}
\end{figure} 
  \begin{lem}
  \label{lem:4}
  Sei $M \in \N$ und sei $\sigma\colon \R \to [0, 1]$ 2-zulässig. Sei $a > 0$ und $$R \geq \frac{\|\sigma''\|_{\infty} \cdot (M + 1)}{2 \cdot |\sigma'(t_{\sigma})|},$$ sei $y \in [-a, a]$ und $f_{\ReLU}$ das neuronale Netz aus Lemma~\ref{lem:3}. Dann erfüllt das neuronale Netz 
  \begin{equation*}
  \begin{split}
  f_{\mathrm{hat},y}(x) = f_{\ReLU}& \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) \\ &+ f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg)
  \end{split}
  \end{equation*}
  für alle $x \in [-a ,a]$ mit $z_+ = \max\{0, z\} \, (z \in \R):$   
  $$\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma, \id}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R}.$$
  \end{lem}
  \begin{proof}
  Für $x \in \R$ gilt die Gleichung:
\begin{equation}
\label{lem:4:eq}
 (1 - \frac{M}{2a} \cdot |x|)_+ = \max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}, 
 \end{equation}  
die wir im zweiten Teil dieses Beweises zeigen werden. Damit beweisen wir das Resultat mit Hilfe von Lemma~\ref{lem:3}, denn mit der Definition von $f_{\mathrm{hat},y}(x)$ und der Dreiecksungleichung folgt:
\begin{equation*}
\begin{split}
\bigg|f_{\mathrm{hat},y}(x) - \bigg(1 - \frac{M}{2a} \cdot |x - y|\bigg)_+\bigg| & \leq \bigg|f_{\ReLU} \bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) + 1, 0\}\bigg| \\ 
& + 2 \cdot \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) - \max\{\frac{M}{2a} \cdot (x - y), 0\}\bigg| \\
& + \bigg|f_{\ReLU}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg) - \max\{\frac{M}{2a} \cdot (x - y) - 1, 0\}\bigg|\\ 
& \leq 1792 \cdot \frac{\max\{|\sigma''\|_{\infty}, \|\sigma'''\|_{\infty}, 1\}}{\min\{2 \cdot |\sigma'(t_{\sigma}), |\sigma''(t_{\sigma})|, 1\}} \cdot M^3 \cdot \frac{1}{R},
\end{split}
\end{equation*} 
wobei die letzte Ungleichung daraus folgt, dass wir auf jeden Summanden Lemma~\ref{lem:3} mit $$1 \leq a = M + 1$$ angewendet haben, da aus $x , y \in [-a, a]$ folgt, dass $\frac{M}{2a}\cdot (x - y) \in [-M, M]$ gilt. Schließlich haben wir
$$ (M + 1)^3 \leq (2M)^3 = 8M^3$$ verwendet, da $M \geq 1$ ist. $\hfill(\square)$

Um Gleichung~(\ref{lem:4:eq}) zu zeigen unterscheiden wir vier Fälle.
  \begin{itemize}
  \item[Fall 1] ($x < 0$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) nach der Definition des Betrags die Gestalt $$\max\{1 + \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot 0 + 0,$$ da $x < 0$ und damit die letzten zwei Summanden 0 sind. Es erfordert hier eine weitere Fallunterscheidung.
 \item[Fall 1.1] ($0 > x \geq -\frac{2a}{M}$) In diesem Fall gilt für die linke und rechte Seite von Gleichung~(\ref{lem:4:eq}):
 $$\max\{1 + \frac{M}{2a} \cdot x, 0\} = 1 + \frac{M}{2a} \cdot x.$$
 \item[Fall 1.2] ($x < -\frac{2a}{M}$) In diesem Fall sind beide Seiten gleich 0, da $1 + \frac{M}{2a} \cdot x \leq 0$ ist. $\hfill(\square)$
  \item[Fall 2] ($x \geq 0$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) nach der Definition des Betrags die Gestalt $$\max\{1 - \frac{M}{2a} \cdot x, 0\}$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\max\{\frac{M}{2a} \cdot x + 1, 0\} - 2 \cdot \max\{\frac{M}{2a} \cdot x, 0\} + \max\{\frac{M}{2a} \cdot x - 1, 0\}$$ und erfordert daher eine weitere Fallunterscheidung.
 \item[Fall 2.1] ($0 \leq x < \frac{2a}{M}$) In diesem Fall hat die linke Seite von Gleichung~(\ref{lem:4:eq}) die Gestalt $$1 - \frac{M}{2a} \cdot x$$ und die rechte Seite von Gleichung~(\ref{lem:4:eq}) die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + 0 = 1 - \frac{M}{2a} \cdot x$$ und stimmt daher mit der linken Seite überein.
 \item[Fall 2.2] ($x \geq \frac{2a}{M}$) In diesem Fall ist die linke Seite von Gleichung~(\ref{lem:4:eq}) gleich 0, da $1 - \frac{M}{2a} \cdot x < 0$ ist und die rechte Seite besitzt die Form $$\frac{M}{2a} \cdot x + 1 - 2 \cdot \frac{M}{2a} \cdot x + \frac{M}{2a} \cdot x - 1 = 0. $$ $\hfill(\square)$
\end{itemize} 
Durch diese Fallunterscheidung wurde die Gleichung~(\ref{lem:4:eq}) bewiesen und damit ist der Beweis vollständig.
  \end{proof}
Das nächste Lemma ist ein Resultat aus der Kombinatorik, welches wir in Kapitel~\ref{chap:2} benötigen werden.
\begin{lem}
\label{lem:kombi}
Sei $d, N \in \N$ und $k \in \N_0$ mit $k < N$. Dann gilt:
$$\bigg|\Bigl\{\bj \in [N]^d : |\bj|_1 = k \Bigr\}\bigg| = \binom{d + k - 1}{k}.$$
\end{lem}
\begin{proof}
Diese Aussage folgt aus einer Analogie zu einem Urnenexperiment. Wir betrachten eine Urne mit $d$ Kugeln, die wir mit $1,\dots,d$ beschriften. Wir ziehen $k$-mal aus dieser Urne mit Zurücklegen und ohne Beachtung der Reihenfolge und konstruieren so einen Vektor $\bj = (j_1,\dots,j_d)$ aus der Menge mit $|\bj|_1 = k$. Der Koeffizient $j_i$ mit $i= 1,\dots,d$ gibt an wie oft die Kugel mit der Nummer $i$ gezogen wurde. Damit stimmt die Kardinalität der Menge auf der linken Seite mit der Anzahl aller Möglichkeiten überein die man erhält, wenn man $k$-mal aus dieser Urne mit Zurücklegen und ohne Beachtung der Reihenfolge zieht.
\end{proof}

Im nächsten Kapitel werden wir die hier eingeführten neuronalen Netze als Bausteine verwenden, um unseren Neuronale-Netze-Schätzer zu konstruieren.