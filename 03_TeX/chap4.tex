\chapter{Anwendungsbeispiel auf simulierte Daten}
\label{chap:4}

In diesem Kapitel betrachten wir die Leistung unseres Neuronale-Netze"=Regressionsschätzers aus Kapitel~\ref{chap:2}, bei endlicher Stichprobengröße auf simulierte Daten in $Python$~\cite[Version 3.7.3]{van1995python} mithilfe der $Keras$ Bibliothek \cite{chollet2015keras}.

Die simulierten Daten, welche wir verwenden werden, sehen wie folgt aus:
Wir wählen $X$ gleichverteilt auf $[-2, 2]^d$, wobei $d$ die Dimension des Inputs ist, zudem wählen wir $\epsilon$ standardnormalverteilt und unabhängig von $X$ und wir definieren $Y$ durch:
$$Y = m_j(X) + \sigma \cdot \lambda_j \cdot \epsilon,$$ 
wobei $m_j \colon [-2, 2]^d \to \R$ für  $j \in \{1, 2\}$ den Funktionen:
$$ m_1(x) =  \sin\big(0.2 \cdot x^2\big) + \exp(0.5 \cdot x) + x^3$$
und
$$ m_2(x_0, x_1) = \sin\big(\sqrt[2]{x_0^2 + x_1^2}\big)$$
entspricht.
Den Skalierungsfaktor $\lambda_j > 0$ wählen wir als Interquartilsabstand \emph{(IQR)} einer Stichprobe der Größe $N = 800$ von $m(X)$. Für den Rauschfaktor $\sigma$ gilt $\sigma \in \{0.05, 0.1\}.$ Mit diesen Daten lässt sich nun auch $Y$ darstellen.

Um die Leistung unseres Neuronale-Netze-Regressionsschätzers zu überprüfen, haben wir erstmals die Funktion $m_1$ und die jeweilige Schätzung durch unseren Neuronale-Netze-Regressionsschätzer für $\sigma = 0.05$ zeichnen lassen. 
Visuell kann man mit Abbildung~\ref{fig:subfig1.a.4} erkennen, dass der Schätzer eine sehr gute Approximation der Funktion liefert, aber um genauer beurteilen zu können wie gut die Schätzung wirklich ist und wie gut unser Schätzer im Vergleich zu anderen Schätzern abschneidet, betrachten wir in Tabelle~\ref{tab:truthTablesm1} und Tabelle~\ref{tab:truthTablesm2} den Interquartilsabstand und den Median des skalierten $L_2$-Fehlers der einzelnen Schätzer. 

Unser Vorgehen zum Vergleich der drei hier betrachteten Regressionsschätzer gestaltet sich wie folgt:
Wir bestimmen als Erstes den $L_2$-Fehler der einzelnen Schätzer $m_{n,i}$ für $i = 1,2,3$ approximativ durch den empirisch arithmetischen $L_2$-Fehler $\epsilon_{L_2,N}(m_{n,i})$ auf einer unabhängigen Stichprobe von $X$ der Größe $N = 1000$. 

Da wir unsere Regressionsfunktionen kennen und der Fehler stark vom Verhalten der Funktion $m_j$ abhängt, betrachten wir den empirischen $L_2$-Fehler im Verhältnis zum simpelsten Schätzer von $m_j$, einer konstanten Funktion. Der Wert dieses \textit{konstanten Schätzers} bestimmen wir, in dem wir das empirische Mittel der beobachteten Daten $Y$ nehmen. Wir erhalten damit ein skaliertes Fehlermaß $\epsilon_{L_2,N}(m_{n,i})/\bar{\epsilon}_{L_2,N}(avg)$ mit $\bar{\epsilon}_{L_2,N}(avg)$ als Median von $50$ unabhängigen Realisierungen von $\epsilon_{L_2,N}$. 

Wir bestimmten $\epsilon_{L_2,N}(\cdot)$ als empirisches Mittel von $25$ quadratischen Fehlern der Schätzung des konstanten Schätzers. Dieses skalierte Fehlermaß ist so zu deuten, dass ein großer Fehler durch einen der drei Regressionsschätzer im Falle, dass der Fehler des konstanten Schätzers klein ist, auf eine noch schlechtere Leistung hindeutet. Der Fehler $\epsilon_{L_2,N}(m_{n,i})$ wird also durch $\bar{\epsilon}_{L_2,N}(avg)$ gewichtet.

Die resultierenden skalierten Fehler hängen noch von der Stichprobe von $(X, Y)$ ab und um diese Werte besser vergleichen zu können, führen wir die Fehlerberechnung jeweils $50$ mal durch und geben dann den Median und den Interquartilsabstand für die Schätzung der betrachteten Regressionsschätzer aus.
Wir teilen für jeden Schätzer die Stichprobe in ein \textit{learning sample} der Größe $n_l = 0.8 \cdot N$ und in ein \textit{testing sample} der Größe $n_t = 0.2 \cdot N$ auf. Wir bestimmen die Parameterwerte für die Schätzer auf dem learning sample und bestimmen das korrespondiere $L_2$-Risiko auf dem testing sample.

Unser erster Schätzer \textit{fc\_neural\_1\_estimate} ist ein neuronales Netz mit einer verborgenen Schicht. Dieser Schätzer hat eine feste Anzahl an Neuronen die wir aus der Menge $\{5, 10, 25, 50, 75\}$ auswählen die bei der Simulation zu einem minimalen empirischen $L_2$-Risiko führt.

Unser zweiter Schätzer \textit{nearest\_neighbor\_estimate} ist ein Nächste-Nachbar-Schätzer bei der die Anzahl an nächsten Nachbarn so aus der Menge $\{1, 2, 3\} \cup \{4, 8, 12, 16, \dots, 4 \cdot \lfloor\frac{n_l}{4}\rfloor\}$ ausgewählt wird, dass dieser zu minimalen empirischen $L_2$-Risiko führt.

Unser letzter Schätzer ist der in Kapitel~\ref{chap:2} vorgestellte Neuronale-Netze-Regressionsschätzer den wir mit \textit{new\_neural\_network\_estimate} bezeichnen. Hier haben wir die Parameter, je nachdem, welche Regressionsfunktion wir betrachtet haben, entsprechend angepasst, da wir zum Beispiel den Grad der zu schätzenden Funktion kennen und dies bei unserem Schätzer berücksichtigen können. Für die Schätzung von $m_1$ setzten wir: $d = 1$, $N = 3$, $q = 2$, $R = 10^6$, $a = 2$, $M = 2$.
Für die Schätzung von $m_2$ wählten wir: $d = 2$, $N = 2$, $q = 2$, $R = 10^6$, $a = 2$, $M = 2$.

\begin{figure}
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \scalebox{0.9}{
          \input{m1tikz.tex}}
        \caption{Zu schätzende Funktion $m_1$. \\\qquad  }
        \label{fig:subfig8}
    \end{subfigure}
    \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \scalebox{0.9}{
           \input{nnnestimatetikz.tex}}
        \caption{Schätzung von $m_1$ durch unseren Neuronale-Netze-Regressionsschätzer.}   
        \label{fig:subfig9}
    \end{subfigure}
       \hspace{0.4cm}
    \begin{subfigure}[b]{1\textwidth}
    \centering
    \scalebox{0.9}{
	\input{tikzd1.tex}}
	 \caption{Visueller Vergleich der Funktion $m_1$ mit ihrer Schätzung.}
        \label{fig:subfig810}
    \end{subfigure}
    \caption{Darstellung der Funktion $m_1$ und dessen Schätzung durch den Schätzer \textit{new\_neural\_network\_estimate}.} 
\label{fig:subfig1.a.4}
\end{figure}

%\begin{figure}
%\centering
%%\input{tikzfigure1.tikz}
%\begin{tikzpicture}
%\begin{axis}[x dir=reverse]
%\addplot3 [scatter, only marks]
%  table[x=x, y=y, z=f, col sep=comma] {plotpostpro_sort.csv};
%\end{axis}
%\end{tikzpicture}
%\end{figure}

Wie wir in Tabelle~\ref{tab:truthTablesm1} und Tabelle~\ref{tab:truthTablesm2} anhand des skalierten $L_2$-Fehlers sehen können, übertrifft unserer Neuronale-Netze-Regressionsschätzer in allen Fällen die Leistung der anderen Schätzer. 
\begin{table}
\centering
\begin{tabular}{ |p{5cm}||p{4cm}|p{4cm}|}
 \hline
 & \multicolumn{2}{|c|}{$m_1$}\\
 \hline
 \textit{noise}& $5\%$ & $10\%$ \\
 \hline
 $\bar{\epsilon}_{L_2,N}(avg)$& $13.4482362$ & $13.3925910$ \\
 \hline
 \textit{approach}&  Median (IQR) &  Median (IQR)   \\
 \hline
 new\_neural\_network\_estimate & $\mathbf{2.482\textbf{e-}05 (1.612\textbf{e-}05)}$   & $\mathbf{6.908\textbf{e-}05 (3.936\textbf{e-}05)}$  \\
 fc\_neural\_1\_estimate & $4.384\text{e-}04 (2.14\text{e-}03)$ &   $7.261\text{e-}04 (4.57\text{e-}03)$ \\
 nearest\_neighbor\_estimate & $2.9527\text{e-}04 (9.312\text{e-}05)$ & $9.0864\text{e-}04 (2.8952\text{e-}04)$\\
 \hline
\end{tabular}
    \caption{Median und IQR des skalierten empirischen $L_2$-Fehlers für Schätzungen für $m_1$ mit Stichprobengröße $N = 1000$.}
     \label{tab:truthTablesm1}   
\end{table}
    
    \begin{table}
\centering
\begin{tabular}{ |p{5cm}||p{4cm}|p{4cm}|}
 \hline
 & \multicolumn{2}{|c|}{$m_2$}\\
 \hline
 \textit{noise}& $5\%$ & $10\%$ \\
 \hline
 $\bar{\epsilon}_{L_2,N}(avg)$& $0.0324$ & $0.0311$ \\
 \hline
 \textit{approach}&  Median (IQR) &  Median (IQR)   \\
 \hline
 new\_neural\_network\_estimate & $\mathbf{0.003961 (0.000932)}$   & $\mathbf{0.00431 (0.000973)}$  \\
 fc\_neural\_1\_estimate & $0.0257 (0.3803)$ &   $0.0559 (0.52033)$ \\
 nearest\_neighbor\_estimate & $0.01616 (0.005906)$ &$0.01763 (0.007081)$\\
 \hline
\end{tabular}
    \caption{Median und IQR des skalierten empirischen $L_2$-Fehlers für Schätzungen $m_2$ mit Stichprobengröße $N = 1000$.}
    \label{tab:truthTablesm2}   
\end{table}
