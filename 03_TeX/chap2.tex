\chapter{Konstruktion des Neuronale-Netze-Schätzers}
\label{chap:2}

In diesem Kapitel werden wir mithilfe der gegebenen Datenmenge 
\begin{equation}
\label{dataset}
\mathcal{D}_n = \{(X_1, Y_1),\dots,(X_n, Y_n)\},
\end{equation}
unseren Regressionsschätzer $\tilde{m}_n$ konstruieren. 
In Kapitel~\ref{chap:1} haben wir bereits in Definition~\ref{def:nn} vorgestellt, was wir unter einem mehrschichtigen feedforward neuronalen Netz verstehen.

Für die Konstruktion unseres Neuronale-Netze-Schätzers wählen wir den logistischen Squasher~(\ref{logsquasher}) als Aktivierungsfunktion $\sigma$, verwenden die gegebene Datenmenge $\mathcal{D}_n$ und wählen die Gewichte des neuronalen Netzes so, dass die resultierende Funktion aus Definition~\ref{def:nn} eine gute Schätzung für die Regressionsfunktion $m$ ist. Dafür wählen wir die Gewichte bis auf die in der Ausgabeschicht fest und schätzen die Gewichte in der Ausgabeschicht, indem wir mit unserer Datenmenge~(\ref{dataset}) ein regularisiertes Kleinste-Quadrate-Problem (REFERENZ) lösen.

\section{Definition der Netzwerkarchitektur}
Zunächst fixieren wir die Multiindexnotation, die wir aufgrund der Übersichtlichkeit im weiteren Verlauf dieser Arbeit verwenden werden. Sei $a > 0$ fest und $M \in \N$. 

Sei $[M]^d \coloneqq\{0, 1, \dots, M\}^d.$ 
Für $(\mathbf{i}^{(1)},\dots,\mathbf{i}^{(d)}) = \bi \in [M]^d$ und $x \in \R^d$ definieren wir
$$|\mathbf{i}|_1 \coloneqq \sum_{k= 1}^d \mathbf{i}^{(k)} \quad \mathbf{i}! \coloneqq \mathbf{i}^{(1)}! \cdots \, \mathbf{i}^{(d)}! \quad x^{\mathbf{i}} \coloneqq x_1^{\mathbf{i}^{(1)}} \cdots \,    x_d^{\mathbf{i}^{(d)}}.$$
Für $f\colon \R^d \to \R$ ausreichend oft differenzierbar definieren wir 
$$\partial^{\mathbf{i}}f(x) \coloneqq \frac{\partial^{|\mathbf{i}|_1}f}{\partial^{\mathbf{i}^{(1)}} x_1 \cdots \, \partial^{\mathbf{i}^{(d)}} x_d} (x).$$
Wir betrachten im Folgenden ein $d$-dimensionales äquidistantes Gitter im Würfel $[-a, a]^d$ mit Schrittweite $\frac{2a}{M}.$ 
%Sei $\bi_1,\dots,\bi_{(M + 1)^d}$ eine Aufzählung der Elemente von $[M]^d$.
Dann ordnen wir jedem Multiindex $\bi \in [M]^d$ einen Gitterpunkt $x_{\bi}$ mit:
\begin{equation}
\label{eq:gitter}
x_{\bi} = \bigg( -a + \bi^{(1)} \cdot \frac{2a}{M},\dots, -a + \bi^{(d)} \cdot \frac{2a}{M}\bigg) = -\mathbf{a} + \frac{2a}{M} \cdot \bi,
\end{equation}
zu, mit $\mathbf{a} = (a, a, \dots, a) \in \R^d$.

Hiermit lässt sich das zu $m$ gehörige Taylorpolynom der Ordnung $q$ mit Entwicklungspunkt $x_{\bi}$ schreiben als
$$p_{\bi}^m(x) = \sum_{\substack{\mathbf{j} \in [q]^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}m(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}.$$
Zudem betrachten wir eine Funktion
\begin{equation}
\label{konvexkomb}
P_m(x) = \sum_{\bi \in [M]^d} p_{\bi}^m(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+,
\end{equation}
für die wir im weiteren Verlauf dieser Arbeit zeigen werden, dass diese die Regeressionsfunktion $m$ approximiert.

Wir zeigen mithilfe des folgenden Lemmas, dass $P_m(x)$ eine lokale Spline Interpolation von Taylorpolynomen von $m$ ist.
\begin{lem}
\label{lem:loccon}
Sei $a >0$ und $M \in \N$. Dann sind für $\bi \in [M]^d$ und $x \in \R^d$ die Funktionen 
$$B_{\bi}(x) = \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+,$$
B-Splines im $\R^d$, wobei wir unter $x_{\bi}$ die Gitterpunkte aus Gleichung~(\ref{eq:gitter}) verstehen. Dafür sind folgende drei Bedingungen für $x \in \R^d$ zu überprüfen:
\begin{itemize}
\item[\textbf{i)}] Zerlegung der Eins: $\sum_{\bi \in [M]^d} B_{\bi}(x) = 1$.
\item[\textbf{ii)}] Nichtnegativität: $B_{\bi}(x) \geq 0$ für alle $\bi \in [M]^d$.
\item[\textbf{iii)}] Lokaler Träger: Für $\bi \in [M]^d$ ist $B_{\bi}(x) > 0$ falls $|x^{(j)} - x_{\bi}^{(j)}| < \frac{2a}{M}$ für alle $j \in \{1,\dots,d\}$ gilt und andernfalls $B_{\bi}(x) = 0$.
\end{itemize}
\end{lem}
\begin{proof}
Als erstes möchten wir für $d = 2$ und $M = 3$ eine Skizze angeben um die Idee des Beweises zu veranschaulichen. 
\begin{figure}[htp]
\centering
\begin{tikzpicture} 
   %Raster zeichnen 
   %\draw [color=gray!50]  [step=20mm] (-3,-3) grid (4,4); 
   \draw [color=gray!50] (0,0) -- (6,0) -- (6,6) -- (0,6) -- (0,0);			
   \draw [color=gray!50] (0,2) -- (6,2);
   \draw [color=gray!50] (0,4) -- (6,4);
        \draw [color=gray!50] (2,6) -- (2,0);
                \draw [color=gray!50] (4,6) -- (4,0);

	\fill[black] (0,0) circle (0.08cm) node[label=below:{$(-a, -a)$}]{};
		\fill[black] (6,0) circle (0.08cm) node[label=below:{$(a, -a)$}]{};
				\fill[black] (0,6) circle (0.08cm) node[label=above:{$(-a, a)$}]{};
						\fill[black] (6,6) circle (0.08cm) node[label=above:{$(a, a)$}]{};
						\fill[black] (0,0) circle (0.00cm) node[label=left:{$x_{\mathbf{i}_1}$}]{};
						\fill[black] (2,0) circle (0.00cm) node[label=below:{$x_{\mathbf{i}_2}$}]{};
						\fill[black] (4,0) circle (0.00cm) node[label=below:{$x_{\mathbf{i}_3}$}]{};
						\fill[black] (6,0) circle (0.00cm) node[label=right:{$x_{\mathbf{i}_4}$}]{};

						\fill[black] (9,6) circle (0.00cm) node[label=right:{$\mathbf{i}_1 = (0, 0)$}]{};			
						\fill[black] (9,5.5) circle (0.00cm) node[label=right:{$\mathbf{i}_2 = (1, 0)$}]{};					
						\fill[black] (9,5) circle (0.00cm) node[label=right:{$\mathbf{i}_3 = (2, 0)$}]{};	
						\fill[black] (9,4.5) circle (0.00cm) node[label=right:{$\mathbf{i}_4 = (3, 0)$}]{};	
						\draw [dotted, ultra thick] (10,4) -- (10,3.65);
						\fill[black] (9,3.15) circle (0.00cm) node[label=right:{$\mathbf{i}_{16} = (3, 3)$}]{};	
						
						\fill[black] (0,2) circle (0.08cm);
						\fill[black] (0,4) circle (0.08cm); 
						\fill[black] (2,2) circle (0.08cm);
						\fill[black] (2,4) circle (0.08cm);
						\fill[black] (2,6) circle (0.08cm);
						\fill[black] (2,0) circle (0.08cm);
						\fill[black] (0,4) circle (0.08cm);
						\fill[black] (2,4) circle (0.08cm);
						\fill[black] (4,4) circle (0.08cm);
						\fill[black] (4,6) circle (0.08cm);																\fill[black] (4,0) circle (0.08cm);
						\fill[black] (4,2) circle (0.08cm);						
						\fill[black] (6,2) circle (0.08cm);
						\fill[black] (6,4) circle (0.08cm);
\end{tikzpicture}
\caption{Beispielhafte Darstellung der $x_{\mathbf{i}_k}$ für $d = 2$ und $M = 3$.}
\label{fig:gitter}
\end{figure}
Es ist ein Gitter mit $(M + 1)^d$ Gitterpunkten die den $x_{\mathbf{i}_k}$ entsprechen. Der Abstand zwischen zwei Gitterpunkten beträgt $\frac{2a}{M}.$ Man betrachtet immer den Abstand zu den nächsten $2^d$ Gitterpunkten, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}}^{(j)}$ größer als $\frac{2a}{M}$ ist.     

\emph{\textbf{i)}} Im Folgenden wollen wir 
\begin{equation}
\label{induktiongl1}
\sum_{\bi \in [M]^d} B_{\bi}(x) = 1 \quad (x \in \R^d)
\end{equation}
per Induktion über $d$ zeigen. 
	
	 \emph{Induktionsanfang} (IA): Für $d = 1$ kann $x$ nur zwischen zwei Gitterpunkten $x_{\mathbf{i}_1} \neq x_{\mathbf{i}_2}$ liegen. Sei ohne Beschränkung der Allgemeinheit $x_{\mathbf{i}_1}\leq x \leq x_{\mathbf{i}_2}$, dann gilt mit der gleichen Begründung wie im einleitenden Beispiel:
	\begin{equation*}
	\begin{split}
	\sum_{\bi \in [M]^d} (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}}|)_+ & = (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_1}|)_+ + (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_2}|)_+ \\
	& = 1 + 1 - \frac{M}{2a} \cdot (x - x_{\mathbf{i}_1} + x_{\mathbf{i}_2} - x) \\
	& = 1 + 1 - \frac{M}{2a} \cdot \frac{2a}{M} \\
	& = 1,
	\end{split}
	\end{equation*} wobei wir unter anderem verwendet haben, dass beide Summanden unabhängig von dem Positivteil nichtnegativ sind, da der Abstand von $x$ zu den beiden Gitterpunkten $x_{\mathbf{i}_1}$ und$ x_{\mathbf{i}_2}$ kleiner gleich $\frac{2a}{M}$ ist. Zudem haben wir verwendet, dass $x_{\mathbf{i}_2} - x_{\mathbf{i}_1} = \frac{2a}{M}$ gilt, da beides Gitterpunkte sind.     
	
\emph{Induktionshypothese} (IH): Aussage~(\ref{induktiongl1}) gelte für ein beliebiges aber festes $d \in \N.$

\emph{Induktionsschritt} (IS): Wir nehmen ohne Beschränkung der Allgemeinheit an, dass $x_{(0,\dots,0)} \leq x \leq x_{(1,\dots,1)}$ komponentenweise gilt. Das heißt also, dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt. Im Folgenden zeigen wir $$\sum_{\bi \in [M]^{(d + 1)}} B_{\bi}(x) = \sum_{\bi \in [M]^{(d + 1)}}\prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot \big|x^{(j)} - x_{\mathbf{i}}^{(j)}\big|\bigg)_+ = 1.$$
Ein Summand ist Null, wenn ein $j \in \{1,\dots,d+1\}$ existiert mit $\big|x^{(j)} - x_{\mathbf{i}}^{(j)}\big| \geq \frac{2a}{M}$. Zudem haben wir ohne Beschränkung der Allgemeinheit angenommen dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt, damit haben wir also nur noch $2^{d + 1}$ Summanden, was der Anzahl der Gitterpunkte die am nächsten bei $x$ liegen entspricht. Zudem wissen wir, dass alle Gitterpunkte, die in der $(d + 1)$-ten Komponente den selben Wert haben, in dieser Dimension gleich weit von $x^{(d + 1)}$ entfernt sind. Das heißt, in jedem Summanden kommt der Faktor $(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}\big|)$ bzw.\@ $(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}\big|)$ vor, da 
\begin{equation*}
\bigg(1 - \frac{M}{2a} \cdot \Big|x^{(d + 1)} - x_\mathbf{i}^{(d + 1)}\Big|\bigg) = \begin{cases}
(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}\big|) &\text{$\mathbf{i} \in \{0, 1\}^d \times \{0\}$}\\
(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|) &\text{$\mathbf{i} \in \{0, 1\}^d \times \{1\}$}
\end{cases}
\end{equation*}
gilt. Daraus ergibt sich:
\begin{equation*}
\begin{split}
\sum_{\bi \in [M]^{(d + 1)}} & \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& = \sum_{\mathbf{i} \in \{0, 1\}^{d + 1}} \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg) \\
& = \Bigg(\sum_{\mathbf{i} \in \{0, 1\}^d \times \{0\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|\bigg) \\
& \qquad + \Bigg(\sum_{\mathbf{i} \in \{0, 1\}^d \times \{1\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|\bigg) \\
& \stackrel{(\text{IV})}{=} 1 \cdot \bigg(1 - \frac{M}{2a} \cdot \Big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}\Big|\bigg) + 1 \cdot \bigg(1 - \frac{M}{2a} \cdot \Big|x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}\Big|\bigg) \\
& = 1 + 1 - \frac{M}{2a} \cdot \Big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)} + x_{(1,\dots,1)}^{(d + 1)} - x^{(d + 1)}\Big| \\
& = 1 + 1 - 1 \\
& = 1,
\end{split}
\end{equation*}
wobei wir bei der vorletzten Gleichung angewendet haben, dass $x_{(1,\dots,1)}^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)} = \frac{2a}{M}$ ist, da beides Gitterpunkte sind.  $\hfill(\square)$ 		

\emph{\textbf{ii)}} Es folgt $\prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \geq 0$ für alle $\bi \in [M]^d$, da $$z_+= \max\{z, 0\} \geq 0 \text{ für $z \in \R$}$$ gilt. Damit wäre die Nichtnegativität der Koeffizienten der Linearkombination gezeigt. Damit ist jeder Summand in $$\sum_{\bi \in [M]^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$ größer gleich Null und wegen der Zerlegung der Eins aus Gleichung~(\ref{induktiongl1}) gilt dann auch 
\begin{equation}
\label{induktiongl2}
\prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \leq 1.
\end{equation}

\emph{\textbf{iii)}} Es handelt sich hierbei um einen lokalen Träger, da nach der Konstruktion von $B_{\bi}(x)$ der Funktionswert genau dann Null ist, wenn ein $j \in \{1,\dots, d\}$ existiert, sodass $|x^{(j)} - x_{\bi}^{(j)}| \geq \frac{2a}{M}$ gilt. Andernfalls erhalten wir mit Bedingung~\textbf{ii)}, dass $B_{\bi}(x) > 0$ ist. 
\end{proof}

Hiermit erhalten wir:
\begin{equation*}
\begin{split}
P_m(x) &= \sum_{\bi \in [M]^d} p_{\bi}^m(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& = \sum_{\bi \in [M]^d} p_{\bi}(x) \cdot B_{\bi}(x).
\end{split}
\end{equation*}
Damit wissen wir, dass $P_m(x)$ eine Spline Interpolation von Taylorpolynomen von $m$ ist.
Die Wahl der Netzwerkarchitektur und der Werte aller Gewichte bis auf die der Ausgabeschicht ist durch folgendes Approximationsresultat durch eine lokale Spline Interpolation von Taylorpolynomen für $(p,C)$-glatte Funktionen auf dem Intervall $[-a, a]^d$ motiviert.
\begin{lem}
\label{lem:pcsmooth}
Sei $M \in \N$, $c > 0$, $a > 0$ und $f$ eine ($p, C$)-glatte Funktion, wobei $p = q + s$ mit $q \in \N_0, s \in (0,1]$ und $C > 0$ sind. Sei zudem $P_f(x)$ analog wie in (\ref{konvexkomb}) eine lokale Spline Interpolation von Taylorpolynomen von $f$. Dann gilt$\colon$
$$\sup_{x \in [-a, a]^d} |f(x) - P_f(x)|  \leq c \cdot \bigg(\frac{a}{M}\bigg)^p.$$
\end{lem}
\begin{proof}
Nach Lemma~\ref{lem:lagrange} über die Lagrange Form des Restglieds existiert ein $\xi \in [0, 1],$ so, dass 
\begin{equation}
\label{eq:lagrange}
\begin{split}
f(x) & = T_{x_{\mathbf{i}},q - 1}[f(x)] \\
& = \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q - 1}}  \partial^{\bj}f(x_{\mathbf{i}}) \cdot \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} + \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!}.
\end{split}
\end{equation}
Nach der Basispline Eigenschaft aus Gleichung~(\ref{induktiongl1}) erhalten wir 
$$f(x) = \sum_{\bi \in [M]^d} f(x) \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+.$$ Zudem wissen wir, dass man immer den Abstand zu den nächsten $2^d$ Gitterpunkten betrachtet, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}}^{(j)}$ größer als $\frac{2a}{M}$ ist. Daher ergibt sich:
\begin{equation}
\label{eq:absch}
\begin{split}
& \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} \leq \sum_{\substack{ \bj \in [q]^d \\ |\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{1}{\bj!} \cdot \bigg(\frac{2a}{M}\bigg)^q.
\end{split}
\end{equation} 
Mithilfe der Dreiecksungleichung und der Definition von $P_f(x)$ erhalten wir:
\begin{equation}
\label{eq:drei}
\begin{split}
& |f(x) - P_f(x)| \\
& \leq \sum_{\bi \in [M]^d} \bigg|f(x) - \sum_{\substack{\mathbf{j} \in [q]^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}f(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}\bigg| \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+.
\end{split}
\end{equation}
Nach Gleichung~(\ref{eq:lagrange}) erhalten wir:
\begin{equation}
\label{eq:sum}
\begin{split}
& \bigg|f(x) - \sum_{\substack{\mathbf{j} \in [q]^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}f(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}\bigg| \\ 
& = \bigg| \sum_{\substack{\bj \in [q]^d \\ |\bj|_1 \leq q - 1}}  \partial^{\bj}f(x_{\mathbf{i}}) \cdot \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} + \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} \\
& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad - \sum_{\substack{\mathbf{j} \in [q]^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}f(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}\bigg| \\
& = \bigg| \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} - \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi}) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!}\bigg|.
\end{split}
\end{equation}
Aus der $(p,C)$-Glattheit von $f$ nach Definition~\ref{def:pc} mit $x_{\bi} + \xi(x - x_{\mathbf{i}}), x_{\bi} \in \R^d$ folgt durch (\ref{eq:absch}) und (\ref{eq:sum}):
\begin{equation}
\label{eq:last}
\begin{split}
& \bigg| \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} - \sum_{\substack{ \bj \in [q]^d \\|\bj|_1 = q}} \partial^{\bj}f(x_{\bi}) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!}\bigg| \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \|x_{\bi} + \xi(x - x_{\mathbf{i}}) - x_{\bi} \|^s \cdot C.
\end{split}
\end{equation}
Fassen wir die Gleichungen~(\ref{eq:drei}), (\ref{eq:sum}) und (\ref{eq:last}) zusammen, erhalten wir:
\begin{equation*}
\begin{split}
|f(x) - P_f(x)| & \leq \sum_{\bi \in [M]^d} \bigg|f(x) - \sum_{\substack{\mathbf{j} \in [q]^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}f(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}\bigg| \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \|x_{\bi} + \xi(x - x_{\mathbf{i}}) - x_{\bi} \|^s \cdot C \cdot \sum_{\bi \in [M]^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \cdot \bigg(\frac{2a}{M}\bigg)^s \cdot d^{s/2} \cdot C\\
& = c \cdot \bigg(\frac{a}{M}\bigg)^p,
\end{split}
\end{equation*}
wobei wir bei der letzten Ungleichung die Eingenschaft der Zerlegung der Eins aus Gleichung~(\ref{induktiongl1}) und $q + s = p$ verwendet haben, mit 
$$c = 2^p \cdot d^{s/2} \cdot C.$$
\end{proof}

Durch geeignet gewählte $a_{\bi, \bj} \in \R$ lässt sich $P_m(x)$ in die Form 
$$\sum_{\bi \in [M]^d} \sum_{\substack{ \bj \in [q]^d\\|\bj|_1 \leq q}} a_{\bi, \bj} \cdot (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$
bringen, da sich jedes $p^m_{\bi}(x)$ als Polynom umordnen lässt und wir daher auch $P_m(x)$ umschreiben können.

Als nächstes wollen wir geeignete neuronale Netze $f_{\net, \bj, \bi}$ definieren, die die Funktionen
$$x \mapsto (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$ approximieren. Zudem möchten wir die Netzwerkarchitektur so wählen, dass neuronale Netze der Form
$$\sum_{\bi \in [M]^d} \sum_{\substack{ \bj \in [q]^d\\|\bj|_1 \leq q}} a_{\mathbf{i}, \bj} \cdot f_{\net,\bj,\mathbf{i}}(x) \qquad (a_{\mathbf{i},\bj} \in \R)$$ in ihr enthalten sind.
Um dies zu erreichen, sei $$\sigma(x) = \frac{1}{(1 + \exp(-x))} \quad (x \in \R)$$ der logistische Squasher~(\ref{logsquasher}), wählen $R \geq 1$ und definieren die folgenden neuronale Netze:

Das neuronale Netz
\begin{equation}
\label{def:fid}
f_{\id}(x) = 4R \cdot \sigma\Big(\frac{x}{R}\Big) - 2R,
\end{equation}
welches, wie in Lemma~\ref{lem:1} gezeigt, die Funktion $f(x) = x$ approximiert und in Abbildung~\ref{fig:fid} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
& & \\
};
\foreach \ai [count=\mi ]in {2}
  \draw[<-] (mat-\ai-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {2}
{\foreach \aii in {2}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2}
  \draw[->] (mat-\ai-2) -- (mat-2-3);
\draw[->] (mat-2-3) -- node[above] {$f_{\id}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\id}(x)$}
\label{fig:fid}
\end{figure}

Das neuronale Netz 
\begin{equation}
\label{def:fmult}
\begin{split}
f_{\mult}(x, y) = \frac{R^2}{4} \cdot \frac{(1 + \exp(- 1))^3}{\exp(-2) - \exp(-1)} \cdot & \bigg(\sigma\Big(\frac{2(x + y)}{R} + 1\Big) - 2 \cdot \sigma \Big(\frac{x + y}{R} + 1\Big) \\
& - \sigma\Big(\frac{2(x - y)}{R} + 1\Big) + 2 \cdot \sigma\Big(\frac{x - y}{R} + 1\Big)\bigg),
\end{split}
\end{equation}
welches, wie in Lemma~\ref{lem:2} gezeigt, die Funktion $f(x, y) = x \cdot y$ approximiert und in Abbildung~\ref{fig:fmult} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| \\
& & |[plain]| \\
|[plain]| & |[plain]| & \\
& & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| \\
|[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-4-1) -- node[above] {$x$} +(-1cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$y$} +(-1cm,0);
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-5-3);
\draw[->] (mat-5-3) -- node[above] {$f_{\mult}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mult}(x, y)$}
\label{fig:fmult}
\end{figure}

Das neuronale Netz 
\begin{equation}
\label{def:frelu}
f_{ReLu}(x) = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)),
\end{equation}
welches, wie in Lemma~\ref{lem:3} gezeigt, die Funktion $f(x) = x_+$ approximiert und in Abbildung~\ref{fig:frelu} veranschaulicht wird
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-5-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {5}
{\foreach \aii in {4,6}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-3) -- (mat-5-4);
\draw[->] (mat-5-4) -- node[above] {$f_{\ReLU}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\ReLU}(x)$}
\label{fig:frelu}
\end{figure}
und schließlich das neuronale Netz 
\begin{equation}
\label{def:fhat}
f_{\mathrm{hat},y}(x) = f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) +  f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg),
\end{equation}
welches, wie in Lemma~\ref{lem:4} gezeigt, für fixes $y \in \R$ die Funktion $$f(x) = \bigg(1 - \bigg(\frac{M}{2a}\bigg) \cdot |x - y|\bigg)_+$$ approximiert und in Abbildung~\ref{fig:fhat} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-14-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {14}
{\foreach \aii in {4,6,13,15,22,24}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {13,15}
{\foreach \aii in {11,13,15,17}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {22,24}
{\foreach \aii in {20,22,24,26}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8,11,13,15,17,20,22,24,26}
  \draw[->] (mat-\ai-3) -- (mat-14-4);
\draw[->] (mat-14-4) -- node[above] {$f_{\mathrm{\mathrm{hat}},y}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mathrm{\mathrm{hat}},y}(x)$}
\label{fig:fhat}
\end{figure} 

Mit diesen neuronalen Netzen können wir nun $f_{\net,\bj,\bi}$ rekursiv definieren. Dafür wählen wir $N > q$, setzen $s = \lceil\log_2(N + d)\rceil$ und definieren für $\bj \in [N]^d$, $\bi \in [M]^d$ und $k \in \{1,\dots,(M + 1)^d\}$:  
\begin{align}
\label{fnet}
f_{\net,\bj,\mathbf{i}}(x) & = f_1^{(0)}(x). \nonumber
\intertext{wobei} \nonumber
f_k^{(l)}(x) & = f_{\mult}\Big(f_{2k - 1}^{(l + 1)}(x),f_{2k}^{(l + 1)}(x)\Big) \nonumber
\intertext{für $k \in \{1, 2, \dots, 2^l \}$ und $l \in\{0,\dots,s - 1\}, $ und}
f_k^{(s)}(x) & = f_{\id}(f_{\id}(x^{(l)} - x_{\mathbf{i}_k}^{(l)}))
\intertext{für $j_1 + j_2 + \dots + j_{l-1} + 1 \leq k \leq j_1 + j_2 + \dots + j_l$ und $1 \leq l \leq d$ und}
f_{|\bj|_1+ k}^{(s)}(x) & = f_{\mathrm{hat}, x_{\mathbf{i}_k}^{(k)}}(x^{(k)}) \nonumber
\intertext{für $1 \leq k \leq d$ und}
f_k^{(s)}(x) & = 1 \nonumber
\end{align} 
für $|\bj|_1 + d + 1 \leq k \leq 2^s.$
 
Da das neuronale Netz $f_{\net,\bj,\bi}$ aus mehrere neuronalen Netzen zusammengebaut wurde, lässt sich dadurch auch die Anzahl an Schichten und Neuronen pro Schicht durch diese Struktur erklären. Aus der rekursiven Definition~(\ref{fnet}) entnimmt man, dass $f_{\net,\bj,\bi}$ $s + 2$ verborgene Schichten, durch $s$-maliges Anwenden von $f_{\mult}$ und einer Anwendung von $f_{\mathrm{hat}}$ bzw.\@ $f_{\id}(f_{\id})$, hat. Da $f_{\mathrm{hat}}$ zwei verborgene Schichten besitzt, ergibt sich daraus die Anzahl an verborgenen Schichten von $f_{\net,\bj,\bi}$.
%Für die Anzahl an Neuronen für die jeweiligen Schichten können wir nur eine oberen Schranke angeben, da ... (TBD)
Die Anzahl der Neuronen pro verborgener Schicht von $f_{\net,\bj,\bi}$ ergeben sich wie folgt:
\begin{itemize}
\item Die erste verborgene Schicht enthält maximal $3 \cdot 2 \cdot 2^s = 6 \cdot 2^s$ Neuronen, da dies die erste verborgene Schicht von $f_{\mathrm{hat}}$ ist und maximal $2^s$ mal aufgerufen wird. 
\item Die zweite verborgene Schicht maximal $3 \cdot 4 \cdot 2^s = 12\cdot 2^s$ Neuronen, da dies die zweite verborgene Schicht von $f_{\mathrm{hat}}$ ist und maximal $2^s$-mal aufgerufen wird.
\item Die verborgenen Schichten $3,\dots,s + 2$ enthalten maximal   $2^{s+ 1}, 2^s, \dots, 2^3, 2^2$ Neuronen, da wir $s$-mal $f_{\mult}$ ineinander geschachtelt aufrufen. 
\end{itemize}  
Da man bei fully connected neuronalen Netzen die Gewichte der Verbindungen zwischen zwei Neuronen auf Null setzen kann, können auch nicht fully connected neuronale Netze in dieser Klasse enthalten sein. Daher liegt auch $f_{\net,\bj,\bi}$ in der der Klasse aller fully connected neuronaler Netze, mit $s + 2$ verbogenen Schichten mit jeweils $24 \cdot (N + d)$ Neuronen pro Schicht, da die größte Anzahl an Neuronen in einer Schicht $$12 \cdot 2^s = 12 \cdot 2^{\lceil\log_2(N + d)\rceil} \leq 12 \cdot 2^{\log_2(N + d) + 1} = 24 \cdot (N + d)$$ ist. Weiterhin erkennt man durch die Zusammensetzung der neuronalen Netze, dass alle Gewichte im Betrag durch $c \cdot \max\{\frac{M}{2a}, R^2\}$ beschränkt sind, wobei $c > 0$ ist. 

\section{Definition der Gewichte der Ausgabeschicht}
\label{subsec:2.2}

Wir definieren unseren Neuronale-Netze-Regressionsschätzer $\tilde{m}_n(x)$  durch:
\begin{equation}
\label{estimate}
\tilde{m}_n(x) = \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj} \cdot f_{\net,\bj,\mathbf{i}}(x),
\end{equation}
wobei $n$ die Größe unserer gegebenen Datenmenge~(\ref{dataset}) ist und wir die Koeffizienten $a_{\mathbf{i},\bj}$ durch Minimierung des Funktionals 
\begin{equation}
\label{min} 
\begin{split}
\varphi(a_{\bi,\bj}) & \coloneqq \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \lambda\|(a_{\bi,\bj})\|_2^2 \\
& =\frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2
\end{split}
\end{equation}
für Regularitätsterm $\lambda\|(a_{\bi,\bj})\|_2^2$ mit einer von $n$ unabhängigen Konstante $c > 0$ erhalten. Diese regularisierte lineare Kleinste-Quadrate Schätzung erhalten wir durch die Lösung des, in Gleichung~(\ref{les}) folgenden, linearen Gleichungssystems. Dafür definieren wir uns die Menge 
\begin{align*} 
\{U_s : s = 1,\dots,S\} = \Bigl\{f_{\net,\bj,\bi}(x) : \bi \in [M]^d \text{ und } |\bj|_1 \leq N \, \text{ mit }\, \bj \in [N]^d \Bigr\}
\end{align*}
wobei
$$ S = \big|[M]^d\big| \cdot  \binom{N + d}{d} = (M + 1)^d \cdot \binom{N + d}{d}$$ die Kardinalität der Menge ist .
Diese Kardinalität erhalten wir mit einem Kombinatorik Argument.
Wir wissen, dass es insgesamt $(M + 1)^d$ Möglichkeiten gibt $d$ viele Zahlen aus einer Menge mit der Größe $(M + 1)$ zu ziehen mit Zurücklegen und da wir Vektoren betrachten und die Komponenten nicht vertauschbar sind, ist auch die Reihenfolge der Ziehung zu beachten.
Für jede dieser $(M + 1)^d$ Möglichkeiten ist noch zu beachten, dass wir zusätzlich $d$ mal aus einer Menge mit $(N + 1)$ vielen Zahlen ziehen müssen und gleichzeitig die Bedingung dass die Summe der gezogen $d$ Elemente zwischen Null und $N$ liegt.
Gesucht ist also 
$$\bigg|\Bigl\{\bj \in [N]^d : |\bj|_1 \leq N \Bigr\}\bigg| \eqqcolon H.$$ 
Wir stellen fest, dass:
\begin{equation*}
\begin{split}
& \Bigl\{\bj \in [N]^d : |\bj|_1 \leq N \Bigr\} \\
& = \Bigl\{\bj \in [N]^d : |\bj|_1 = 0 \Bigr\}
 \cup \Bigl\{\bj \in [N]^d : |\bj|_1 = 1 \Bigr\}
 \cup \dots 
 \cup \Bigl\{\bj \in [N]^d : |\bj|_1 = N\Bigr\}
\end{split}
\end{equation*}
gilt. Mit Lemma~\ref{lem:kombi} wissen wir, dass für $d, N \in \N$ und $k \in \N_0$:
$$\big|\Bigl\{\bj \in [N]^d : |\bj|_1 = k \Bigr\}\big| = \binom{d + k - 1}{k}$$ gilt.
Damit erhalten wir:
$$|H| = \sum_{k = 0}^N \binom{N - 1 + k}{k} = \binom{N + d}{d},$$
mit der \emph{Hockey-Stick Identität}
$$\binom{n}{k} = \sum_{i = 0}^k \binom{n - k - 1 + i}{i} \quad (k, n \in \N  \text{ mit  } k < n).$$
Wir setzen nun 
$$ \mathbf{U} = (U_s(X_i))_{1\leq i \leq n,1\leq s \leq S} \quad \text{und} \quad \mathbf{Y} = (Y_i)_{i = 1,\dots,n}.$$
Im folgenden Lemma bestimmen wir den Koeffizientenvektor unseres Schätzers $\tilde{m}_n$.
\begin{lem}
Der Koeffizientenvektor unseres Schätzers $\tilde{m}_n$ aus Gleichung~(\ref{estimate}) ist die eindeutige Lösung des linearen Gleichungssystems 
\begin{equation}	
\label{les}
\bigg(\frac{1}{n}\mathbf{U}^T\mathbf{U} + \frac{c}{n} \cdot \mathbf{1} \bigg) \mathbf{a} = \frac{1}{n} \mathbf{U}^T\mathbf{Y}
\end{equation}
Hierbei ist $\mathbf{1}$ eine $S \times S$-Einheitsmatrix und $\ba \in \R^S$, wobei $\ba$ wie in Gleichung~(\ref{min}) den Ausdruck:
\begin{equation}
\label{eq:min}
\begin{split}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2 \\
& = \frac{1}{n}(\mathbf{Y} - \mathbf{U}\mathbf{a})^T(\mathbf{Y} - \mathbf{U}\mathbf{a}) + \frac{c}{n} \cdot \mathbf{a}^T\mathbf{a}
\end{split}
\end{equation}
minimiert.
\end{lem}
\begin{proof}
Der Schätzer aus Gleichung~(\ref{estimate}) lässt sich umschreiben zu 
\begin{equation}
\label{umschreiben}
\tilde{m}_n(x) = \sum_{s = 1}^S a_s \cdot U_s(x)
\end{equation}
mit $(a_s)_{s = 1,\dots,S} = \mathbf{a}\in \R^S$. 

Da $\mathbf{Y}^T\mathbf{U}\mathbf{a} = \mathbf{a}^T\mathbf{U}^T\mathbf{Y}$ gilt, da dieser Ausdruck eine reelle Zahl und damit insbesondere symmetrisch ist erhalten wir mit Gleichung~(\ref{eq:min}):
\begin{equation}
\label{eq:matrix}
\begin{split}
& \frac{1}{n}(\mathbf{Y} - \mathbf{U}\mathbf{a})^T(\mathbf{Y} - \mathbf{U}\mathbf{a}) + \frac{c}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - \mathbf{Y}^T\mathbf{U}\mathbf{a} - \mathbf{a}^T\mathbf{U}^T\mathbf{Y} + \mathbf{a}^T\mathbf{U}^T\mathbf{U}\mathbf{a}) + \frac{c}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{U}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{U}^T\mathbf{U} + \frac{c}{n} \cdot \mathbf{1}\bigg) \mathbf{a}.
\end{split}
\end{equation} 
Die Matrix $\mathbf{U}^T\mathbf{U} \in \R^{S \times S}$ ist positiv semidefinit, denn aufgrund der Verschiebungseigenschaft des Standardskalarprodukts gilt für alle $x \in \R$:
$$\langle x, \mathbf{U^T}\mathbf{U} x\rangle = \langle \mathbf{U} x, \mathbf{U} x\rangle \geq 0.$$
Zudem wissen wir dass $\frac{c}{n}\mathbf{1}$ durch die Wahl von $c$ nur positive Eigenwerte besitzt und damit positiv definit ist.  
Daher wissen wir, dass die Matrix
$$\mathbf{A} \coloneqq \frac{1}{n}\mathbf{U}^T\mathbf{U} + \frac{c}{n} \cdot \mathbf{1}$$ also Summe einer positiv semidefiniten und einer positiv definiten Matrix nur positive Eigenwerte besitzt (REFERENZ), damit also positiv definit und insbesondere invertierbar ist. Zudem ist die Matrix $\mathbf{A}$ symmetrisch. 
Mit $$\mathbf{b} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{U}^T\mathbf{Y} \in \R^S$$ und $$\mathbf{b}^T\mathbf{A}\mathbf{a} = \mathbf{a}^T\mathbf{A}\mathbf{b} = \mathbf{Y}^T\mathbf{U}\mathbf{a},$$ was aus der Symmetrie von $\mathbf{A}$ folgt, erhalten wir in Gleichungs~(\ref{eq:matrix}):
\begin{equation*}
\begin{split}
& \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{U}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{U}^T\mathbf{U} + \frac{c}{n} \cdot \mathbf{1}\bigg) \mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{U}\mathbf{a}) + \mathbf{a}^T \bA \mathbf{a} \\
& = \frac{1}{n}\bY^T\bY - \frac{1}{n}\bb^T\bA\ba - \frac{1}{n}\ba^T\bA\bb + \ba^T\bA\ba \\
& = \frac{1}{n}\bY^T\bY - \frac{1}{n}\bb^T\bA\ba - \frac{1}{n}\ba^T\bA\bb + \frac{1}{n}\bb^T\bU^T\bY - \frac{1}{n}\bY^T\bU\bb + \ba^T\bA\ba \\
& = \frac{1}{n}\bY^T\bY - \frac{1}{n}\bb^T\bA\ba - \frac{1}{n}\ba^T\bA\bb + \frac{1}{n}\bb^T\bA\bA^{-1}\bU^T\bY - \frac{1}{n}\bY^T\bU\bb + \ba^T\bA\ba \\
& = \frac{1}{n}\bY^T\bY - \frac{1}{n}\bb^T\bA\ba - \frac{1}{n}\ba^T\bA\bb + \frac{1}{n}\bb^T\bA\bb - \frac{1}{n^2}\bY^T\bU\bA^{-1}\bU^T\bY + \ba^T\bA\ba \\
& = \mathbf{a}^T\mathbf{A}\mathbf{a} - \frac{1}{n}\mathbf{b}^T\mathbf{A}\mathbf{a} - \frac{1}{n}\mathbf{a}^T\mathbf{A}\mathbf{b} + \mathbf{b}^T\mathbf{A}\mathbf{b} + \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{U}\mathbf{A}^{-1}\mathbf{U}^T\mathbf{Y} \\
& = (\mathbf{a} - \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{U}^T\mathbf{Y})^T \mathbf{A} (\mathbf{a} - \frac{1}{n} \cdot \mathbf{A}^{-1} \mathbf{U}^T\mathbf{Y}) - \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{U}\mathbf{A}^{-1}\mathbf{U}^T\mathbf{Y}.
\end{split} 
\end{equation*} 
Die letzte Gleichung wird für $\mathbf{a} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{U}^T\mathbf{Y}$ minimal, 
da wir wissen dass $\mathbf{A}$ positiv definit ist und damit $x^T\mathbf{A}x > 0$ für alle $x \in \R^S$ mit $x \neq 0$ gilt und $(\mathbf{a} - \mathbf{b})^T\mathbf{A}(\mathbf{a} - \mathbf{b}) = 0$ genau dann, wenn $\mathbf{a} = \mathbf{b}$ gilt.
Dies zeigt also, dass der Koeffizientenvektor unseres Schätzers~(\ref{estimate}) die eindeutige Lösung des linearen Gleichungssystems~(\ref{les}) ist.
\end{proof}
\begin{bemnumber}
\label{mtildebeschraenkt}
Da der Koeffizientenvektor $\mathbf{a}$ die Gleichung~(\ref{eq:min}) minimiert, erhalten wir, wenn wir den Koeffizientenvektor gleich Null setzen:
$$\frac{1}{n}(\mathbf{Y} - \mathbf{U}\mathbf{a})^T(\mathbf{Y} - \mathbf{U}\mathbf{a}) + \frac{c}{n} \cdot \mathbf{a}^T\mathbf{a} \leq \frac{1}{n} \sum_{i = 1}^n Y_i^2,$$
was uns erlaubt eine obere Schranke für den absoluten Wert unserer Koeffizienten abzuleiten. Daraus können wir folgern, dass unser Neuronale-Netze-Regressionsschätzer $\tilde{m}_n$ beschränkt ist, da $f_{\net,\bj,\bi}$ nach Konstruktion ebenfalls beschränkt ist.
\end{bemnumber}