\chapter{Konstruktion des Neuronale Netze Schätzers}
\label{chap:2}

In diesem Kapitel werden wir mithilfe von unseren gegebenen Datenmenge 
$$ \mathcal{D}_n = \{(X_1, Y_1),\dots,(X_n, Y_n)\},$$
unseren Regressionsschätzer konstruieren. 

Die Netzwerkarchitektur $(L, k)$ hängt von einer positiven ganzen Zahl $L$, die der Anzahl der verborgenen Schichten ist und einem Vektor $k = (k_1,\dots,k_L) \in \N^L$, der mit jeder Komponente die Anzahl der Neuronen in der jeweiligen verborgen Schichte angibt. 

Ein mehrschichtiges feedforward neuronales Netz mit Architektur $(L, k)$ und dem logistischen squasher (\ref{logsquasher}) als Aktivierungsfunktion, ist eine reelwertige Funktion $f\colon \R^d \to \R$ definiert durch:
\begin{align}
\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{für $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ rekursiv definiert durch$\colon$} 
f_i^{(r)}(x) & = \sigma \bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg)\\
\intertext{für $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R (r = 2,\dots, L)$ und$\colon$}
f_i^{(1)}(x) & = \sigma \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber
\end{align} 
für $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$.

Bei neuronale Netze Regressionsschätzer wir für die Ausgabeschicht keine Aktivierungsfunktion gewählt, da wir Funktionswerte schätzen möchten und es sich hier nicht um ein Klassifizierungssproblem handelt.

Für die Konstruktion unseres Schätzers verwenden wir die gegebene Datenmenge $\mathcal{D}_n$ und wählen die Gewichte des neuronalen Netzes so, dass die resultierende Funktion aus (\ref{networkarch}) eine gute Schätzungen für die Regressionsfunktion ist. Dafür wählen wir die Gewichte bis auf die in der Ausgabeschicht fest und schätzen die Gewichte in der Ausgabeschicht, indem wir mit unserer Datenmenge ein regularisiertes Kleinste-Quadrate-Problem (REFERENZ) lösen.

\section{Definition der Netzwerkarchitektur}
Sei $a > 0$ fest und $M \in \N$. Die Wahl der Netzwerkarchitektur und der Werte aller Gewichte bis auf die aus der Ausgabeschicht ist durch folgendes Approximationsresultat durch eine lokale Konvexkombination von Taylorpolynomen für $(p,C)$-glatte Funktionen auf dem Intervall $[-a, a]^d$ motiviert.
Sei $[M]^d \coloneqq\{0, 1, \dots, M\}^d = \{\mathbf{i}_1,\dots, \mathbf{i}_{(M + 1)^d}\}.$ 

Für $(\mathbf{i}^{(1)},\dots,\mathbf{i}^{(d)}) = \bi \in [M]^d$ und $x \in \R^d$ definieren wir
$$|\mathbf{i}|_1 \coloneqq \sum_{k= 1}^d \mathbf{i}^{(k)} \quad \mathbf{i}! \coloneqq \mathbf{i}^{(1)}! \cdots \, \mathbf{i}^{(d)}! \quad x^{\mathbf{i}} \coloneqq x_1^{\mathbf{i}^{(1)}} \cdots \,    x_d^{\mathbf{i}^{(d)}}.$$
Für $f\colon \R^d \to \R$ ausreichend oft differenzierbar definieren wir 
$$\partial^{\mathbf{i}}f(x) \coloneqq \frac{\partial^{|\mathbf{i}|_1}f}{\partial x_1^{\mathbf{i}^{(1)}} \cdots \, \partial x_d^{\mathbf{i}^{(d)}}} (x).$$
Wir betrachten im Folgenden ein $d$-dimensionales äquidistantes Gitter im Würfel $[-a, a]^d$ mit Schrittweite $\frac{2a}{M}.$ Die Gitterpunkte haben für $\bi \in [M]^d$ die Gestalt:
$$x_{\bi} = \bigg( -a + \bi^{(1)} \cdot \frac{2a}{M},\dots, -a + \bi^{(d)} \cdot \frac{2a}{M}\bigg) = -\mathbf{a} + \frac{2a}{M} \cdot \bi,$$
mit $\mathbf{a} = (a, a, \dots, a) \in \R^d$.
Hiermit lässt sich das Taylorpolynom von $m$ der Ordnung $q$ im Entwicklungspunkt $x_{\bi}$ schreiben als
$$p_{\bi}(x) = \sum_{\substack{\mathbf{j} \in \{0,\dots,q\}^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}m(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}$$
und sei
\begin{equation}
\label{konvexkomb}
P(x) = \sum_{\bi \in [M]^d} p_{\bi}(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+.
\end{equation}
Wir zeigen im folgenden Lemma dass $P(x)$ eine lokale Spline Interpolation von Taylorpolynomen von m ist.
\begin{lem}[Multilineare Spline Interpolation]
\label{lem:loccon}
Sei $a >0, M \in \N$ und $$P(x) = \sum_{\bi \in [M]^d} p_{\bi}(x) \cdot B_{\bi}(x).$$ mit $p_{\mathbf{i}}(x)$ wie oben und 
$$B_{\bi}(x) = \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+,$$
dann ist $P(x)$ eine lokale Spline Interpolation von Taylorpolynomen von m.
\end{lem}
\begin{proof}
Es sind drei Bedingungen zu überprüfen. Als erstes geben wir aber für $d = 2$ und $M = 3$ eine Skizze an um die Idee des Beweises zu veranschaulichen. 
\begin{figure}[htp]
\centering
\begin{tikzpicture} 
   %Raster zeichnen 
   %\draw [color=gray!50]  [step=20mm] (-3,-3) grid (4,4); 
   \draw [color=gray!50] (0,0) -- (6,0) -- (6,6) -- (0,6) -- (0,0);			
   \draw [color=gray!50] (0,2) -- (6,2);
   \draw [color=gray!50] (0,4) -- (6,4);
        \draw [color=gray!50] (2,6) -- (2,0);
                \draw [color=gray!50] (4,6) -- (4,0);

	\fill[black] (0,0) circle (0.08cm) node[label=below:{$(-a, -a)$}]{};
		\fill[black] (6,0) circle (0.08cm) node[label=below:{$(a, -a)$}]{};
				\fill[black] (0,6) circle (0.08cm) node[label=above:{$(-a, a)$}]{};
						\fill[black] (6,6) circle (0.08cm) node[label=above:{$(a, a)$}]{};
						\fill[black] (0,0) circle (0.00cm) node[label=left:{$x_{\mathbf{i}_1}$}]{};
						\fill[black] (2,0) circle (0.00cm) node[label=below:{$x_{\mathbf{i}_2}$}]{};
						\fill[black] (4,0) circle (0.00cm) node[label=below:{$x_{\mathbf{i}_3}$}]{};
						\fill[black] (6,0) circle (0.00cm) node[label=right:{$x_{\mathbf{i}_4}$}]{};

						\fill[black] (9,6) circle (0.00cm) node[label=right:{$\mathbf{i}_1 = (0, 0)$}]{};			
						\fill[black] (9,5.5) circle (0.00cm) node[label=right:{$\mathbf{i}_2 = (1, 0)$}]{};					
						\fill[black] (9,5) circle (0.00cm) node[label=right:{$\mathbf{i}_3 = (2, 0)$}]{};	
						\fill[black] (9,4.5) circle (0.00cm) node[label=right:{$\mathbf{i}_4 = (3, 0)$}]{};	
						\draw [dotted, ultra thick] (10,4) -- (10,3.65);
						\fill[black] (9,3.15) circle (0.00cm) node[label=right:{$\mathbf{i}_{16} = (3, 3)$}]{};	
						
						\fill[black] (0,2) circle (0.08cm);
						\fill[black] (0,4) circle (0.08cm); 
						\fill[black] (2,2) circle (0.08cm);
						\fill[black] (2,4) circle (0.08cm);
						\fill[black] (2,6) circle (0.08cm);
						\fill[black] (2,0) circle (0.08cm);
						\fill[black] (0,4) circle (0.08cm);
						\fill[black] (2,4) circle (0.08cm);
						\fill[black] (4,4) circle (0.08cm);
						\fill[black] (4,6) circle (0.08cm);																\fill[black] (4,0) circle (0.08cm);
						\fill[black] (4,2) circle (0.08cm);						
						\fill[black] (6,2) circle (0.08cm);
						\fill[black] (6,4) circle (0.08cm);
\end{tikzpicture}
\caption{Beispielhafte Darstellung der $x_{\mathbf{i}_k}$ für $d = 2$ und $M = 3$.}
\label{fig:gitter}
\end{figure}
Es ist ein Gitter mit $(M + 1)^d$ Gitterpunkten die den $x_{\mathbf{i}_k}$ entsprechen. Der Abstand zwischen zwei Gitterpunkten beträgt $\frac{2a}{M}.$ Man betrachtet immer den Abstand zu den nahesten $2^d$ Gitterpunkten, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}}^{(j)}$ größer als $\frac{2a}{M}$ ist.     

\textbf{•} Im Folgenden wollen wir 
\begin{equation}
\label{induktiongl1}
\sum_{\bi \in [M]^d} B_{\bi}(x) = 1 \quad (x \in \R^d)
\end{equation}
per Induktion über $d$ zeigen. 
	
	 \emph{Induktionsanfang} (IA): Für $d = 1$ kann nur zwischen zwei Gitterpunkten liegen und mit der obigen Begründung ist der Rest gleich Null, daher nehmen wir oBdA an, dass $x_{\mathbf{i}_1}\leq x \leq x_{\mathbf{i}_2}$. 
	Damit folgt 
	\begin{equation*}
	\begin{split}
	\sum_{\bi \in [M]^d} (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}}|)_+ & = (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_1}|)_+ + (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_2}|)_+ \\
	& = 1 + 1 - \frac{M}{2a} \cdot (x - x_{\mathbf{i}_1} + x_{\mathbf{i}_2} - x) \\
	& = 1 + 1 - \frac{M}{2a} \cdot \frac{2a}{M} \\
	& = 1,
	\end{split}
	\end{equation*} wobei wir unter anderem verwendet haben, dass beide Summanden unabhängig von dem Positivteil nichtnegativ sind, da der Abstand von $x$ zu den beiden Gitterpunkten $x_{\mathbf{i}_1}$ und$ x_{\mathbf{i}_2}$ kleiner gleich $\frac{2a}{M}$ ist. Zudem haben wir verwendet, dass $x_{\mathbf{i}_2} - x_{\mathbf{i}_1} = \frac{2a}{M}$ gilt, da beides Gitterpunkte sind.     
	
\emph{Induktionshypothese} (IH): Aussage (\ref{induktiongl1}) gelte für ein beliebiges aber festes $d \in \N.$

\emph{Induktionsschritt} (IS): Wir nehmen oBdA an, dass $x_{(0,\dots,0)} \leq x \leq x_{(1,\dots,1)}$ gilt, mit $\mathbf{i}_1 = (0,\dots,0)$ und $\mathbf{i}_{(M + 1)^{(d + 1)}} = (1,\dots,1).$ Das heißt also, dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt. Im Folgenden zeigen wir $$\sum_{\bi \in [M]^{(d + 1)}} B_{\bi}(x) = \sum_{\bi \in [M]^{(d + 1)}}\prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot \big|x^{(j)} - x_{\mathbf{i}}^{(j)}\big|\bigg)_+ = 1.$$
Ein Summand ist Null, wenn $\big|x^{(j)} - x_{\mathbf{i}}^{(j)}\big| \geq \frac{2a}{M}$ ist. Zudem haben wir oBdA angenommen dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt, damit haben wir also nur noch $2^{d + 1}$ Summanden, was der Anzahl der Gitterpunkte die am nahesten bei $x$ liegen entspricht. Zudem wissen wir, dass alle Gitterpunkte, die in der $(d + 1)$ Komponente den selben Wert haben, in dieser Dimension gleich weit von $x^{(d + 1)}$ entfernt sind. Das heißt, in jedem Summanden kommt der Faktor $(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}\big|)$ bzw. $(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}\big|)$ vor, da 
\begin{equation*}
\bigg(1 - \frac{M}{2a} \cdot \Big|x^{(d + 1)} - x_\mathbf{i}^{(d + 1)}\Big|\bigg) = \begin{cases}
(1 - \frac{M}{2a} \cdot \big|x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}\big|) &\text{$\mathbf{i} \in \{0, 1\}^d \times \{0\}$}\\
(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|) &\text{$\mathbf{i} \in \{0, 1\}^d \times \{1\}$}
\end{cases}
\end{equation*}
daraus ergibt sich$\colon$
\begin{equation*}
\begin{split}
\sum_{\bi \in [M]^{(d + 1)}} & \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& = \sum_{\mathbf{i} \in \{0, 1\}^{d + 1}} \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg) \\
& = \Bigg(\sum_{\mathbf{i} \in \{0, 1\}^d \times \{0\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|\bigg) \\
& \qquad + \Bigg(\sum_{\mathbf{i} \in \{0, 1\}^d \times \{1\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\bi}^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|\bigg) \\
& \stackrel{(IV)}{=} 1 \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|\bigg) + 1 \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|\bigg) \\
& = 1 + 1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)} + x_{(1,\dots,1)}^{(d + 1)} - x^{(d + 1)}| \\
& = 1 + 1 - 1 \\
& = 1,
\end{split}
\end{equation*}
wobei wir bei der vorletzten Gleichung angewendet haben, dass $x_{(1,\dots,1)}^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)} = \frac{2a}{M}$ ist, da beides Gitterpunkte sind.  $\hfill(\square)$ 		
		
\textbf{•} Es folgt $\prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ \geq 0$ für alle $\bi \in [M]^d$, da $$z_+= \max\{z, 0\} \geq 0 \quad (z \in \R)$$ gilt. Damit wäre die Nichtnegativität der Koeffizienten der Linearkombination gezeigt. Damit ist jeder Summand in $$\sum_{\bi \in [M]^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$ größer gleich Null und wegen (\ref{induktiongl1}) muss dann auch 
\begin{equation}
\label{induktiongl2}
\prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \leq 1
\end{equation}
gelten.

\textbf{•} Es handelt sich hierbei um eine lokale Konvexität, da die Bedingungen (\ref{induktiongl1}) und (\ref{induktiongl2}) für alle $x \in [-a, a]^d$ gelten.
\end{proof}
Als nächstes zeigen wir ein Resultat für $(p, C)$-glatte Funktionen, welches wir im weiteren Verlauf dieser Arbeit wieder benötigen werden.
\begin{lem}
\label{lem:pcsmooth}
Sei $M \in \N$, $c_1$ eine Konstante, $a > 0$ und $f$ eine ($p, C$)-glatte Funktion, wobei $p = q + s$ mit $q \in \N_0$ und $s \in (0,1]$. Sei zudem $P(x)$ analog wie in (\ref{konvexkomb}) eine lokale Spline Interpolation von Taylorpolynomen von $f$. Dann gilt$\colon$
$$\sup_{x \in [-a, a]^d} |f(x) - P(x)|  \leq c_1 \cdot \frac{1}{M^p}.$$
\end{lem}
\begin{proof}
Nach dem Satz über die Lagrange Form des Restglieds (REFERENZ) existiert ein $\xi \in [0, 1],$ so, dass 
\begin{equation*}
\begin{split}
f(x) & = T_{x_{\mathbf{i}},q - 1}[f(x)] \\
& = \sum_{|\bj|_1 \leq q - 1}  \partial^{\bj}f(x_{\mathbf{i}}) \cdot \frac{ (x - x_{\mathbf{i}}^{\bj} }{\bj!} + \sum_{q - 1 < |\bj|_1 \leq q} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!}.
\end{split}
\end{equation*}
Nach (\ref{induktiongl1}) erhalten wir 
$$f(x) = \sum_{\bi \in [M]^d} f(x) \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+.$$ Zudem wissen wir dass man immer den Abstand zu den nahesten $2^d$ Gitterpunkten betrachtet, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}}^{(j)}$ größer als $\frac{2a}{M}$ ist, daher ergibt sich:
\begin{equation*}
\begin{split}
&\sum_{q - 1 < |\bj|_1 \leq q} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{ (x - x_{\mathbf{i}})^{\bj} }{\bj!} \leq \sum_{q - 1 < |\bj|_1 \leq q} \partial^{\bj}f(x_{\bi} + \xi(x - x_{\bi})) \frac{1}{\bj!} \cdot \bigg(\frac{2a}{M}\bigg)^q
\end{split}
\end{equation*} 
und folgern mithilfe der Dreiecksungleichung und der $(p,C)$-Glattheit von $f$:
\begin{equation*}
\begin{split}
|f(x) - P(x)| & \leq \sum_{\bi \in [M]^d} \bigg|f(x) - \sum_{\substack{\mathbf{j} \in \{0,\dots,q\}^d \\|\mathbf{j}|_1 \leq q}} \partial^{\mathbf{j}}f(x_{\mathbf{i}}) \cdot \frac{(x - x_{\mathbf{i}})^{\mathbf{j}}}{\mathbf{j}!}\bigg| \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \|x_{\bi} - x_{\bi} + \xi(x - x_{\mathbf{i}}) \|^s \cdot C \cdot \sum_{\bi \in [M]^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \cdot d^{s/2} \cdot \bigg(\frac{2a}{M}\bigg)^s \cdot C \cdot \sum_{\bi \in [M]^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+ \\
& = c_1 \cdot \frac{1}{M^p},
\end{split}
\end{equation*}
wobei wir bei der letzten Gleichheit Bedingung (\ref{induktiongl1}) und $q + s = p$ verwendet haben.
\end{proof}

$P(x)$ lässt sich in die Form 
$$\sum_{\bi \in [M]^d} \sum_{\substack{ \bj \in [q]^d\\|\bj|_1 \leq q}} a_{\bi, \bj} \cdot (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$
durch geeignet gewählte $a_{\bi, \bj} \in \R$ bringen.
Als nächstes wollen wir geeignete neuronale Netze $f_{\net, \bj, \bi}$ definieren, die die Funktionen
$$x \mapsto (x - x_{\mathbf{i}})^{\bj} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}}^{(j)}|\bigg)_+$$ approximieren. Zudem möchten wir die Netzwerkarchitektur so wählen, dass neuronale Netze der Form
$$\sum_{\bi \in [M]^d} \bigg(\frac{2a}{M}\bigg)^q a_{\mathbf{i}, \bj} \cdot f_{\net,\bj,\mathbf{i}}(x) \qquad (a_{\mathbf{i},\bj} \in \R)$$ in ihr enthalten sind.
Um dies zu erreichen, sei $$\sigma(x) = \frac{1}{(1 + \exp(-x))} \quad (x \in \R)$$ der logistische Squasher (\ref{logsquasher}), wählen $R \geq 1$ und definieren die folgenden neuronale Netze:

Das neuronale Netz
\begin{equation}
\label{def:fid}
f_{\id}(x) = 4R \cdot \sigma\Big(\frac{x}{R}\Big) - 2R,
\end{equation}
welches, wie in Lemma \ref{lem:1} gezeigt, die Funktion $f(x) = x$ approximiert und in Abbildung \ref{fig:fid} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
& & \\
};
\foreach \ai [count=\mi ]in {2}
  \draw[<-] (mat-\ai-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {2}
{\foreach \aii in {2}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2}
  \draw[->] (mat-\ai-2) -- (mat-2-3);
\draw[->] (mat-2-3) -- node[above] {$f_{\id}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\id}(x) = 4R \cdot \sigma(\frac{x}{R}) - 2R$}
\label{fig:fid}
\end{figure}

Das neuronale Netz 
\begin{equation}
\label{def:fmult}
\begin{split}
f_{\mult}(x, y) = \frac{R^2}{4} \cdot \frac{(1 + \exp(- 1))^3}{\exp(-2) - \exp(-1)} \cdot & \bigg(\sigma\Big(\frac{2(x + y)}{R} + 1\Big) - 2 \cdot \sigma \Big(\frac{x + y}{R} + 1\Big) \\
& - \sigma\Big(\frac{2(x - y)}{R} + 1\Big) + 2 \cdot \sigma\Big(\frac{x - y}{R} + 1\Big)\bigg),
\end{split}
\end{equation}
welches, wie in Lemma \ref{lem:2} gezeigt, die Funktion $f(x, y) = x \cdot y$ approximiert und in Abbildung \ref{fig:fmult} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1cm}{\centering Hidden\\layer} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| \\
& & |[plain]| \\
|[plain]| & |[plain]| & \\
& & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| \\
|[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-4-1) -- node[above] {$x$} +(-1cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$y$} +(-1cm,0);
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-5-3);
\draw[->] (mat-5-3) -- node[above] {$f_{\mult}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mult}(x) = ...$}
\label{fig:fmult}
\end{figure}

Das neuronale Netz 
\begin{equation}
\label{def:frelu}
f_{ReLu}(x) = f_{\mult}(f_{\id}(x), \sigma(R \cdot x)),
\end{equation}
welches, wie in Lemma \ref{lem:3} gezeigt, die Funktion $f(x) = x_+$ approximiert und in Abbildung \ref{fig:frelu} veranschaulicht wird
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-5-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {5}
{\foreach \aii in {4,6}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8}
  \draw[->] (mat-\ai-3) -- (mat-5-4);
\draw[->] (mat-5-4) -- node[above] {$f_{\ReLU}(x, y)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\ReLU}(x) = ...$}
\label{fig:frelu}
\end{figure}
und schließlich noch das neuronale Netz 
\begin{equation}
\label{def:fhat}
f_{\mathrm{hat},y}(x) = f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) +  f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg),
\end{equation}
welches, wie in Lemma \ref{lem:4} gezeigt, für fixes $y \in \R$ die Funktion $$f(x) = \bigg(1 - \bigg(\frac{M}{2a}\bigg) \cdot |x - y|\bigg)_+$$ approximiert und in Abbildung \ref{fig:fhat} veranschaulicht wird.
\begin{figure}[htp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=6pt
    },
  nodes in empty cells,
  column sep=0.6cm,
  row sep=-5pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering Input\\layer} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 1} & |[plain]| \parbox{1.5cm}{\centering Hidden\\layer 2} & |[plain]| \parbox{1cm}{\centering Output\\layer} \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
& |[plain]| & |[plain]| & \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & & & |[plain]| \\
|[plain]| & |[plain]| & |[plain]| & |[plain]| \\
|[plain]| & |[plain]| &  & |[plain]|\\
};
  \draw[<-] (mat-14-1) -- node[above] {$x$} +(-1cm,0);
\foreach \ai in {14}
{\foreach \aii in {4,6,13,15,22,24}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}  
\foreach \ai in {4,6}
{\foreach \aii in {2,4,6,8}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {13,15}
{\foreach \aii in {11,13,15,17}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {22,24}
{\foreach \aii in {20,22,24,26}
  \draw[->] (mat-\ai-2) -- (mat-\aii-3);
}
\foreach \ai in {2,4,6,8,11,13,15,17,20,22,24,26}
  \draw[->] (mat-\ai-3) -- (mat-14-4);
\draw[->] (mat-14-4) -- node[above] {$f_{\mathrm{\mathrm{hat}},y}(x)$} +(2.5cm,0);
\end{tikzpicture}

\caption{Neuronales Netz $f_{\mathrm{\mathrm{hat}},y}(x) = ...$}
\label{fig:fhat}
\end{figure} 
Mit diesen neuronalen Netzen können wir nun $f_{\net,\bj,\bi}$ rekursiv definieren. Dafür wählen wir $N \geq q$, setzen $s = \lceil\log_2(N + d)\rceil$ und definieren für $\bj = j_1,\dots,j_d \in \{0, 1,\dots, N\}$ und mit $\bi \in [M]^d$ und $k \in \{1,\dots,(M + 1)^d\}$:  
\begin{align}
\label{fnet}
f_{\net,\bj,\mathbf{i}}(x) & = f_1^{(0)}(x). \nonumber
\intertext{wobei} \nonumber
f_k^{(l)}(x) & = f_{\mult}\Big(f_{2k - 1}^{(l + 1)}(x),f_{2k}^{(l + 1)}(x)\Big) \nonumber
\intertext{für $k \in \{1, 2, \dots, 2^l \}$ und $l \in\{0,\dots,s - 1\}, $ und}
f_k^{(s)}(x) & = f_{\id}(f_{\id}(x^{(l)} - x_{\mathbf{i}_k}^{(l)})) \nonumber
\intertext{für $j_1 + j_2 + \dots + j_{l-1} + 1 \leq k \leq j_1 + j_2 + \dots + j_l$ und $1 \leq l \leq d$ und}
f_{j_1 + j_2 + \dots + j_d + k}^{(s)}(x) & = f_{\mathrm{hat}, x_{\mathbf{i}_k}^{(k)}}(x^{(k)})
\intertext{für $1 \leq k \leq d$ und}
f_k^{(s)}(x) & = 1 \nonumber
\end{align} 
für $j_1 + j_2 + \dots + j_d + d + 1 \leq k \leq 2^s.$
 
Da das neuronale Netz $f_{\net,\bj,\bi}$ aus mehrere neuronalen Netzen zusammengebaut wurden lässt sich dadurch auch die Anzahl an Schichten und Neuronen pro Schicht durch diese Struktur erklären. Durch (\ref{fnet}) erkennt man, dass $f_{\mathrm{net}}$ $s + 2$ verborgene Schichten, durch $s$-maliges Anwenden von $f_{\mult}$ und einer Anwendung von $f_{\mathrm{hat}}$, hat. Da $f_{\mathrm{hat}}$ zwei verbogene Schichten besitzt ergibt sich daraus die Anzahl an verborgenen Schichten von $f_{\mathrm{net}}$. Für die Anzahl an Neuronen für die jeweiligen Schichten können wir nur eine oberen Schranke angeben, da ... 
Die Anzahl der Neuronen pro verborgener Schicht von $f_{\mathrm{net}}$ ergeben sich wie folgt:
\begin{itemize}
\item Die erste verborgene Schicht enthält maximal $3 \cdot 2 \cdot 2^s = 6 \cdot 2^s$ Neuronen, da dies die erste verborgene Schicht von $f_{\mathrm{hat}}$ ist und maximal $2^s$ mal aufgerufen wird. 
\item Die zweite verborgene Schicht maximal $3 \cdot 4 \cdot 2^s = 12\cdot 2^s$ Neuronen, da dies die zweite verborgene Schicht von $f_{\mathrm{hat}}$ ist und maximal $2^s$-mal aufgerufen wird.
\item Die verborgenen Schichten $3,\dots,s + 2$ enthalten maximal   $2 \cdot 2^s, 2^s, \dots, 8, 4$, da wir $s$-mal $f_{\mult}$ ineinander geschachtelt aufrufen. 
\end{itemize}  
Da man bei fully connencted neuronalen Netzen die Gewichte der Verbindungen zwischen zwei Neuronen auf Null setzen kann, können auch nicht fully connected neuronale Netze in dieser Klasse enthalten sein. Daher liegt auch $f_{\net,\bj,\bi}$ in der der Klasse aller fully connected neuronaler Netze, mit $s + 2$ verbogenen Schichten mit jeweils $24 \cdot (N + d)$ Neuronen pro Schicht, da für die größte Anzahl an Neuronen in einer Schicht $$12 \cdot 2^s = 12 \cdot 2^{\lceil\log_2(N + d)\rceil} \leq 12 \cdot 2^{\log(N + d) + 1} = 24 \cdot (N + d)$$ gilt. DIE GEWICHTE SIND BESCHRÄNKT DURCH ::: TBD

\section{Definition der Gewichte der Ausgabeschicht}

Wir definieren unseren neuronale Netze Regressionsschätzer $\tilde{m}_n(x)$ durch$\colon$
\begin{equation}
\label{estimate}
\tilde{m}_n(x) = \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj} \cdot f_{\net,\bj,\mathbf{i}}(x),
\end{equation}
wobei wir die Koeffizienten $a_{\mathbf{i},\bj}$ durch Minimierung von 
\begin{equation}
\label{min} \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2
\end{equation}
für eine Konstante $c_3 > 0.$ Dieses regularisierte lineare Kleinste-Quadrate Schätzung erhalten wir durch die Lösung eine linearen Gleichungssystems. Dafür definieren wir uns 
\begin{align*} 
\{B_j \mid j = 1,\dots,J\} = \Bigl\{f_{\net,\bj,\bi}(x) \mid \bi \in [M]^d \quad \text{und} \quad 0 \leq |\bj|_1 \leq N \Bigr\}
\end{align*}
wobei
$$ J = (M + 1)^d \cdot \binom{N + d}{d}$$ die Kardinalität der Menge ist.
Dies erhält man durch TBD.
Wir setzen nun 
$$ \mathbf{B} = (B_j(X_i))_{1\leq i \leq n,1\leq j \leq J} \quad \text{und} \quad \mathbf{Y} = (Y_i)_{i = 1,\dots,n}.$$
Wir zeigen im Folgenden, dass der Koeffizientenvektor unseres Schätzers \ref{estimate} die eindeutige Lösung des linearen Gleichungssystems 
\begin{equation}
\label{les}
\bigg(\frac{1}{n}\mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1} \bigg) \mathbf{a} = \frac{1}{n} \mathbf{B}^T\mathbf{Y},
\end{equation}
wobei $\mathbf{1}$ eine $JxJ$-Einheitsmatrix ist.
Den Schätzer aus \ref{estimate} kann man umschreiben zu 
$$ \tilde{m}_n(x) = \sum_{j = 1}^J a_j \cdot B_j(x)$$
wobei $\mathbf{a} = (a_j)_{j = 1,\dots,J} \in \R^J$ wie in \ref{min} den Ausdruck
\begin{equation}
\begin{split}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\bi \in [M]^d} \sum_{\substack{\bj \in [N]^d\\|\bj|_1 \leq N}} a_{\mathbf{i},\bj}^2 \\
& = \frac{1}{n}(\mathbf{Y} - \mathbf{B}\mathbf{a})^T(\mathbf{Y} - \mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - \mathbf{Y}^T\mathbf{B}\mathbf{a} - \mathbf{a}^T\mathbf{B}^T\mathbf{Y} + \mathbf{a}^T\mathbf{B}^T\mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{B}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}\bigg) \mathbf{a}, 
\end{split}
\end{equation} 
minimiert. In der vorletzten Gleichung haben wir verwendet das $\mathbf{Y}^T\mathbf{B}\mathbf{a} = \mathbf{a}^T\mathbf{B}^T\mathbf{Y}$ gilt, da dieser Ausdruck eine reelle Zahl und damit insbesondere symmetrisch ist. 
Die Matrix $\mathbf{B}^T\mathbf{B} \in \R^{JxJ}$ ist positiv semidefinit, denn aufgrund der Verschiebungseigenschaft des Standardskalarprodukts gilt für alle $x \in \R\colon$
$$\langle x, \mathbf{B^T}\mathbf{B} x\rangle = \langle \mathbf{B} x, \mathbf{B} x\rangle \geq 0.$$
Zudem wissen wir dass $\frac{c_3}{n}\mathbf{1}$ durch die Wahl von $c_3$ nur positive Eigenwerte besitzt und damit positiv definit ist.  
Daher wissen wir, dass die Matrix
$$\mathbf{A} = \frac{1}{n}\mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}$$ ebenfalls nur positive Eigenwerte besitzt (REFERENZ), damit also positiv definit ist und eine inverse Matrix $\mathbf{A}^{-1}$ existiert. Zudem ist die Matrix $\mathbf{A}$ symmetrisch. 
Mit $\mathbf{b} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y} \in \R^J$ und $\mathbf{b}^T\mathbf{A}\mathbf{a} = \mathbf{a}\mathbf{A}\mathbf{b} = \mathbf{Y}^T\mathbf{B}\mathbf{a}$, was aus der Symmetrie von $\mathbf{A}$ folgt, erhalten wir$\colon$
\begin{equation*}
\begin{split}
& \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{B}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}\bigg) \mathbf{a} \\
& = \mathbf{a}^T\mathbf{A}\mathbf{a} + \mathbf{b}^T\mathbf{A}\mathbf{a} + \mathbf{a}^T\mathbf{A}\mathbf{b} + \mathbf{b}^T\mathbf{A}\mathbf{b} + \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{B}\mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y} \\
& = (\mathbf{a} + \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y})^T \mathbf{A} (\mathbf{a} - \frac{1}{n} \cdot \mathbf{A}^{-1} \mathbf{B}^T\mathbf{Y}) - \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{B}\mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y}.
\end{split} 
\end{equation*} 
Die letzte Gleichung wird für $\mathbf{a} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y}$ minimal, 
da wir wissen dass $\mathbf{A}$ positiv definit ist und damit $x^T\mathbf{A}x > 0$ für alle $x \in \R^J$ mit $x \neq 0$ gilt und $(\mathbf{a} - \mathbf{b})^T\mathbf{A}(\mathbf{a} - \mathbf{b}) = 0$ ist für $\mathbf{a} = \mathbf{b}$. Dies zeigt also, dass der Koeffizientenvektor unseres Schätzers \ref{estimate} die eindeutige Lösung des linearen Gleichungssystems \ref{les} ist.
Da der Koeffizientenvektor die Gleichung \ref{min} minimiert, erhalten wir wenn wir den Koeffizientenvektor mit dem Nullvektor gleichsetzen$\colon$
$$\frac{1}{n}(\mathbf{Y} - \mathbf{B}\mathbf{a})^T(\mathbf{Y} - \mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \leq \frac{1}{n} \sum_{i = 1}^n Y_i^2,$$
was uns erlaubt eine obere Schranke für den absoluten Wert unsere Koeffizienten abzuleiten.


