\chapter{Konstruktion des Neuronale Netze Regressionsschätzers}
\label{chap:2}

In diesem Kapitel werden wir mithilfe von unseren gegebenen Datenmenge 
$$ \mathfrak{D}_n = \{(X_1, Y_1),\dots,(X_n, Y_n)\},$$
unseren Regressionsschätzer konstruieren. 

Die Netzwerkarchitektur $(L, k)$ hängt von einer positiven ganzen Zahl $L$, die der Anzahl der verborgenen Schichten ist und einem Vektor $k = (k_1,\dots,k_L) \in \N^L$, der mit jeder Kombonente die Anzahl der Neuronen in der jeweiligen verborgen Schichte angibt. 

Ein mehrschichtiges feedforward neuronales Netz mit Architektur $(L, k)$ und dem logistischen squasher (\ref{logsquasher}) als Aktivierungsfunktion, ist eine reelwertige Funktion $f\colon \R^d \to \R$ definiert durch$\colon$

\begin{align}
\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{für $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ rekursiv definiert durch$\colon$} \nonumber \\
f_i^{(r)}(x) & = \sigma \bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg) \nonumber \\
\intertext{für $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R (r = 2,\dots, L)$ und$\colon$} \\
f_i^{(1)}(x) & = \sigma \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber \\
\intertext{für $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$.} \nonumber
\end{align} 
Bei neuronale Netze Regressionsschätzer wählt man keine Aktivierungsfunktion mehr, da wir einen Funktionswert schätzen wollen nichts mit einer Wahrscheinlichkeit klassifizieren möchten. (REFERENZ)

Für die Konstruktion unseren Schätzers verwenden wir die gegebene Datenmenge $\mathfrak{D}$ und wählen die Gewichte des neuronalen Netzes so, dass die resultierende Funktion aus (\ref{networkarch}) eine gute Schätzungen für die Regressionsfunktion ist. Dafür wählen wir die Gewichte bis auf die in der Ausgabeschicht fest und schätzen die Gewichte in der Ausgabeschicht in dem wir mit unserer Datenmenge ein regularisiertes Kleinste-Quadrate-Problem (REFERENZ) lösen.

\section{Definition der Netzwerkarchitektur}
Sei $a > 0$ fest. Die Wahl der Netzwerkarchitektur und der Werte aller Gewichte bis auf die aus der Ausgabeschicht ist durch folgendes Approximationsresultat durch eine lokale Konvexkombination von Taylorpolynomen für $(p,C)$-glatte Funktionen für $x \in [-a, a]^d$ motiviert. 
Sei dafür $M \in \N$ und $\mathbf{i} = (i^{(1)},\dots,i^{(d)}) \in \{0,\dots, M\}^d$, sei
$$x_{\mathbf{i}} = \bigg( -a + i^{(1)} \cdot \frac{2a}{M},\dots, -a + i^{(d)} \cdot \frac{2a}{M}\bigg)$$
und sei
$$\{\mathbf{i}_1,\dots, \mathbf{i}_{(M + 1)^d}\} = \{0,\dots,M\}^d,$$
d.h.  $\mathbf{i}_1,\dots,\mathbf{
i}_{(M + 1)^d}$ sind insgesamt $M + 1$ Vektoren der Dimension $d$, wobei jede Komponente aus der Menge $\{0,\dots,M\}$ ausgewählt wurde.
Für $k \in \{1,\dots,(M + 1)^d\}$ sei
$$p_{\mathbf{i}_k}(x) = \sum_{\substack{j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}}(x_{\mathbf{i}_k}) \cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d}$$
das Taylorpolynom von $m$ der Ordnung $q$ im Entwicklungspunkt $x_{\mathbf{i}_k}$ und sei
$$P(x) = \sum_{k = 1}^{(M + 1)^d} p_{\mathbf{i}_k}(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+.$$
Wir zeigen im folgenden Lemma dass $P(x)$ eine lokale Konvexkombination von Taylorpolynomen von m ist.
\begin{lem}
\label{lem:loccon}

\end{lem}
\begin{proof}

\end{proof}


\section{Definition der Gewichte der Ausgabeschicht}

TBD