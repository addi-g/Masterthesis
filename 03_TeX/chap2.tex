\chapter{Konstruktion des Neuronale Netze Regressionsschätzers}
\label{chap:2}

In diesem Kapitel werden wir mithilfe von unseren gegebenen Datenmenge 
$$ \mathfrak{D}_n = \{(X_1, Y_1),\dots,(X_n, Y_n)\},$$
unseren Regressionsschätzer konstruieren. 

Die Netzwerkarchitektur $(L, k)$ hängt von einer positiven ganzen Zahl $L$, die der Anzahl der verborgenen Schichten ist und einem Vektor $k = (k_1,\dots,k_L) \in \N^L$, der mit jeder Kombonente die Anzahl der Neuronen in der jeweiligen verborgen Schichte angibt. 

Ein mehrschichtiges feedforward neuronales Netz mit Architektur $(L, k)$ und dem logistischen squasher (\ref{logsquasher}) als Aktivierungsfunktion, ist eine reelwertige Funktion $f\colon \R^d \to \R$ definiert durch$\colon$

\begin{align}
\label{networkarch}
f(x) & = \sum_{i = 1}^{k_L} c_i^{(L)} \cdot f_i^{(L)}(x) + c_0^{(L)} \nonumber \\
\intertext{für $c_0^{(L)},\dots,c_{k_L}^{(L)} \in \R$ und für $f_i^{(L)}$ rekursiv definiert durch$\colon$} \nonumber \\
f_i^{(r)}(x) & = \sigma \bigg(\sum_{j = 1}^{k_r - 1} c_{i,j}^{(r - 1)} \cdot f_j^{(r - 1)}(x) + c_{i,0}^{(r - 1)} \bigg) \nonumber \\
\intertext{für $c_{i,0}^{(r - 1)},\dots,c_{i,k_{r - 1}}^{(r - 1)} \in \R (r = 2,\dots, L)$ und$\colon$} \\
f_i^{(1)}(x) & = \sigma \Bigg(\sum_{j = 1}^{d} c_{i,j}^{(0)} \cdot x^{(j)} + c_{i,0}^{(0)} \Bigg) \nonumber \\
\intertext{für $c_{i,0}^{(0)},\dots,c_{i,d}^{(0)} \in \R$.} \nonumber
\end{align} 
Bei neuronale Netze Regressionsschätzer wählt man keine Aktivierungsfunktion mehr, da wir einen Funktionswert schätzen wollen nichts mit einer Wahrscheinlichkeit klassifizieren möchten. (REFERENZ)

Für die Konstruktion unseren Schätzers verwenden wir die gegebene Datenmenge $\mathfrak{D}$ und wählen die Gewichte des neuronalen Netzes so, dass die resultierende Funktion aus (\ref{networkarch}) eine gute Schätzungen für die Regressionsfunktion ist. Dafür wählen wir die Gewichte bis auf die in der Ausgabeschicht fest und schätzen die Gewichte in der Ausgabeschicht in dem wir mit unserer Datenmenge ein regularisiertes Kleinste-Quadrate-Problem (REFERENZ) lösen.

\section{Definition der Netzwerkarchitektur}
Sei $a > 0$ fest. Die Wahl der Netzwerkarchitektur und der Werte aller Gewichte bis auf die aus der Ausgabeschicht ist durch folgendes Approximationsresultat durch eine lokale Konvexkombination von Taylorpolynomen für $(p,C)$-glatte Funktionen für $x \in [-a, a]^d$ motiviert. 
Sei dafür $M \in \N$ und $\mathbf{i} = (i^{(1)},\dots,i^{(d)}) \in \{0,\dots, M\}^d$, sei
$$x_{\mathbf{i}} = \bigg( -a + i^{(1)} \cdot \frac{2a}{M},\dots, -a + i^{(d)} \cdot \frac{2a}{M}\bigg)$$
und sei
$$\{\mathbf{i}_1,\dots, \mathbf{i}_{(M + 1)^d}\} = \{0,\dots,M\}^d,$$
d.h.  $\mathbf{i}_1,\dots,\mathbf{
i}_{(M + 1)^d}$ sind insgesamt $M + 1$ Vektoren der Dimension $d$, wobei jede Komponente aus der Menge $\{0,\dots,M\}$ ausgewählt wurde.
Für $k \in \{1,\dots,(M + 1)^d\}$ sei
$$p_{\mathbf{i}_k}(x) = \sum_{\substack{j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}}(x_{\mathbf{i}_k}) \cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d}$$
das Taylorpolynom von $m$ der Ordnung $q$ im Entwicklungspunkt $x_{\mathbf{i}_k}$ und sei
\begin{equation}
\label{konvexkomb}
P(x) = \sum_{k = 1}^{(M + 1)^d} p_{\mathbf{i}_k}(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+,
\end{equation}
 mit $z_+ = \max\{z, 0\} (z \in \R).$
Wir zeigen im folgenden Lemma dass $P(x)$ eine lokale Konvexkombination von Taylorpolynomen von m ist.
\begin{lem}
\label{lem:loccon}
Sei $a >0, M \in \N$ und $$P(x) = \sum_{k = 1}^{(M + 1)^d} p_{\mathbf{i}_k}(x) \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+$$ mit $p_{\mathbf{i}_k}(x)$ wie oben, dann ist $P(x)$ eine lokale Konvexkombination von Taylorpolynomen von m.
\end{lem}
\begin{proof}
Es sind drei Bedingungen zu überprüfen. Als erstes geben wir aber für $d = 2$ und $M = 3$ eine Skizze an um die Idee des Beweises zu veranschaulichen. (SKIZZE EINFÜGEN)
Es ist ein Gitter mit $(M + 1)^d$ Gitterpunkten die den $x_{\mathbf{i}_k}$ entsprechen. Der Abstand zwischen zwei Gitterpunkten beträgt $\frac{2a}{M}.$ Man betrachtet immer den Abstand zu den nähesten $2^d$ Gitterpunkten, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}_k}^{(j)}$ größer als $\frac{2a}{M}$ ist.     

\textbf{i)} Im folgenden wollen wir $$\sum_{k = 1}^{(M + 1)^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ = 1,$$ per Induktion über $d$ zeigen. 
	
	 \emph{Induktionsanfang} (IA): Für $d = 1$ kann nur zwischen zwei Gitterpunkten liegen und mit der obigen Begründung ist der Rest gleich Null, daher nehmen wir oBdA an, dass $x$ zwischen $x_{\mathbf{i}_1}$ und $x_{\mathbf{i}_2}$ liegt. 
	Damit folgt 
	\begin{equation*}
	\begin{split}
	\sum_{k = 1}^{(M + 1)} (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_k}|)_+ & = (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_1}|)_+ + (1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_2}|)_+ \\
	& = 1 + 1 - \frac{M}{2a} \cdot |x - x_{\mathbf{i}_1} + x_{\mathbf{i}_2} - x| \\
	& = 1 + 1 - \frac{M}{2a} \cdot \frac{2a}{M} \\
	& = 1,
	\end{split}
	\end{equation*} wobei wir unter anderem verwendet haben, dass beide Summenden unabhängig von dem Positivteil nichtnegativ sind, da der Abstand von $x$ zu den beiden Gitterpunkten $x_{\mathbf{i}_1}$ und$ x_{\mathbf{i}_2}$ kleiner gleich $\frac{2a}{M}$ ist. Zudem haben wir verwendet, dass $x_{\mathbf{i}_2} - x_{\mathbf{i}_1} = \frac{2a}{M}$ gilt, da beides Gitterpunkte sind.     
	
\emph{Induktionshypothese} (IH): Aussage \textbf{i)} gilt für eine beliebiges aber festes $d \in \N.$

\emph{Induktionsschritt} (IS): Wir nehmen oBdA. an, dass $x_{(0,\dots,0) \leq x \leq x_{(1,\dots,1}}$ gilt, mit $\mathbf{i}_1 = (0,\dots,0)$ und $\mathbf{i}_{(M + 1)}^{d + 1} = (1,\dots,1).$ Das heißt also, dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt. Im folgenden zeigen wir $$\sum_{k = 1}^{(M + 1)^{d + 1}} \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ = 1.$$
Alle Summanden sind Null, wenn $|x^{(j)} - x_{\mathbf{i}_k}^{(j)}| \geq \frac{2a}{M}$ ist. Zudem haben wir oBdA angenommen dass $x \in [-a, -a + \frac{2a}{M}]^{d + 1}$ gilt, damit haben wir also nur noch $2^{d + 1}$ Summanden, nämlich die Anzahl der Gitterpunkte die am nähesten zu $x$ sind. Zudem wissen wir, dass alle Gitterpunkte, die in der $(d + 1)$ Komponente den selben Wert haben, sind in dieser Dimension gleich weit von $x^{(d + 1)}$ entfernt. Das heißt, in jedem Summanden kommt der Faktor $(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|)$ bzw. $(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|)$ vor, da 
\begin{equation*}
(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_i^{(d + 1)}|) = \begin{cases}
(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|) &\text{$i \in \{0, 1\}^d \times \{0\}$}\\
(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|) &\text{$i \in \{0, 1\}^d \times \{1\}$}
\end{cases}
\end{equation*}
daraus ergibt sich$\colon$
\begin{equation*}
\begin{split}
\sum_{k = 1}^{(M + 1)^{d + 1}} & \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ \\
& = \sum_{i \in \{0, 1\}^{d + 1}} \prod_{j = 1}^{d + 1} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_i^{(j)}|\bigg) \\
& = \Bigg(\sum_{i \in \{0, 1\}^d \times \{0\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_i^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|\bigg) \\
& \qquad + \Bigg(\sum_{i \in \{0, 1\}^d \times \{1\}} \prod_{j = 1}^{d} \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_i^{(j)}|\bigg)\Bigg) \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|\bigg) \\
& \stackrel{(IV)}{=} 1 \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)}|\bigg) + 1 \cdot \bigg(1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(1,\dots,1)}^{(d + 1)}|\bigg) \\
& = 1 + 1 - \frac{M}{2a} \cdot |x^{(d + 1)} - x_{(0,\dots,0)}^{(d + 1)} + x_{(1,\dots,1)}^{(d + 1)} - x^{(d + 1)}| \\
& = 1 + 1 - 1 \\
& = 1,
\end{split}
\end{equation*}
wobei wir bei der vorletzten Gleichung angewendet haben, dass $x_{(1,\dots,1)} - x_{(0,\dots,0)} = \frac{2a}{M}$ ist, da beides Gitterpunkte sind.  $\hfill(\square)$ 		
		
\textbf{ii)} Es folgt $\prod_{j = 1}^d (1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|)_+ \geq 0$ für alle $k = 1,\dots, (M + 1)^d$, da $$z_+= \max\{z, 0\} \geq 0 (z \in \R)$$ gilt. Damit wäre die Nichtnegativität der Koeffizienten der Linearkombination gezeigt. Damit ist jeder Summand in $$\sum_{k = 1}^{(M + 1)^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+$$ größer gleich Null und wegen i) muss dann auch $$\prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ \leq 1$$ gelten.

\textbf{iii)} Es handelt sich hierbei um eine lokale Konvexität, da die Bedingungen \textbf{i)} und \textbf{ii)} für alle $x \in [-a, a]$ gelten.
\end{proof}
Als nächstes zeigen wir ein Resultat für $(p, C)$-glatte Funktion welches wir im weiteren Verlauf dieser Arbeit wieder benötigen werden.
\begin{lem}
\label{lem:pcsmooth}
Sei $M \in \N$, $c_1$ eine Konstante, $a > 0$ und $m$ eine ($p, C$)-glatte Funktion, wobei $p = q + s$ mit $q \in \N_0$ und $s \in (0,1]$. Sei zudem $P(x)$ wie in (\ref{konvexkomb}) eine lokale Konvexkombination von Taylorpolynomen von $m$. Dann gilt$\colon$
$$\sup_{x \in [-a, a]^d} |m(x) - P(x)|  \leq c_1 \cdot \frac{1}{M^p}.$$
\end{lem}
\begin{proof}
Nach dem Satz über die Lagrange Form des Restglieds (REFERENZ) existiert ein $\xi \in [x, x_{\mathbf{i}_k}],$ so, dass 
\begin{equation*}
\begin{split}
m(x) & = T_{x_{\mathbf{i}_k},q - 1}[m(x)] \\
& = \sum_{\substack{j_1,\dots,j_d \in \{0,\dots,q - 1\} \\j_1+\dots +j_d \leq q - 1}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m(x_{\mathbf{i}_k})}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}} \cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d} \\
 & \quad + \sum_{\substack{q - 1 < j_1,\dots,j_d \leq q \\j_1+\dots +j_d \leq q}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m(\xi)}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}}\cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d}.
\end{split}
\end{equation*}
Nach dem Beweis von Lemma \ref{konvexkomb} \textbf{i)} erhalten wir 
$$m(x) = \sum_{k = 1}^{(M + 1)^d} m(x) \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+.$$ Zudem wissen wir dass man immer den Abstand zu den nähesten $2^d$ Gitterpunkten betrachtet, da  $(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|)_+ = 0$ immer dann gilt, wenn der Abstand zwischen $x^{(j)}$ und $x_{\mathbf{i}_k}^{(j)}$ größer als $\frac{2a}{M}$ ist, daher ergibt sich$\colon$
\begin{equation*}
\begin{split}
& \sum_{\substack{q - 1 < j_1,\dots,j_d \leq q \\j_1+\dots +j_d \leq q}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m(\xi)}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}}\cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d} \\
& \qquad \leq \sum_{\substack{q - 1 < j_1,\dots,j_d \leq q \\j_1+\dots +j_d \leq q}} \frac{1}{j_1! \cdots j_d!} \cdot \frac{\partial^{j_1+\cdots + j_d} m(\xi)}{\partial^{j_1} x^{(1)}\cdots \partial^{j_d} x^{(d)}}\cdot \bigg(\frac{2a}{M}\bigg)^q
\end{split}
\end{equation*} 
und folgern mithilfe der Dreiecksungleichung und der $(p,C)$-Glattheit von $m\colon$
\begin{equation*}
\begin{split}
|m(x) - P(x)| & \leq \sum_{k = 1}^{(M + 1)^d} |m(x) - p_{\mathbf{i}_k}| \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ \\
& \leq \bigg(\frac{2a}{M}\bigg)^q \|\xi - x_{\mathbf{i}_k}\|^s \cdot C \cdot \sum_{k = 1}^{(M + 1)^d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+ \\
& = C \cdot \bigg(\frac{2a}{M}\bigg)^p \\
& = c_1 \cdot \frac{1}{M^p},
\end{split}
\end{equation*}
wobei wir bei der letzten Gleichung  Bedingung \textbf{i)} aus dem Beweis von Lemma \ref{konvexkomb} und $q + s = p$ verwendet haben.
\end{proof}

$P(x)$ lässt sich in die Form 
$$\sum_{k = 1}^{(M + 1)^d} \sum_{\substack{ j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} a_{\mathbf{i}_k,j_1,\dots,j_d} \cdot (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+$$
durch geeignet gewählte $a_{\mathbf{i}_k,j_1,\dots,j_d} \in \R$ bringen.
Als nächstes wollen wir geeignete neuronale Netze $f_{net,j_1,\dots,j_d,\mathbf{i}_k}$ definieren, die die Funktionen
$$x \mapsto (x^{(1)} - x_{\mathbf{i}_k}^{(1)})^{j_1} \cdots (x^{(d)} - x_{\mathbf{i}_k}^{(d)})^{j_d} \prod_{j = 1}^d \bigg(1 - \frac{M}{2a} \cdot |x^{(j)} - x_{\mathbf{i}_k}^{(j)}|\bigg)_+$$ approximieren. Zudem möchten wir die Netzwerkarchitektur so wählen, dass neuronale Netze der Form
$$\sum_{k = 1}^{(M + 1)^d} \sum_{\substack{ j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} a_{\mathbf{i}_k,j_1,\dots,j_d} \cdot f_{net,j_1,\dots,j_d,\mathbf{i}_k}(x) \qquad (a_{\mathbf{i}_k,j_1,\dots,j_d} \in \R)$$ in ihr enthalten sind.
Um dies zu erreichen, sei $\sigma(x) = \frac{1}{(1 + \exp(-x))} \quad (x \in \R)$ der logistische Squasher (\ref{logsquasher}), wählen $R \geq 1$ und definieren die folgenden neuronale Netze$\colon$
Das neuronale Netz
\begin{equation}
\label{def:fid}
f_{id}(x) = 4R \cdot \sigma\Big(\frac{x}{R}\Big) - 2R,
\end{equation}
welches, wie in Lemma \ref{lem:1} gezeigt, die Funktion $f(x) = x$ approximiert. 
Das neuronale Netz 
\begin{equation}
\label{def:fmult}
\begin{split}
f_{mult}(x, y) = \frac{R^2}{4} \cdot \frac{(1 + \exp(- 1))^3}{\exp(-2) - \exp(-1)} \cdot & \bigg(\sigma\Big(\frac{2(x + y)}{R} + 1\Big) - 2 \cdot \sigma \Big(\frac{x + y}{R} + 1\Big) \\
& - \sigma\Big(\frac{2(x - y)}{R} + 1\Big) + 2 \cdot \sigma\Big(\frac{x - y}{R} + 1\Big)\bigg),
\end{split}
\end{equation}
welches, wie in Lemma \ref{lem:2} gezeigt, die Funktion $f(x, y) = x \cdot y$ approximiert. Das neuronale Netz 
\begin{equation}
\label{def:frelu}
f_{ReLu}(x) = f_{mult}(f_{id}(x), \sigma(R \cdot x)),
\end{equation}
welches, wie in Lemma \ref{lem:3} gezeigt, die Funktion $f(x) = x_+$ approximiert und schließlich noch das neuronale Netz 
\begin{equation}
\label{def:fhat}
f_{hat,y}(x) = f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) + 1\bigg) - 2 \cdot f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y)\bigg) +  f_{ReLu}\bigg(\frac{M}{2a} \cdot (x - y) - 1\bigg),
\end{equation}
welches, wie in Lemma \ref{lem:4} gezeigt, für fixes $y \in \R$ die Funktion $$f(x) = \bigg(1 - \bigg(\frac{M}{2a}\bigg) \cdot |x - y|\bigg)_+$$ approximiert. 
Mit diesen neuronalen Netzen können wir nun $f_{net,j_1,\dots,j_d,\mathbf{i}_k}$ rekursiv definieren. Dafür wählen wir $N \geq q$, setzen $s = \lceil\log_2(N + d)\rceil$ und definieren für $j_1,\dots,j_d \in \{0, 1,\dots, N\}$ und $k \in \{1,\dots,(M + 1)^d\}\colon$  
\begin{align*}
\label{fnet}
& f_{net,j_1,\dots,j_d,\mathbf{i}_k}(x) = f_1^{(0)}(x). \\
\intertext{wobei} \\
& f_k^{(l)}(x) = f_{mult}\Big(f_{2k - 1}^{(l + 1)}(x),f_{2k}^{(l + 1)}(x)\Big) \\
\intertext{für $k \in \{1, 2, \dots, 2^l \}$ und $l \in\{0,\dots,s - 1\}, $ und} \\
& f_k^{(s)}(x) = f_{id}(f_{id}(x^{(l)} - x_{\mathbf{i}_k}^{(l)}))  \\
\intertext{für $j_1 + j_2 + \dots + j_{l-1} + 1 \leq k \leq j_1 + j_2 + \dots + j_l$ und $l = 1,\dots,d$ und} \\ 
& f_{j_1 + j_2 + \dots + j_d + k}^{(s)}(x) = f_{hat,x_{\mathbf{i}_k}^{(k)}}(x^{(k)}) \\
\intertext{für $k = 1,\dots,d$ und} \\
& f_k^{(s)}(x) = 1 \\
\intertext{für $k = j_1 + j_2 + \dots + j_d + d + 1,j_1 + j_2 + \dots + j_d + d + 2,\dots,2^s.$}
\end{align*} 
ANZAHL DER SCHICHTEN UND NEURONEN PRO SCHICHT ERLÄUTERN.
Da man bei fully-connencted neuronalen Netzen die Gewichte der Verbindungen zwischen zwei Neuronen auf Null setzen kann, sind auch nicht fully-connected neuronale Netze in der der Klasse aller fully-connected neuronaler Netze, mit $s + 2$ verbogenen Schichten mit jeweils $24 \cdot (N + d)$ Neuronen pro Schicht, enthalten und damit auch insbesondere $f_{net,j_1,\dots,j_d,\mathbf{i}_k}$.


\section{Definition der Gewichte der Ausgabeschicht}

Wir definieren unseren neuronale Netze Regressionsschätzer $\tilde{m}_n(x)$ durch$\colon$
\begin{equation}
\label{estimate}
\tilde{m}_n(x) = \sum_{k = 1}^{(M + 1)^d} \sum_{\substack{ j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} a_{\mathbf{i}_k,j_1,\dots,j_d} \cdot f_{net,j_1,\dots,j_d,\mathbf{i}_k}(x),
\end{equation}
wobei wir die Koeffizienten $a_{\mathbf{i}_k,j_1,\dots,j_d}$ durch Minimierung von 
\begin{equation}
\label{min} \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\substack{ j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} a_{\mathbf{i}_k,j_1,\dots,j_d}^2
\end{equation}
für eine Konstante $c_3 > 0.$ Dieses regularisierte lineare Kleinste-Quadrate Schätzung erhalten wir durch die Lösung eine linearen Gleichungssystems. Dafür definieren wir uns 
\begin{align*} 
\{B_j \mid j = 1,\dots,J\} = \Bigl\{f_{net,j_1,\dots,j_d,\mathbf{i}_k}(x) \mid 1\leq k \leq (1 + M)^d \quad \text{und} \quad 0 \leq j_1 + \cdots + j_d \leq N \Bigr\}
\end{align*}
wobei
$$ J = (M + 1)^d \cdot \binom{N + d}{d}$$ die Kardinalität der Menge ist.
Dies erhält man durch TBD.
Wir setzen nun 
$$ \mathbf{B} = (B_j(X_i))_{1\leq i \leq n,1\leq j \leq J} \quad \text{und} \quad \mathbf{Y} = (Y_i)_{i = 1,\dots,n}.$$
Wir zeigen im Folgenden, dass der Koeffizientenvektor unseres Schätzers \ref{estimate} die eindeutige Lösung des linearen Gleichungssystems 
\begin{equation}
\label{les}
\bigg(\frac{1}{n}\mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1} \bigg) \mathbf{a} = \frac{1}{n} \mathbf{B}^T\mathbf{Y},
\end{equation}
wobei $\mathbf{1}$ eine $JxJ$-Einheitsmatrix ist.
Den Schätzer aus \ref{estimate} kann man umschreiben zu 
$$ \tilde{m}_n(x) = \sum_{j = 1}^J a_j \cdot B_j(x)$$
wobei $\mathbf{a} = (a_j)_{j = 1,\dots,J} \in \R^J$ wie in \ref{min} den Ausdruck
\begin{equation}
\begin{split}
& \frac{1}{n} \sum_{i = 1}^n|Y_i - \tilde{m}_n(X_i)|^2 + \frac{c_3}{n} \cdot \sum_{\substack{ j_1,\dots,j_d \in \{0,\dots,q\} \\j_1+\dots +j_d \leq q}} a_{\mathbf{i}_k,j_1,\dots,j_d}^2 \\
& = \frac{1}{n}(\mathbf{Y} - \mathbf{B}\mathbf{a})^T(\mathbf{Y} - \mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - \mathbf{Y}^T\mathbf{B}\mathbf{a} - \mathbf{a}^T\mathbf{B}^T\mathbf{Y} + \mathbf{a}^T\mathbf{B}^T\mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \\
& = \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{B}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}\bigg) \mathbf{a}, 
\end{split}
\end{equation} 
minimiert. In der vorletzten Gleichung haben wir verwendet das $\mathbf{Y}^T\mathbf{B}\mathbf{a} = \mathbf{a}^T\mathbf{B}^T\mathbf{Y}$ gilt, da dieser Ausdruck eine reelle Zahl und damit insbesondere symmetrisch ist. 
Die Matrix $\mathbf{B}^T\mathbf{B} \in \R^{JxJ}$ ist positiv semidefinit, denn aufgrund der Verschiebungseigenschaft des Standardskalarprodukts gilt für alle $x \in \R\colon$
$$\langle x, \mathbf{B^T}\mathbf{B} x\rangle = \langle \mathbf{B} x, \mathbf{B} x\rangle \geq 0.$$
Zudem wissen wir dass $\frac{c_3}{n}\mathbf{1}$ durch die Wahl von $c_3$ nur positive Eigenwerte besitzt und damit positiv definit ist.  
Daher wissen wir, dass die Matrix
$$\mathbf{A} = \frac{1}{n}\mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}$$ ebenfalls nur positive Eigenwerte besitzt (REFERENZ), damit also positiv definit ist und eine inverse Matrix $\mathbf{A}^{-1}$ existiert. Zudem ist die Matrix $\mathbf{A}$ symmetrisch. 
Mit $\mathbf{b} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y} \in \R^J$ und $\mathbf{b}^T\mathbf{A}\mathbf{a} = \mathbf{a}\mathbf{A}\mathbf{b} = \mathbf{Y}^T\mathbf{B}\mathbf{a}$, was aus der Symmetrie von $\mathbf{A}$ folgt, erhalten wir$\colon$
\begin{equation*}
\begin{split}
& \frac{1}{n}(\mathbf{Y}^T\mathbf{Y} - 2\mathbf{Y}^T\mathbf{B}\mathbf{a}) + \mathbf{a}^T\bigg(\frac{1}{n} \mathbf{B}^T\mathbf{B} + \frac{c_3}{n} \cdot \mathbf{1}\bigg) \mathbf{a} \\
& = \mathbf{a}^T\mathbf{A}\mathbf{a} + \mathbf{b}^T\mathbf{A}\mathbf{a} + \mathbf{a}^T\mathbf{A}\mathbf{b} + \mathbf{b}^T\mathbf{A}\mathbf{b} + \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{B}\mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y} \\
& = (\mathbf{a} + \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y})^T \mathbf{A} (\mathbf{a} - \frac{1}{n} \cdot \mathbf{A}^{-1} \mathbf{B}^T\mathbf{Y}) - \frac{1}{n}\mathbf{Y}^T\mathbf{Y} - \frac{1}{n^2}\mathbf{Y}^T\mathbf{B}\mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y}.
\end{split} 
\end{equation*} 
Die letzte Gleichung wird für $\mathbf{a} = \frac{1}{n} \cdot \mathbf{A}^{-1}\mathbf{B}^T\mathbf{Y}$ minimal, 
da wir wissen dass $\mathbf{A}$ positiv definit ist und damit $x^T\mathbf{A}x > 0$ für alle $x \in \R^J$ mit $x \neq 0$ gilt und $(\mathbf{a} - \mathbf{b})^T\mathbf{A}(\mathbf{a} - \mathbf{b}) = 0$ ist für $\mathbf{a} = \mathbf{b}$. Dies zeigt also, dass der Koeffizientenvektor unseres Schätzers \ref{estimate} die eindeutige Lösung des linearen Gleichungssystems \ref{les} ist.
Da der Koeffizientenvektor die Gleichung \ref{min} minimiert, erhalten wir wenn wir den Koeffizientenvektor mit dem Nullvektor gleichsetzen$\colon$
$$\frac{1}{n}(\mathbf{Y} - \mathbf{B}\mathbf{a})^T(\mathbf{Y} - \mathbf{B}\mathbf{a}) + \frac{c_3}{n} \cdot \mathbf{a}^T\mathbf{a} \leq \frac{1}{n} \sum_{i = 1}^n Y_i^2,$$
was uns erlaubt eine obere Schranke für den absoluten Wert unsere Koeffizienten abzuleiten.


